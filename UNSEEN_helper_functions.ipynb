{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fd3604-adb9-4e79-881c-9e18988d2e64",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "\n",
    "This notebook collates all the functions that help the other notebooks do their thing, without clogging the other notebooks with function definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad1828-3272-4b6b-b8c4-7557b24bfe80",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ece23826-2a4d-451b-8002-501a226f745c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "import numpy\n",
    "import pandas\n",
    "pandas.set_option('display.max_colwidth', None)\n",
    "import datetime\n",
    "import itertools\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot\n",
    "import plotly\n",
    "from google.cloud import bigquery, exceptions\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from IPython import get_ipython\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet\n",
    "import fastparquet\n",
    "import pathlib\n",
    "import time\n",
    "import re\n",
    "import rpy2.ipython\n",
    "from multiprocessing.pool import Pool\n",
    "import functools\n",
    "import copy\n",
    "import pyinform\n",
    "import EntropyHub\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b06dd66-0f5e-414f-ac99-7a918e2009d3",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a3885-d79d-4a12-be5a-63ef2e2a9981",
   "metadata": {},
   "source": [
    "### `entropy_output()`: Compute and present the entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e423811f-5800-42c5-9f0e-a3ddd889fbc3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to compute and present the entropy of a column in a pandas.Dataframe.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. my_df_column:            A column from a pandas.Dataframe containing the variable\n",
    "#                             for which entropy needs calculating.\n",
    "#\n",
    "# RETURNS\n",
    "# 1. entropy_caseness:        Entropy in nats.\n",
    "# 2. entropy_caseness_scaled: Entropy scaled to the theoretical maximum for a binary variable.\n",
    "#\n",
    "\n",
    "def entropy_output(my_df_column):\n",
    "    my_df_column.dropna(inplace = True)\n",
    "    entropy_caseness = scipy.stats.entropy(my_df_column.value_counts(), base = math.e)\n",
    "    \n",
    "    entropy_caseness_scaled = round(entropy_caseness / math.log(len(my_df_column.unique()), math.e) * 100, 1)\n",
    "    if entropy_caseness < 0.001:\n",
    "        print('\\t Caseness variable entropy < 0.001 nats')\n",
    "    else:\n",
    "        print(f'\\t Caseness variable entropy = {round(entropy_caseness, 3)} nats')\n",
    "    if entropy_caseness < 0.001:\n",
    "        print(f'\\t The caseness variable\\'s entropy is < 0.001 % its theoretical maximum\\n')\n",
    "    else:\n",
    "        print(f'\\t The caseness variable\\'s entropy is {entropy_caseness_scaled} % of its theoretical maximum\\n')\n",
    "    \n",
    "    return entropy_caseness, entropy_caseness_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2798124-2ccd-4129-9c23-1b8afcb61ea0",
   "metadata": {},
   "source": [
    "### `hitrate_output()`: Compute and present the hit rate - a.k.a. (mis)classification error - of a caseness variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db7dc06-6195-4223-80e2-2ad15604f81e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to compute and present the hitrate - a.k.a. (mis)classification\n",
    "# error - of a caseness variable.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. my_caseness_variable:  A column from a pandas.Dataframe containing patients'\n",
    "#                           caseness values.\n",
    "#\n",
    "# RETURNS\n",
    "# 1. hitRate_none:          The hitrate assuming no one demonstrates the caseness.\n",
    "# 2. hitRate_all:           The hitrate assuming everyone demonstrates the caseness.\n",
    "#\n",
    "\n",
    "def hitrate_output(my_caseness_variable):\n",
    "    \n",
    "    # Calculations.\n",
    "    my_caseness_variable.dropna(inplace = True)\n",
    "    numerator = my_caseness_variable.astype(bool).sum()\n",
    "    denominator = len(my_caseness_variable)\n",
    "    hitRate_all = (numerator / denominator) * 100\n",
    "    hitRate_none = 100 - hitRate_all\n",
    "    Odds_noYes = hitRate_none / hitRate_all\n",
    "    \n",
    "    # Message to user.\n",
    "    if hitRate_all < 0.001:\n",
    "        print(f'\\t Hit rate (all) < 0.001 %')\n",
    "        print(f'\\t Hit rate (none) \\u2248 100 %')\n",
    "        print(f'\\t Odds (No : Yes) \\u2248 infitely-times less likely to demonstrate caseness than to not.')\n",
    "    else:\n",
    "        print(f'\\t Hit rate (all) = {round(hitRate_all, 3)} %')\n",
    "        print(f'\\t Hit rate (none) = {round(hitRate_none, 3)} %')\n",
    "        print(f'\\t Odds (No : Yes) = {int(Odds_noYes):,}-times less likely to demonstrate caseness than to not.')\n",
    "    \n",
    "    return hitRate_none, hitRate_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b2573-22d8-4151-8f79-16ba39b60b80",
   "metadata": {},
   "source": [
    "### `boundaryfilter()`: Filter out feature sets that are not within the prevalence bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29853444-5d86-4d07-bf2f-26b174093610",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to filter out feature sets that are not within the prevlance bounds.\n",
    "#\n",
    "# The function counts the non-zero elements of each feature set can compares it \n",
    "# to the minimum and maximum count criteria.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. my_featureSet_array:       An n-by-fs pandas.Datqframe of n patients represented\n",
    "#                               by rows and fs features represented by columns.\n",
    "# 2. caseness:                  One of {'dxandrx', 'dxnotrx', 'rxnotdx', 'multi'} to\n",
    "#                               indicate which caseness variable's prevalence bounds to use.\n",
    "# 3. verbose:                   An optional argument to indicate whether the user \n",
    "#                               wants feedback on how many feature sets were removed.\n",
    "#\n",
    "# RETURNS\n",
    "# 1. filtered_featureSet_array: The inputted feature-set array but with the prevlance\n",
    "#                               violating feature sets removed.\n",
    "# 2. fs_removed_lower:          A numpy array of the names of feature sets removed \n",
    "#                               because their prevalence was too low.\n",
    "# 3. n_fs_removed_lower:        The count of feature sets removed for high low\n",
    "#                               prevalence.\n",
    "# 4. fs_removed_upper:          A numpy array of the namesof feature sets removed \n",
    "#                               because their prevalence was too high.\n",
    "# 5. n_fs_removed_upper:        The count of feature sets removed for having high\n",
    "#                               prevalence.\n",
    "#\n",
    "\n",
    "def boundaryfilter(my_featureSet_array, caseness = None, verbose = True):\n",
    "    \n",
    "    # Select filter boundaries based on `caseness` argument.\n",
    "    if caseness == 'dxandrx':\n",
    "        lb = DxAndRxCaseness_count_LB\n",
    "        ub = DxAndRxCaseness_count_UB\n",
    "    elif caseness == 'dxnotrx':\n",
    "        lb = DxNotRxCaseness_count_LB\n",
    "        ub = DxNotRxCaseness_count_UB\n",
    "    elif caseness == 'rxnotdx':\n",
    "        lb = RxNotDxCaseness_count_LB\n",
    "        ub = RxNotDxCaseness_count_UB\n",
    "    elif caseness == 'multi':\n",
    "        lb = multinomialCaseness_count_LB\n",
    "        ub = multinomialCaseness_count_UB\n",
    "    elif caseness == 'prescriptionVsDefinite':\n",
    "        lb = prescriptionVsDefiniteCaseness_count_LB                  \n",
    "        ub = prescriptionVsDefiniteCaseness_count_UB\n",
    "    elif caseness == None:\n",
    "        print('No values for `caseness` was provided.')\n",
    "        return\n",
    "    \n",
    "    # Identify the feature sets that are too few, and extract the column names.\n",
    "    fs_removed_lower = \\\n",
    "        my_featureSet_array.loc[:, \n",
    "        numpy.insert(\n",
    "        ((my_featureSet_array.loc[:, my_featureSet_array.columns != 'person_id'] != 0).sum(axis=0) < lb).values\n",
    "            ,0\n",
    "            ,False)\n",
    "                   ].columns\n",
    "    # Extract the count of feature sets that are too few.\n",
    "    n_fs_removed_lower = len(fs_removed_lower)\n",
    "\n",
    "    # Identify the feature sets that are too many, and extract the column names.\n",
    "    fs_removed_upper = \\\n",
    "        my_featureSet_array.loc[:, \n",
    "        numpy.insert(\n",
    "        ((my_featureSet_array.loc[:, my_featureSet_array.columns != 'person_id'] !=0).sum(axis=0) > ub).values\n",
    "            ,0\n",
    "            ,False)\n",
    "                   ].columns\n",
    "    # Extract the count of feature sets that are too many.\n",
    "    n_fs_removed_upper = len(fs_removed_upper)\n",
    "\n",
    "    # Remove the feature sets that are no within the prevalence bounds.\n",
    "    filtered_featureSet_array = \\\n",
    "        pandas.DataFrame(my_featureSet_array.drop(columns = numpy.insert(fs_removed_lower, 0, fs_removed_upper)))\n",
    "    \n",
    "    # Print message if arg{verbose} = True.\n",
    "    if verbose == True:\n",
    "        if caseness == 'dxandrx':\n",
    "            print(\"\\n Filtering complete for 'Definite caseness'...\")\n",
    "        elif caseness == 'dxnotrx':\n",
    "            print(\"\\n Filtering complete for 'Diagnosis-based caseness'...\")\n",
    "        elif caseness == 'rxnotdx':\n",
    "            print(\"\\n Filtering complete for 'Prescription-based caseness'...\")\n",
    "        elif caseness == 'multi':\n",
    "            print(\"\\n Filtering complete for 'Multinomial caseness'...\")\n",
    "        elif caseness == 'prescriptionVsDefinite':\n",
    "            print(\"\\n Filtering complete for 'Prescription-based -vs- Definite caseness'...\")\n",
    "        print(\"\\t\", len(filtered_featureSet_array.columns)-1, \" feature sets remain.\")\n",
    "        print(\"\\t\", n_fs_removed_lower + n_fs_removed_upper, \" feature sets removed, in total.\")\n",
    "        print(\"\\t\", n_fs_removed_lower, \" feature sets removed because of low prevalence.\")\n",
    "        print(\"\\t\", n_fs_removed_upper, \" feature sets removed because of high prevalence.\")\n",
    "    \n",
    "    # Return outputs\n",
    "    return [filtered_featureSet_array, fs_removed_lower, n_fs_removed_lower, fs_removed_upper, n_fs_removed_upper]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30efd775-da7e-486c-acc8-62c2c09ad9c7",
   "metadata": {},
   "source": [
    "### `databasefsboundaryreview()`: Output tables showing the count of patient records in which unique SNOMED-CT codes occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f5f077b6-47b3-4754-b994-daaa46fe2a35",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def databasefsboundaryreview(lower_bound, upper_bound):\n",
    "    global sql_variables\n",
    "    global sql_base\n",
    "    \n",
    "    more_sql_variables = \\\n",
    "    \"\"\"\n",
    "    DECLARE lower_bound INT64 DEFAULT \"\"\" + str(lower_bound) + \"\"\";\n",
    "    DECLARE upper_bound INT64 DEFAULT \"\"\" + str(upper_bound) + \"\"\";\n",
    "    \"\"\"\n",
    "    \n",
    "    sql_boundary_table = \\\n",
    "    \"\"\"\n",
    "    ,tbl_category_boundary AS\n",
    "    (\n",
    "    SELECT\n",
    "      DISTINCT snomedcode\n",
    "      ,CASE\n",
    "        WHEN count_patients_with_code < lower_bound THEN \"too infrequent (occurs in < \"\"\" + f'{lower_bound:,}' + \"\"\" patients' records)\"\n",
    "        WHEN count_patients_with_code <= upper_bound THEN \"within bounds\"\n",
    "        ELSE \"too frequent (occurs in > \"\"\" + f'{upper_bound:,}' + \"\"\" patients' records)\"\n",
    "      END AS cnt_SNOMED\n",
    "    FROM tbl_patients_per_code\n",
    "    ORDER BY cnt_SNOMED\n",
    "    )\n",
    "\n",
    "    SELECT\n",
    "      COUNT(cnt_SNOMED) AS This_many_codes__\n",
    "      ,cnt_SNOMED AS __occur_this_often\n",
    "    FROM tbl_category_boundary\n",
    "    GROUP BY cnt_SNOMED\n",
    "    ORDER BY This_many_codes__ DESC\n",
    "    \"\"\"\n",
    "    boundary_Table = client.query(sql_variables + more_sql_variables + sql_base + sql_boundary_table).to_dataframe()\n",
    "    display(boundary_Table)\n",
    "\n",
    "    # Prepare the table for extracting data.\n",
    "    boundary_Table.set_index('__occur_this_often', inplace = True)\n",
    "    n_within_bounds = int(boundary_Table.loc['within bounds'])\n",
    "    \n",
    "    return n_within_bounds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c65f056-f60b-4246-ae69-4f706b110e93",
   "metadata": {},
   "source": [
    "### `evaloutputs()`: Compute the evaluation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9c87092f-8174-4f8d-8c34-90aa46f626a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to compute the evalaution outputs. The function automatically saves the\n",
    "# contingency table and also returns it.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. vec_featureSet:      A column from a pandas.Dataframe containing the feature set\n",
    "#                         that needs evaluating.\n",
    "# 2. vec_caseness:        A column from a pandas.Dataframe containing the caseness\n",
    "#                         variable of interest.\n",
    "# 3. savelocation:        The folder location where the output should be saved.\n",
    "#\n",
    "# RETURNS\n",
    "# 1. prevalence:          The proportion of patients satisfying the definition of the\n",
    "#                         feature set.\n",
    "# 2. cba:                 Class balanced accuracy - the lower bound of the average\n",
    "#                         sensitivity and average positive predictive value\n",
    "#                         (a.k.a. precision) for all caseness values.\n",
    "# 3. oddsRatio:           The ratio of the odds of caseness given the presence of feature\n",
    "#                         set, to the odds of CMHD given the absence of the feature set.\n",
    "#                         It can also be thought of as the multiplicative difference\n",
    "#                         between correct and incorrect classification.\n",
    "# 4. ppv:                 The proportion of patients satisfying the definition of the\n",
    "#                         feature set who satisfy the caseness.\n",
    "# 5. npv:                 The proportion of patients who do not satisfy the definition\n",
    "#                         of the feature set who do not satisfy the caseness.\n",
    "# 6. tn:                  The count of true negatives, i.e. the count of patients whose \n",
    "#                         feature-set value and caseness value are both zero.\n",
    "# 7. fn:                  The count of false negatives, i.e. the count of patients whose \n",
    "#                         feature-set value is zero but whose caseness value is one.\n",
    "# 8. fp:                  The count of false positives, i.e. the count of patients whose \n",
    "#                         feature-set value is one but whose caseness value is zero.\n",
    "# 9. tp:                  The count of true positives, i.e. the count of patients whose \n",
    "#                         feature-set value and caseness value are both one.\n",
    "#\n",
    "def evaloutputs(vec_featureSet,\n",
    "                vec_caseness):\n",
    "    # ## Assess argument validty.\n",
    "    \n",
    "    # Check that both vectors are the same length.\n",
    "    if len(vec_featureSet) != len(vec_caseness):\n",
    "        print(\"\\n**\",\n",
    "              \"Feature-set and caseness vectors are of different lengths.\",\n",
    "             \"**\\n\")\n",
    "        return\n",
    "    \n",
    "    # ## Contingency table.\n",
    "    # Make contingency table.\n",
    "    contingencyTable = \\\n",
    "        pandas.crosstab(\n",
    "            index = vec_featureSet,\n",
    "            columns = vec_caseness\n",
    "    )\n",
    "        \n",
    "    \n",
    "    # Extract components of contingency table\n",
    "    tn = contingencyTable.loc[0,0]\n",
    "    fn = contingencyTable.loc[0,1]\n",
    "    fp = contingencyTable.loc[1,0]\n",
    "    tp = contingencyTable.loc[1,1]\n",
    "    \n",
    "    # ## Compute outputs.\n",
    "    \n",
    "    # Prevalence value per 1,000.\n",
    "    #\n",
    "    # I use 1 minus the prevalence of zeros because that\n",
    "    # combines all the possibly-many values that indicate\n",
    "    # the presence of the feature set.\n",
    "    prevalence = \\\n",
    "        (1 - (sum(vec_featureSet == 0) / len(vec_featureSet))) * 10\n",
    "    if prevalence < 0.01:\n",
    "         prevalence = '< 0.01'\n",
    "    else:\n",
    "         prevalence = round(prevalence, 2)\n",
    "    \n",
    "    # Class balance accuracy.\n",
    "    cba = \\\n",
    "        round( 0.5 * \\\n",
    "              ( (tp / max( (tp + fn), (tp + fp) ) ) + \\\n",
    "               (tn / max( (tn + fp), (tn +fn) ) ) ), 2)\n",
    "    if cba < 0.01:\n",
    "        cba = '< 0.01'\n",
    "    \n",
    "    # Odds ratio.\n",
    "    if min( (tp * tn) , (fp * fn) ) == 0:\n",
    "        oddsRatio = 'Undefined: One of the odds is zero.'\n",
    "    else:\n",
    "        oddsRatio = round( (tp * tn) / (fp * fn), 2)\n",
    "    \n",
    "    # Positive predictive value.\n",
    "    ppv = 0.00 if (tp + fp) == 0 else tp / (tp + fp)\n",
    "    if ppv > 0 and ppv < 0.01:\n",
    "        ppv = '< 0.01'\n",
    "    elif ppv < 1 and ppv > 0.999:\n",
    "        ppv = '\\u2248 1.00'\n",
    "    else:\n",
    "         ppv = round(ppv, 2)\n",
    "         \n",
    "    # Negative predictive value.\n",
    "    npv = 0.00 if (tn + fn) == 0 else tn / (tn + fn)\n",
    "    if npv > 0 and npv < 0.01:\n",
    "        npv = '< 0.01'\n",
    "    elif npv < 1 and npv > 0.999:\n",
    "        npv = '\\u2248 1.00'\n",
    "    else:\n",
    "         npv = round(npv, 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return prevalence, cba, oddsRatio, ppv, npv, tn, fn, fp, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961cd6f-146b-44d2-b653-ccc0dfacfb65",
   "metadata": {},
   "source": [
    "### `evaleachcaseness()`: Iterate through the three caseness variables with a given feature set, and call the evalaution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4613c195-2446-4784-beaa-4bf934be918f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to iterate through the three caseness variables with a given feature set\n",
    "# and call the evaluation function, evaloutputs.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. vec_featureSet:      A column from a pandas.Dataframe containing the feature set\n",
    "#                         that needs evaluating.\n",
    "# 2. vec_caseness:        A column from a pandas.Dataframe containing the caseness\n",
    "#                         variable of interest.\n",
    "# 3. savelocation:        The folder location where the output should be saved.\n",
    "#\n",
    "# RETURNS\n",
    "# n/a\n",
    "#\n",
    "\n",
    "def evaleachcaseness(vec_featureSet,\n",
    "                     array_caseness,\n",
    "                     savelocation = None):\n",
    "    counter = 0\n",
    "    for vec_caseness in array_caseness[[\"CMHD_dx_and_rx\", \"CMHD_rx_not_dx\",\"CMHD_control\"]]:\n",
    "        contingencyTable,\n",
    "        prevalence[counter],\n",
    "        cba[counter],\n",
    "        oddsRatio[counter],\n",
    "        ppv[counter],\n",
    "        npv[counter] = \\\n",
    "            evaloutputs(vec_featureSet,\n",
    "                        vec_caseness,\n",
    "                        savelocation = None)\n",
    "        counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc34d6a-9745-4b0c-902e-0f6d478fd3ea",
   "metadata": {},
   "source": [
    "### `mutlinomRepresentation()`: Compute the multinomial representation of a feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "acf2b93b-bbc0-4d3a-995d-9d75763f2bee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to compute the multinomial representation of a feature set.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. var_vals:      An n-by-fs pandas.Dataframe of n patients and fs\n",
    "#                   features, containing the feature sets that we want to\n",
    "#                   compress into a single, multinomial representation.\n",
    "#\n",
    "# RETURNS\n",
    "# 1. featureSet:    An n-by-1 pandas.Dataframe containing a multinomial\n",
    "#                   representation of the inputted feature sets.\n",
    "# 2. next_iter:     An indicator variable that is used by the parent\n",
    "#                   function featuresetmi().\n",
    "#\n",
    "def mutlinomRepresentation(var_vals):\n",
    "    # Check that the variables have more than three values and\n",
    "    # only progress if False.\n",
    "#    for i_col in range(var_vals.shape[1]-1):\n",
    "#        unique_feature_vals = var_vals.iloc[:, i_col].drop_duplicates()\n",
    "#        if (len(unique_feature_vals) > 3):\n",
    "#            print(\"\\n** Error: At least one of the\",\n",
    "#                  \"component features has more than\",\n",
    "#                  \"three values so the multinomial\",\n",
    "#                  \"representation will not be computed.**\")\n",
    "#            print(f'Offending variable: {var_vals.columns.values[i_col]}')\n",
    "#            unique_feature_vals\n",
    "#            next_iter = True\n",
    "#            return 0, next_iter\n",
    "\n",
    "    # Get all combinations of values of the component features\n",
    "    # and define feature set values for each multinomial combination.\n",
    "    feature_combins = var_vals.drop_duplicates()\n",
    "    feature_combins =\\\n",
    "        pandas.DataFrame(data = feature_combins, columns = var_vals.columns)\\\n",
    "        .reset_index()\\\n",
    "        .drop(['index'], axis = 1)\n",
    "    feature_combins['multinom_vals'] = feature_combins.index\n",
    "\n",
    "\n",
    "    # Define a vector indicating the feature set value.\n",
    "    myMerge =\\\n",
    "        pandas.merge(\n",
    "            var_vals,\n",
    "            feature_combins,\n",
    "            how = 'left',\n",
    "            on = list(var_vals.columns.values)\n",
    "    )\n",
    "\n",
    "    # Extract multinomial representation as output variable.\n",
    "    featureSet = myMerge['multinom_vals']\n",
    "    next_iter = False\n",
    "    return featureSet, next_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871c6b3-0549-44fc-97f0-65a86505ca58",
   "metadata": {
    "tags": []
   },
   "source": [
    "### `featuresetmi()`: Calculate two-way mutual information for a features of order m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc083a26-9003-4931-89bc-3f6c81516b21",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to calculate the two-way mutual information for a feature set.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. featureSet_array:   An n-by-fs pandas.Dataframe of n patients and fs feature\n",
    "#                        sets, or an fs-by-1 pandas.Dataframe containing the names\n",
    "#                        of feature sets. If the fs-by-1 dataframe, then it is\n",
    "#                        assumed that the feature sets are SNOMED-CT codes that \n",
    "#                        can be queried in the WHERE clause of BigQuery syntax.\n",
    "# 2. casenessVector:     A column from a pandas.Dataframe containing the caseness\n",
    "#                        variable of interest.\n",
    "# 3. m:                  The order of feature set to be tested. 1 = Individuals, \n",
    "#                        2 = Pairs, 3 = Triplets.\n",
    "# 4. savelocation:       The folder location where the output should be saved.\n",
    "# 5. representation:     A choice of {'all', 'any', 'multi'} where 'all' = the feature set\n",
    "#                        value is 1 when all components are 1, or 0 otherwise; 'any' =\n",
    "#                        the feature set value is 1 when any component is 1, or 0 otherwise;\n",
    "#                        and 'multi' = the feature set values represent every combination\n",
    "#                        of components' values.\n",
    "# 6. source:             The source of the feature set: {'database', 'clinicial',\n",
    "#                        'literature', 'interviews', 'PPI', 'combined'}.\n",
    "# 7. df_ppl_and_codes:   An optional argument that contains a list of all patients\n",
    "#                        and all SNOMEDCT-CT codes that they have that are within\n",
    "#                        the prevalence bounds.\n",
    "# 8. verbose:            An optional argument to indicate whether the user wants\n",
    "#                        feedback on how many feature sets were removed and saved.\n",
    "\n",
    "#\n",
    "# RETURNS\n",
    "# n/a\n",
    "#  \n",
    "def featuresetmi(featureSet_array,\n",
    "                 casenessVector,\n",
    "                 m = None,\n",
    "                 saveloc = None,\n",
    "                 representation = None,\n",
    "                 src = None,\n",
    "                 df_ppl_and_codes = None,\n",
    "                 verbose = False):\n",
    "    # ## Assess argument validty.\n",
    "    \n",
    "    # Check order of feature set. If not provided,\n",
    "    # default to m = 1.\n",
    "    global order_label\n",
    "    if m == None:\n",
    "        order_int = 1\n",
    "        order_label = \"Individuals\"\n",
    "        print(\"\\nNo value for m provided.\" +\n",
    "              \"\\n...Default value of m = 1 will be used.\")\n",
    "    elif m == 1:\n",
    "        order_int = m\n",
    "        order_label = \"Individuals\"\n",
    "    elif m == 2:\n",
    "        order_int = m\n",
    "        order_label = \"Pairs\"\n",
    "    elif m == 3:\n",
    "        order_int = m\n",
    "        order_label = \"Triplets\"\n",
    "    else:\n",
    "        print(\"\\n** Error: Integer value between 1\",\n",
    "              \"and 3 not supplied for m.**\\n\")\n",
    "        return\n",
    "\n",
    "    # Check and set save location.\n",
    "    \n",
    "    if saveloc == None:\n",
    "        global savelocation\n",
    "        savelocation = \\\n",
    "           (\"Mutual information saves/\"+\\\n",
    "            order_label)\n",
    "        print(\"\\nNo save location provided.\" +\n",
    "              \"\\n...Defaulting to ~/\" + savelocation)  \n",
    "\n",
    "    # ## Check encoding. If not provided, \n",
    "    # ## default to OR encoding.\n",
    "    global representation_label\n",
    "    if representation == None:\n",
    "        representation_label = \"ALL\"\n",
    "        print(\"\\nNo representation provided.\" +\n",
    "              \"\\n...Defaulting to '\" + representation_label + \"' representation.\")\n",
    "    elif representation == \"all\":\n",
    "        representation_label = \"ALL\"\n",
    "    elif representation == \"any\":\n",
    "        representation_label = \"ANY\"\n",
    "    elif representation == \"multi\":\n",
    "        representation_label = \"MULTI\"\n",
    "    else:\n",
    "        print(\"\\n** Error: Representation value from \",\n",
    "              \"{'all', 'any', 'multi'} not provided.**\\n\")\n",
    "        return\n",
    "    \n",
    "    # ## Check the source argument is provided.\n",
    "    \n",
    "    if src == None:\n",
    "        print(\"\\n** Error: No source argument provided.\",\n",
    "              \"**\\n\")\n",
    "        return\n",
    "    else:\n",
    "        global source\n",
    "        source = src\n",
    "    \n",
    "    \n",
    "    # ## Set save string for particular caseness variable.\n",
    "    global caseness_label\n",
    "    caseness_type = casenessVector.columns.values[-1]\n",
    "    if caseness_type == 'CMHD_dx_and_rx': \n",
    "        caseness_label = 'DxAndRx'\n",
    "    elif caseness_type == 'CMHD_dx_not_rx': \n",
    "        caseness_label = 'DxNotRx'\n",
    "    elif caseness_type == 'CMHD_rx_not_dx': \n",
    "        caseness_label = 'RxNotDx'\n",
    "    elif caseness_type == 'CMHD_multi': \n",
    "        caseness_label = 'Multinomial'\n",
    "    elif caseness_type == 'CMHD_control': \n",
    "        caseness_label = 'Control'\n",
    "    \n",
    "    print(\"\\n\\n\\n****************************************\")  \n",
    "    print(\"Calculating mutual information values...\")\n",
    "    \n",
    "    # Instantiate specific storage for mutual information.\n",
    "    global featureSet_MI\n",
    "    featureSet_MI = \\\n",
    "        pandas.DataFrame(columns = ['Feature_set', 'Normalised_mutual_information'])\n",
    "\n",
    "    # Instantiate batch number.\n",
    "    global batch \n",
    "    batch = 0\n",
    "\n",
    "    # Instantiate tally of feature sets that are dropped due to low entropy.\n",
    "    global drop_tally\n",
    "    drop_tally = 0\n",
    "\n",
    "    # Define entropy of the particular caseness variable.\n",
    "    global entropy_caseness\n",
    "    entropy_caseness = \\\n",
    "        scipy.stats.entropy(casenessVector.value_counts())\n",
    "    \n",
    "    # Check if the supplied feature set array is an n-by-fs array\n",
    "    # of feature sets, or an fs-by-1 vector of feature-set names.\n",
    "    # If it is the fs-by-1 vector, then pass all arguments to the\n",
    "    # appropriate function; else, continue with the code.\n",
    "    if featureSet_array.shape[1] == 1:\n",
    "        # Display messages to user.\n",
    "        if verbose == True:\n",
    "            print(\"Overriding 'verbose == True' because there are too many feature sets.\",\n",
    "                 \"Messages would become unweidly and slow down processing.\")\n",
    "            verbose = False\n",
    "            # Inform users about limitation with database feature sets.\n",
    "            if representation_label != \"ALL\":\n",
    "                print(\"Only representation = 'all' is available for database feature sets.\")\n",
    "                return\n",
    "        \n",
    "        # The IPYNB file has already been run in this notebook but I'm repeating the run based\n",
    "        # on guidance from this blog:\n",
    "        # https://medium.com/@grvsinghal/speed-up-your-python-code-using-multiprocessing-on-windows-and-jupyter-or-ipython-2714b49d6fac\n",
    "        %run 'UNSEEN_helper_functions.ipynb'\n",
    "\n",
    "        # Define a function to portion my iterable.\n",
    "        #\n",
    "        # https://stackoverflow.com/questions/51446327/python-3-generator-comprehension-to-generate-chunks-including-last\n",
    "        def portion_maker(gen, portion_size):\n",
    "            it = iter(gen)\n",
    "            while True:\n",
    "                portion = [*itertools.islice(it, 0, portion_size)]\n",
    "                if portion:\n",
    "                    yield portion\n",
    "                else:\n",
    "                    break\n",
    "        \n",
    "        # Do the main job of assessing the feature sets.\n",
    "        if __name__ ==  '__main__': \n",
    "            gen = itertools.combinations(df_fs_database.snomedcode, m)\n",
    "            portion_size = 7\n",
    "            n_workers = 4\n",
    "            for portion in portion_maker(gen, portion_size):\n",
    "                print(f\"This batch is {portion}.\")\n",
    "                list(Pool(n_workers).starmap(processdatabasefs, portion))\n",
    "        \n",
    "        # Calculate the total count of feature sets processed.\n",
    "        count_fs = sum(1 for _ in itertools.combinations(featureSet_array.snomedcode, m))\n",
    "    else:            \n",
    "        # Ensure feature-set and casenesss values are matched for person_id.\n",
    "        global full_array\n",
    "        full_array = pandas.merge(casenessVector,\n",
    "                                  featureSet_array,\n",
    "                                  on = 'person_id',\n",
    "                                  how = 'left')  \n",
    "\n",
    "        # Define the m-way tuples of features sets as a numpy array. We will loop\n",
    "        # through the rows of this array to create the feature sets.\n",
    "        combins = \\\n",
    "            itertools.combinations(\n",
    "                featureSet_array.columns[featureSet_array.columns != 'person_id'],\n",
    "                order_int)\n",
    "        count_fs = len(list(itertools.combinations(\n",
    "                featureSet_array.columns[featureSet_array.columns != 'person_id'],\n",
    "                order_int)))\n",
    "        \n",
    "        # The IPYNB file has already been run in this notebook but I'm repeating the run based\n",
    "        # on guidance from this blog:\n",
    "        # https://medium.com/@grvsinghal/speed-up-your-python-code-using-multiprocessing-on-windows-and-jupyter-or-ipython-2714b49d6fac\n",
    "        %run 'UNSEEN_helper_functions.ipynb'\n",
    "        \n",
    "        # Do the main job of assessing the feature sets.\n",
    "        if __name__ ==  '__main__':\n",
    "            portion_size = 7\n",
    "            n_workers = 4\n",
    "            print('Parallel processing of feature sets has begun...')\n",
    "            mypool = Pool(n_workers)\n",
    "            list(mypool.starmap(processfs, combins))\n",
    "            mypool.close()\n",
    "            mypool.join() # Close down the pool to release resources. https://superfastpython.com/shutdown-the-multiprocessing-pool-in-python/\n",
    "            print('\\t...parallel processing of feature sets has ended.')\n",
    "        \n",
    "                    \n",
    "    # Increment counter.\n",
    "    batch += 1\n",
    "    # Final save.\n",
    "    if len(featureSet_MI) != 0:\n",
    "        current_dir = os.getcwd()\n",
    "        #print(current_dir + \"/\" +savelocation)\n",
    "        os.chdir(current_dir + \"/\" + savelocation)\n",
    "        pyarrow.parquet.write_table(pyarrow.Table.from_pandas(featureSet_MI),\n",
    "                                    source + \"_\" +\n",
    "                                    caseness_label + \"_\" +\n",
    "                                    representation_label + \"_\" + \n",
    "                                    \"batch\" + \n",
    "                                    str(batch) + \n",
    "                                    \".parquet\")\n",
    "        os.chdir(current_dir)\n",
    "        # ## I commented out saving the CSV files to improve runtimes.S\n",
    "        #featureSet_MI.to_csv(savelocation + \"/\" +\n",
    "        #                     source + \"_\" +\n",
    "        #                     caseness_label + \"_\" +\n",
    "        #                     representation_label + \"_\" +\n",
    "        #                     \"batch\" + \n",
    "        #                     str(batch) + \n",
    "        #                     \".csv\", index = False)\n",
    "\n",
    "    # Feedback messages.\n",
    "    print(\"...\\n\")\n",
    "    print(str(batch), \"batch(es) of feature sets processed.\")\n",
    "    print(str(drop_tally), \"/\",\n",
    "          str(count_fs),\n",
    "          \"feature sets dropped due to insufficient normalised mutual information.\")\n",
    "    print(\"****************************************\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f35bcd0a-e401-43ba-affa-e90eb10333d0",
   "metadata": {},
   "source": [
    "### `processdatabasefs()`: Calculate two-way mutual information for *database* features sets, specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75991e67-6819-466a-9c2e-915478d029db",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def processdatabasefs(*snomedcodes):\n",
    "    global drop_tally\n",
    "    global batch\n",
    "    global featureSet_MI\n",
    "    global df_ppl_and_codes\n",
    "    \n",
    "    # Define the feature set.\n",
    "    fs = [snomedcodes]\n",
    "    \n",
    "    # Name the feature set.\n",
    "    name_var = '-'.join(map(str,list(fs)))\n",
    "\n",
    "    # Check to see if anyone in the cohort has the codes in their record.\n",
    "    df_temp = df_ppl_and_codes\n",
    "    df_temp['ary_pAc_in_fsdf'] = df_ppl_and_codes.snomedcode.isin(fs)\n",
    "    do_patients_have_all_query_codes = \\\n",
    "        (df_temp.groupby(['person_id']).ary_pAc_in_fsdf.count() == len(fs)).astype(int)\n",
    "\n",
    "    # Join feature set with caseness_array to match person_id.\n",
    "    fs_val = pandas.merge(caseness_array,\n",
    "                          do_patients_have_all_query_codes,\n",
    "                          how = 'left',\n",
    "                          on = 'person_id')\n",
    "\n",
    "    # Calculate the normalised mutual information between the feature set and\n",
    "    # caseness variable. The mutual information is normalised to the caseness'\n",
    "    # entropy. This ensures that:\n",
    "    # - 0 means no mutual information (in other words, the feature set has done nothing to improve our certainty).\n",
    "    # - 1 means the feature set is a perfect proxy for the caseness variable.\n",
    "    # - 0.5 means the feature set has halved the uncertainty of the caseness variable.\n",
    "    f_MI = sklearn.metrics.mutual_info_score(fs_val, full_array[caseness_array.columns[1]])\n",
    "    f_nMI = f_MI / entropy_caseness\n",
    "\n",
    "    # Test the feature set's normalised mutual information.\n",
    "    if f_nMI < 0: #< 0.8: # This commenting-out is about removing any threshold on nMI scores. The team want to see all of them.\n",
    "        drop_tally += 1\n",
    "    else:            \n",
    "        featureSet_MI.loc[len(featureSet_MI),:] = name_var, f_nMI\n",
    "\n",
    "    if len(featureSet_MI) > 9:\n",
    "            # Increment batch.\n",
    "            batch += 1\n",
    "\n",
    "            # Make an interim save of results.\n",
    "            current_dir = os.getcwd()\n",
    "            #print(current_dir + savelocation)\n",
    "            os.chdir(current_dir + \"/\" + savelocation)\n",
    "            pyarrow.parquet.write_table(pyarrow.Table.from_pandas(featureSet_MI),   \n",
    "                                        source + \"_\" +\n",
    "                                        caseness_label + \"_\" +\n",
    "                                        representation_label + \"_\" + \n",
    "                                        \"batch\" + \n",
    "                                        str(batch) + \n",
    "                                        \".parquet\")\n",
    "            os.chdir(current_dir)\n",
    "            # ## I commented out saving the CSV files to improve runtimes.S\n",
    "            #featureSet_MI.to_csv(savelocation + \"/\" +\n",
    "            #                     source + \"_\" +\n",
    "            #                     caseness_label + \"_\" +\n",
    "            #                     representation_label + \"_\" +\n",
    "            #                     \"batch\" + \n",
    "            #                     str(batch) + \n",
    "            #                     \".csv\", index = False)\n",
    "            #print(\"\\nInterim save made\")\n",
    "            # Instantiate new storage.\n",
    "            featureSet_MI = \\\n",
    "                pandas.DataFrame(columns = ['Feature_set', 'Mutual_information'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac0f118-d0c0-42ea-b2a0-1f4386af3b0d",
   "metadata": {},
   "source": [
    "### `processfs()`: Calculate two-way mutual information for *non-database* features sets, specifically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd87331b-8f42-4ec3-815a-b2c40f9487fa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def processfs(*i_combin_names):\n",
    "    global full_array\n",
    "    global representation_label\n",
    "    global drop_tally\n",
    "    global batch\n",
    "    global featureSet_MI\n",
    "    global savelocation\n",
    "    global source\n",
    "    global casenesss_label\n",
    "    \n",
    "    # Define an array indicating the feature set value.\n",
    "    var_vals = full_array[list(i_combin_names)]\n",
    "    \n",
    "    # Name the feature set.\n",
    "    if representation_label == 'ALL':\n",
    "        name_var = 'ALL OF   ' + '   AND   '.join(var_vals.columns.values)\n",
    "    elif representation_label == 'ANY':\n",
    "        name_var = 'EITHER    ' + '   OR   '.join(var_vals.columns.values)\n",
    "    elif representation_label == 'MULTI':\n",
    "        name_var = 'EVERY COMBINATION OF    ' + '   ,   '.join(var_vals.columns.values)\n",
    "\n",
    "    # Transform feature sets into the chosen representation.\n",
    "    if representation_label == \"ALL\":\n",
    "        fs_val = var_vals.all(True)\n",
    "    elif representation_label == \"ANY\":\n",
    "        fs_val = var_vals.any(True)\n",
    "    elif representation_label == \"MULTI\":\n",
    "        fs_val, next_iter = mutlinomRepresentation(var_vals)\n",
    "        if next_iter:\n",
    "            drop_tally += 1\n",
    "\n",
    "\n",
    "    # Calculate the normalised mutual information between the feature set and caseness\n",
    "    # variable. The mutual information is normalised to the caseness' entropy. This \n",
    "    # ensures that:\n",
    "    # - 0 means no mutual information (in other words, the feature set has done nothing to improve our certainty).\n",
    "    # - 1 means the feature set is a perfect proxy for the caseness variable.\n",
    "    # - 0.5 means the feature set has halved the uncertainty of the caseness variable.\n",
    "    f_MI = sklearn.metrics.mutual_info_score(fs_val, full_array.iloc[:,1])\n",
    "    f_nMI = f_MI / entropy_caseness\n",
    "    \n",
    "    # Test the feature set's normalised mutual information against a threshold.\n",
    "    if f_nMI < 0: #< 0.8: # This commenting-out is about removing any threshold on nMI scores. The team want to see all of them.\n",
    "        drop_tally += 1\n",
    "    else:            \n",
    "        # Store the name and mutual information value.\n",
    "        featureSet_MI.loc[len(featureSet_MI),:] = name_var, round(f_nMI, 7)\n",
    "\n",
    "    if len(featureSet_MI) > 9:\n",
    "            # Increment batch.\n",
    "            batch += 1\n",
    "\n",
    "            # Make an interim save of results.\n",
    "            current_dir = os.getcwd()\n",
    "            #print(current_dir + \"/\" + savelocation)\n",
    "            os.chdir(current_dir + \"/\" + savelocation)\n",
    "            pyarrow.parquet.write_table(pyarrow.Table.from_pandas(featureSet_MI),   \n",
    "                                        source + \"_\" +\n",
    "                                        caseness_label + \"_\" +\n",
    "                                        representation_label + \"_\" + \n",
    "                                        \"batch\" + \n",
    "                                        str(batch) + \n",
    "                                        \".parquet\")\n",
    "            os.chdir(current_dir)\n",
    "            # ## I commented out saving the CSV files to improve runtimes.\n",
    "            #featureSet_MI.to_csv(savelocation + \"/\" +\n",
    "            #                     source + \"_\" +\n",
    "            #                     caseness_label + \"_\" + \n",
    "            #                     representation_label + \"_\" + \n",
    "            #                     \"batch\" + \n",
    "            #                     str(batch) + \n",
    "            #                     \".csv\", index = False)\n",
    "            #print(\"\\nInterim save made\")\n",
    "            # Instantiate new storage.\n",
    "            featureSet_MI = \\\n",
    "                pandas.DataFrame(columns = ['Feature_set', 'Normalised_mutual_information'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad0fbda-4db7-463f-b3da-ec44d8b2a878",
   "metadata": {},
   "source": [
    "### `init_worker()`: Initialize worker processes for parallell processing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "264b4822-0892-4051-9d41-e7a93f8dc1ab",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def init_worker(shared_var):\n",
    "    global df_ppl_and_codes\n",
    "    # store argument in the global variable for this process\n",
    "    df_ppl_and_codes = shared_var\n",
    "    #print(f'df_ppl_and_codes.head() = {df_ppl_and_codes.head()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5696f968-e444-480e-851e-c470f213cd0d",
   "metadata": {},
   "source": [
    "### `chaoticlifeentropyfs()`: Calculate the entropy-based statistics for a patient's timeline of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbe3873-1692-4d0d-9932-07b9be77b82e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chaoticlifeentropyfs(pt_timeline):\n",
    "    '''\n",
    "    There are two categories of entropy-based feature sets for both appointments and did-not-attends:\n",
    "    Sequential\n",
    "    1.\tactiveInformation\n",
    "    2.\tentropyRate\n",
    "    Summative\n",
    "    3.\tspectralEntropy\n",
    "    4.\tsampleEntropy\n",
    "    5.\teoe (entropy of entropy)\n",
    "    6.\taverageEntropy\n",
    "    7.\tbubbleEntropy\n",
    "    Use the following parameters for all summative entropy statistics other than spectral entropy, which doesn't require them:\n",
    "    -\tobs = three-monthly count, enough to amass a period of use.\n",
    "    -\twindow breath (\"embedding dimension\") = 4, to indicate a year's worth of appointments.\n",
    "    -\twindow shift (\"embedding time delay\") = 1, to be sensitive to quarterly changes in behaviour.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Set parameters.\n",
    "    # ## Set warnings parameter to handle divide-by-zero issues with spectral entropy.\n",
    "    warnings.filterwarnings(\"error\")\n",
    "    # ## Window breath (\"embedding dimension\") = 4, to indicate a year's worth of appointments.\n",
    "    embeddingDimension = 4\n",
    "    # ## Window shift (\"embedding time delay\") = 1, to be sensitive to quarterly changes in behaviour.\n",
    "    embeddingTimeDelay = 1\n",
    "    # ## Length of the patient's timeline.\n",
    "    len_timeline = len(pt_timeline)\n",
    "    # Convert pt_timeline into a numpy.array.\n",
    "    pt_timeline = numpy.array(pt_timeline)\n",
    "    \n",
    "    # activeInformation\n",
    "    # ...\n",
    "    if len_timeline <= embeddingDimension:\n",
    "        activeInformation = None\n",
    "    else:\n",
    "        activeInformation = \\\n",
    "            pyinform.activeinfo.active_info(pt_timeline, k = embeddingDimension)\n",
    "    \n",
    "    # entropyRate\n",
    "    # ...\n",
    "    if len_timeline <= embeddingDimension:\n",
    "        entropyRate = None\n",
    "    else:\n",
    "        entropyRate = \\\n",
    "            pyinform.entropyrate.entropy_rate(pt_timeline, k = embeddingDimension)\n",
    "    \n",
    "    # spectralEntropy\n",
    "    # ...\n",
    "    if len_timeline <= 10:\n",
    "        spectralEntropy = None\n",
    "    else:\n",
    "        try:\n",
    "            spectralEntropy, _ = EntropyHub.SpecEn(pt_timeline)\n",
    "        except RuntimeWarning:\n",
    "            spectralEntropy = None\n",
    "    \n",
    "    # sampleEntropy\n",
    "    # ...\n",
    "    if len_timeline <= 10:\n",
    "        sampleEntropy = None\n",
    "    else:\n",
    "        sampleEntropy, _, _ = \\\n",
    "            EntropyHub.SampEn(pt_timeline, m = embeddingDimension, tau = embeddingTimeDelay)\n",
    "        sampleEntropy = sampleEntropy[-1]\n",
    "    \n",
    "    # eoe and averageEntropy\n",
    "    # ...\n",
    "    if len_timeline <= 10:\n",
    "        eoe = None\n",
    "        averageEntropy = None\n",
    "    else:\n",
    "        eoe, averageEntropy, _ = \\\n",
    "            EntropyHub.EnofEn(pt_timeline, tau = embeddingDimension, S = math.floor(len_timeline / 4) )\n",
    "\n",
    "    # bubbleEntropy\n",
    "    # ...\n",
    "    if len_timeline <= 10:\n",
    "        bubbleEntropy = None\n",
    "    else:\n",
    "        bubbleEntropy, _ = EntropyHub.BubbEn(pt_timeline, m = embeddingDimension, tau = embeddingTimeDelay)\n",
    "        bubbleEntropy = bubbleEntropy[-1]\n",
    "    \n",
    "    # Package the output.\n",
    "    ls_entropyBasedFeatureSets = \\\n",
    "        [\n",
    "        activeInformation\n",
    "        ,entropyRate\n",
    "        ,spectralEntropy\n",
    "        ,sampleEntropy\n",
    "        ,eoe\n",
    "        ,averageEntropy\n",
    "        ,bubbleEntropy\n",
    "        ]\n",
    "    \n",
    "    return ls_entropyBasedFeatureSets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f9a269b-76a0-486e-ba20-0d038a401eef",
   "metadata": {},
   "source": [
    "### NOT IN USE patienthassnomed(): Check whether a patient's record includes some particular SNOMED-CT codes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e53085-b9f6-4954-bfc1-8cc5c787e230",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def patienthassnomed(fs_df, snomedcode):\n",
    "    ary_pAc_in_fsdf = []\n",
    "    if math.isnan(i):\n",
    "        ary_pAc_in_fsdf.append(False)\n",
    "    else:\n",
    "        ary_pAc_in_fsdf.append((i == fs_df).any()[0])\n",
    "    return ary_pAc_in_fsdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f639109-1435-4cda-9578-c42428d0938c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NOT IN USE featuresetmi_database(): Calculate two-way mutual information for a *database* features of order m."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6338baa9-6297-451f-a8b8-b377f6ca282c",
   "metadata": {},
   "source": [
    "### NOT IN USE getfsarray(): Produce a database feature set for submission to `featuresetmi()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b430c11-3ed6-418d-982f-ad2c9faa2d70",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getfsarray(fs_df):\n",
    "    \n",
    "    # Instantiate a Google BigQuery client.\n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    # Define the BigQuery syntax.\n",
    "\n",
    "    sql_CTEs_body = \\\n",
    "    \"\"\"\n",
    "    # Make a table of person ID and their SNOMED-CT codes from the list of codes of interest.\n",
    "    WITH tbl_persons_and_codes AS\n",
    "    (\n",
    "    SELECT\n",
    "        DISTINCT person_id\n",
    "        ,src_snomedcode\n",
    "    FROM\n",
    "        yhcr-prd-phm-bia-core.CY_FDM_PrimaryCare_v5.tbl_SRCode\n",
    "    WHERE\n",
    "        src_snomedcode IN ('\"\"\" + '\\', \\''.join(map(str, fs_df['src_snomedcode'].to_list())) + \"\"\"')\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    sql_pivot = \\\n",
    "    \"\"\"\n",
    "        SELECT\n",
    "        CONCAT(\"SELECT person_id,\"\n",
    "               , STRING_AGG(CONCAT(\"CASE WHEN src_snomedcode='\",src_snomedcode,\"' THEN 1 ELSE 0 END AS `_\",src_snomedcode,\"`\")), \n",
    "            \" FROM `tbl_persons_and_codes`\",\n",
    "            \" GROUP BY person_id, src_snomedcode ORDER BY person_id\")\n",
    "    FROM (  SELECT DISTINCT src_snomedcode FROM `tbl_persons_and_codes` ORDER BY src_snomedcode  )\n",
    "    \"\"\"\n",
    "    sql = client.query(sql_CTEs_body + sql_pivot).to_dataframe()['f0_'].iloc[0]\n",
    "    \n",
    "    feastureSet_array = \\\n",
    "        client.query(sql_CTEs_body +\n",
    "                     sql).to_dataframe()\n",
    "    \n",
    "    return feastureSet_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085ccba-bc80-46aa-876d-713eb04bc686",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NOT IN USE calculatemi(): Do the work of calculating the MI between a given feature set and caseness variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a29e471c-97a9-4685-b584-d9058d7d8757",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculatemi(var_vals,\n",
    "                name_var,\n",
    "                representation_label,\n",
    "                drop_tally,\n",
    "                verbose):\n",
    "    \n",
    "    # Formulate the representation.\n",
    "    if representation_label == \"ALLrepresentation\":\n",
    "        fs_val = var_vals.all(True)\n",
    "    elif representation_label == \"MULTIrepresentation\":\n",
    "        fs_val, next_iter = mutlinomRepresentation(var_vals)\n",
    "        if next_iter:\n",
    "            return next_iter\n",
    "\n",
    "    # Calculate the mutual information between the feature set and\n",
    "    # caseness variable.\n",
    "    f_MI = sklearn.metrics.mutual_info_score(fs_val, full_array.iloc[:,-1])\n",
    "\n",
    "    if f_MI < entropy_caseness:\n",
    "        drop_tally += 1\n",
    "        if verbose == True:\n",
    "            print(\"Dropped\",name_var,\"because f_MI =\", f_MI)\n",
    "    else:            \n",
    "        # Store the name and mutual information value.\n",
    "        if verbose == True:\n",
    "            print(\"\\tSaved\",name_var,\"because f_MI =\", f_MI)\n",
    "        featureSet_MI.loc[len(featureSet_MI),:] = name_var, f_MI\n",
    "\n",
    "    if len(featureSet_MI) > 9:\n",
    "            # Increment batch.\n",
    "            batch += 1\n",
    "\n",
    "            # Make an interim save of results.\n",
    "            featureSet_MI.to_csv(savelocation +\n",
    "                              representation_label + \"_\" +\n",
    "                              \"_batch\" + \\\n",
    "                              str(batch) + \\\n",
    "                              \".csv\", index = False)\n",
    "            print(\"\\nInterim save made\")\n",
    "            # Instantiate new storage.\n",
    "            featureSet_MI = \\\n",
    "                pandas.DataFrame(columns = ['Feature set', 'Mutual information'])"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-1.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-1:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
