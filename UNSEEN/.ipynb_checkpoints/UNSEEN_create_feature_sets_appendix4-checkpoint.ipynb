{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17fa9c5d-6626-4c1e-b228-3ff73515057a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Add feature sets that are combinations of the families of feature sets.\n",
    "\n",
    "The purpose of this notebook is to append feature sets that are defined by combinations of feature-set families.\n",
    "\n",
    "This notebook is expected to be called by its parent `UNSEEN_create_feature_sets.ipynb`. It will not run without the requisite loaded during the parent notebook.\n",
    "\n",
    "All possble combintions of feature-set families are considered. The feature set families are:\n",
    "- 'antecedent'                   : (coded as 'A')\n",
    "- 'concurrent'                   : (coded as 'C')\n",
    "- 'serviceUse'                   : (coded as 'S')\n",
    "- 'treatment'                    : (coded as 'T')\n",
    "- 'inconsistency'                : (coded as 'K')\n",
    "- 'patternsOfPrescription'       : (coded as 'P')\n",
    "- 'relevantPrescriptions'        : (coded as 'R')\n",
    "- 'antipsychoticsPrescription'   : (coded as 'Y')\n",
    "\n",
    "Each feature-set family has multiple versions indicating the level of presence:\n",
    "- 'none'     : (coded as '0') none of the family's component feature sets are present in the patient's record.\n",
    "- 'notNone'  : (coded as '1') none of the family's component feature sets are present in the patient's record.\n",
    "- 'few'      : (coded as '2') the lowest level of presence of the family's component feature sets are in the patient's record.\n",
    "- 'some'     : (coded as '3') the modest level of presence of the family's component feature sets are in the patient's record.\n",
    "- 'many'     : (coded as '4') the highest level of presence of the family's component feature sets are in the patient's record.\n",
    "- 'wildcard' : (coded as 'x') the level of presence is ignored for this family, in the particular feature-set combination.\n",
    "\n",
    "As an example, a feature set entitled `A0_C1_Sx_Tx_Kx_Px_Rx_Yx` represents the patients that have no antecedent feature sets (A0), notNone concurrent feature sets (C1), and any presence/absence - zero of otherwise - of the other families' feature sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3918947a-258d-4d7c-be1c-61d6074d7301",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Refresh store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69265487-9cff-438b-a37c-b14976964d05",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get helper functions.\n",
    "%run 'UNSEEN_helper_functions.ipynb'\n",
    "# Refresh stored variables, if they are present.\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5717c07a-2e86-409f-9250-5c70ebfacd36",
   "metadata": {},
   "source": [
    "## Create all combinations of family flavours."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9574aa3-427f-49a9-8bfc-2a1169e02ea4",
   "metadata": {},
   "source": [
    "### FOR loop implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b29e5147-d77f-4f96-9962-185b14cd3f89",
   "metadata": {},
   "source": [
    "#### Definte the list of combinations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "537ac18f-f0b9-460d-9caa-f22f93dcf9a7",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'family_combins' (list)\n"
     ]
    }
   ],
   "source": [
    "family_combins = []\n",
    "A_cols = list(nafsm_family_membership.filter(regex = 'antecedent').columns) + ['wildcard']\n",
    "C_cols = list(nafsm_family_membership.filter(regex = 'concurrent').columns) + ['wildcard']\n",
    "S_cols = list(nafsm_family_membership.filter(regex = 'serviceUse').columns) + ['wildcard']\n",
    "T_cols = list(nafsm_family_membership.filter(regex = 'treatment').columns) + ['wildcard']\n",
    "K_cols = list(nafsm_family_membership.filter(regex = 'inconsistency').columns) + ['wildcard']\n",
    "P_cols = list(nafsm_family_membership.filter(regex = 'patternsOfPrescription').columns) + ['wildcard']\n",
    "R_cols = list(nafsm_family_membership.filter(regex = 'relevantPrescriptions').columns) + ['wildcard']\n",
    "Y_cols = list(nafsm_family_membership.filter(regex = 'antipsychotics').columns) + ['wildcard']\n",
    "for i_antecedent in A_cols:\n",
    "    for i_concurrent in C_cols:\n",
    "        for i_serviceUse in S_cols:\n",
    "            for i_treatment in T_cols:\n",
    "                for i_inconsistency in K_cols:\n",
    "                    for i_patternsOfPrescription in P_cols:\n",
    "                        for i_relevantPrescriptions in R_cols:\n",
    "                            for i_antipsychotics in Y_cols:\n",
    "                                family_combins.append([i_antecedent, i_concurrent, i_serviceUse, i_treatment, i_inconsistency,\n",
    "                                                       i_patternsOfPrescription, i_relevantPrescriptions, i_antipsychotics])\n",
    "%store family_combins"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25e36b1-22fc-48cd-bbe4-3f88df3ceda5",
   "metadata": {},
   "source": [
    "#### Define some look up dictionaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4d400aaf-fd21-449f-9fc3-8e730973eedf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instantiate a feature-set storage dictionary.\n",
    "new_fs_dict = {'person_id' : nafsm_family_membership.person_id}\n",
    "\n",
    "# Set lookup of family names.\n",
    "family_lookup_list = ['antecedent', 'concurrent', 'serviceUse', 'treatment', 'inconsistency', 'patternsOfPrescription', 'relevantPrescriptions', 'antipsychoticsPrescription']\n",
    "\n",
    "# Set mapping dictionary for count level to count-level code.\n",
    "level_options = \\\n",
    "        {'none' : '0',\n",
    "         'notNone'  : '1',\n",
    "         'few'  : '2',\n",
    "         'some' : '3',\n",
    "         'many' : '4',\n",
    "         'wildcard' : 'x'\n",
    "        }\n",
    "\n",
    "# Set mapping dictionary for family name to family code.\n",
    "family_options = \\\n",
    "            {'antecedent' : 'A',\n",
    "             'concurrent'  : 'C',\n",
    "             'serviceUse'  : 'S',\n",
    "             'treatment' : 'T',\n",
    "             'inconsistency' : 'K',\n",
    "             'patternsOfPrescription' : 'P',\n",
    "             'relevantPrescriptions' : 'R',\n",
    "             'antipsychoticsPrescription' : 'Y'\n",
    "            }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42541cbe-e104-4640-aefb-b68769c3cb6e",
   "metadata": {},
   "source": [
    "#### Define some storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fe8edf6b-92a5-42f1-b4b8-ed2c406193fe",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Storage of the evaluation of the feature sets.\n",
    "# ## Unlike all other feature sets, these combinations will be evaluated in the same loop that they are created. I \n",
    "# ## made this decision because the GoogleCloudPlatform python kernel kept dying trying to save so many feature sets\n",
    "# ## that turned out to be non-informative anyway.\n",
    "ls_output1 = []\n",
    "\n",
    "# Storage for list of names of non-informative feture sets.\n",
    "discarded_because_only_one_value1 = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bb7275a-7362-46c5-9115-82cfa3a1ca2d",
   "metadata": {},
   "source": [
    "#### Do the work."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2bc95d91-f8de-4662-8dfa-319b5e01e8af",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d67183b0168746d097aaab9f637f2005",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ? feature sets/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 8724.343080282211 to process.\n",
      "37826 feature sets contained sufficient information to be evaluated.\n",
      "These feature sets' evaluation statistics can be viewed in the `ls_output` variable.\n",
      "\n",
      "12174 feature sets were discarded because they only presented one value for all patients.\n",
      "These discards can be viewed in the `discarded_because_only_one_value` variable.\n",
      "Stored 'ls_output1' (list)\n",
      "Stored 'discarded_because_only_one_value1' (list)\n"
     ]
    }
   ],
   "source": [
    "# Do work.\n",
    "t1 = time.time()\n",
    "# Create the names and values for the feature sets.\n",
    "for i_family_combins in tqdm.notebook.tqdm_notebook(family_combins[:50000], unit = \" feature sets\"):\n",
    "    selection = i_family_combins.copy()\n",
    "\n",
    "    # Get name of new feature set.\n",
    "    wildcard_idx = 0\n",
    "    new_fs_name_list = []\n",
    "    for i_family in i_family_combins:\n",
    "        # Extract the level and family.\n",
    "        try:\n",
    "            level, family = i_family.split('_')\n",
    "            wildcard_idx += 1 # This increment says 'There is no wildcard at this index location.'\n",
    "        except:\n",
    "            level = 'wildcard'\n",
    "            family = family_lookup_list[wildcard_idx]\n",
    "            wildcard_idx += 1\n",
    "\n",
    "        # Select `level_code` that will be used to represent the variable name.\n",
    "        level_code = level_options.get(level, 'The provided value for \\'level\\' is not valid.')\n",
    "\n",
    "        # Select family_code that will be used to represent the variable name.\n",
    "        family_code = family_options.get(family, 'The provided value for \\'family\\' is not valid.')\n",
    "\n",
    "        # Build variable name string.\n",
    "        new_fs_name_list.append(family_code + level_code)\n",
    "\n",
    "    new_fs_name = ('_'.join(new_fs_name_list))\n",
    "\n",
    "    # Remove wildcard family, if present.\n",
    "    try:\n",
    "        selection = [i for i in selection if i != 'wildcard']\n",
    "    except ValueError:\n",
    "        None\n",
    "    \n",
    "    # Make new feature set.\n",
    "    new_fs_value = nafsm_family_membership[selection].all(True)\n",
    "\n",
    "    # Skip if non-informative.  \n",
    "    a = len(new_fs_value.value_counts())\n",
    "    if a < 2:\n",
    "        # Keep a list of the non-informative feature sets.\n",
    "        discarded_because_only_one_value1.append(new_fs_name)\n",
    "    else:\n",
    "    # Else, evaluate the feature set.\n",
    "        fs_eval = evaloutputs(new_fs_value, caseness_array.caseness_1isYes.astype(int))\n",
    "        ls_output1.append([new_fs_name] + list(fs_eval)[1:])\n",
    "\n",
    "print(f'It took {time.time() - t1} to process.')\n",
    "\n",
    "# Store the list of output evaluations because I need to append to this in the evaluation of other feature sets.\n",
    "#%store ls_output\n",
    "print(f'{len(ls_output1)} feature sets contained sufficient information to be evaluated.')\n",
    "print('These feature sets\\' evaluation statistics can be viewed in the `ls_output` variable.\\n')\n",
    "\n",
    "# Store the discard list because I need to append to this in the evaluation of other feature sets.\n",
    "#%store discarded_because_only_one_value\n",
    "print(f'{len(set(discarded_because_only_one_value1) ) } feature sets were discarded because they only presented one value for all patients.')\n",
    "print('These discards can be viewed in the `discarded_because_only_one_value` variable.')\n",
    "\n",
    "%store ls_output1 discarded_because_only_one_value1\n",
    "# Join to `feature_set_array`.\n",
    "# ## No join is necessary because the fetaure sets have been processed and evaluated here, rather than saved for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4a9a02f3-9eb1-4f9f-9211-e6195ac65683",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bac49b5c87d484a83a57eae509a0541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ? feature sets/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 8487.588908433914 to process.\n",
      "38259 feature sets contained sufficient information to be evaluated.\n",
      "These feature sets' evaluation statistics can be viewed in the `ls_output` variable.\n",
      "\n",
      "11741 feature sets were discarded because they only presented one value for all patients.\n",
      "These discards can be viewed in the `discarded_because_only_one_value` variable.\n",
      "Stored 'ls_output2' (list)\n",
      "Stored 'discarded_because_only_one_value2' (list)\n"
     ]
    }
   ],
   "source": [
    "ls_output2 = []\n",
    "discarded_because_only_one_value2 = []\n",
    "# Do work.\n",
    "t1 = time.time()\n",
    "# Create the names and values for the feature sets.\n",
    "for i_family_combins in tqdm.notebook.tqdm_notebook(family_combins[50000:100000], unit = \" feature sets\"):\n",
    "    selection = i_family_combins.copy()\n",
    "\n",
    "    # Get name of new feature set.\n",
    "    wildcard_idx = 0\n",
    "    new_fs_name_list = []\n",
    "    for i_family in i_family_combins:\n",
    "        # Extract the level and family.\n",
    "        try:\n",
    "            level, family = i_family.split('_')\n",
    "            wildcard_idx += 1 # This increment says 'There is no wildcard at this index location.'\n",
    "        except:\n",
    "            level = 'wildcard'\n",
    "            family = family_lookup_list[wildcard_idx]\n",
    "            wildcard_idx += 1\n",
    "\n",
    "        # Select `level_code` that will be used to represent the variable name.\n",
    "        level_code = level_options.get(level, 'The provided value for \\'level\\' is not valid.')\n",
    "\n",
    "        # Select family_code that will be used to represent the variable name.\n",
    "        family_code = family_options.get(family, 'The provided value for \\'family\\' is not valid.')\n",
    "\n",
    "        # Build variable name string.\n",
    "        new_fs_name_list.append(family_code + level_code)\n",
    "\n",
    "    new_fs_name = ('_'.join(new_fs_name_list))\n",
    "\n",
    "    # Remove wildcard family, if present.\n",
    "    try:\n",
    "        selection = [i for i in selection if i != 'wildcard']\n",
    "    except ValueError:\n",
    "        None\n",
    "    \n",
    "    # Make new feature set.\n",
    "    new_fs_value = nafsm_family_membership[selection].all(True)\n",
    "\n",
    "    # Skip if non-informative.  \n",
    "    a = len(new_fs_value.value_counts())\n",
    "    if a < 2:\n",
    "        # Keep a list of the non-informative feature sets.\n",
    "        discarded_because_only_one_value2.append(new_fs_name)\n",
    "    else:\n",
    "    # Else, evaluate the feature set.\n",
    "        fs_eval = evaloutputs(new_fs_value, caseness_array.caseness_1isYes.astype(int))\n",
    "        ls_output2.append([new_fs_name] + list(fs_eval)[1:])\n",
    "\n",
    "print(f'It took {time.time() - t1} to process.')\n",
    "\n",
    "# Store the list of output evaluations because I need to append to this in the evaluation of other feature sets.\n",
    "#%store ls_output\n",
    "print(f'{len(ls_output2)} feature sets contained sufficient information to be evaluated.')\n",
    "print('These feature sets\\' evaluation statistics can be viewed in the `ls_output` variable.\\n')\n",
    "\n",
    "# Store the discard list because I need to append to this in the evaluation of other feature sets.\n",
    "#%store discarded_because_only_one_value\n",
    "print(f'{len(set(discarded_because_only_one_value2) ) } feature sets were discarded because they only presented one value for all patients.')\n",
    "print('These discards can be viewed in the `discarded_because_only_one_value` variable.')\n",
    "\n",
    "%store ls_output2 discarded_because_only_one_value2\n",
    "# Join to `feature_set_array`.\n",
    "# ## No join is necessary because the fetaure sets have been processed and evaluated here, rather than saved for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6482b1f6-2dff-4421-a027-385b20e09dc4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c4769cb7477b44928707cd9849a3bb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ? feature sets/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 6806.741814851761 to process.\n",
      "9945 feature sets contained sufficient information to be evaluated.\n",
      "These feature sets' evaluation statistics can be viewed in the `ls_output` variable.\n",
      "\n",
      "40055 feature sets were discarded because they only presented one value for all patients.\n",
      "These discards can be viewed in the `discarded_because_only_one_value` variable.\n",
      "Stored 'ls_output3' (list)\n",
      "Stored 'discarded_because_only_one_value3' (list)\n"
     ]
    }
   ],
   "source": [
    "ls_output3 = []\n",
    "discarded_because_only_one_value3 = []\n",
    "# Do work.\n",
    "t1 = time.time()\n",
    "# Create the names and values for the feature sets.\n",
    "for i_family_combins in tqdm.notebook.tqdm_notebook(family_combins[100000:150000], unit = \" feature sets\"):\n",
    "    selection = i_family_combins.copy()\n",
    "\n",
    "    # Get name of new feature set.\n",
    "    wildcard_idx = 0\n",
    "    new_fs_name_list = []\n",
    "    for i_family in i_family_combins:\n",
    "        # Extract the level and family.\n",
    "        try:\n",
    "            level, family = i_family.split('_')\n",
    "            wildcard_idx += 1 # This increment says 'There is no wildcard at this index location.'\n",
    "        except:\n",
    "            level = 'wildcard'\n",
    "            family = family_lookup_list[wildcard_idx]\n",
    "            wildcard_idx += 1\n",
    "\n",
    "        # Select `level_code` that will be used to represent the variable name.\n",
    "        level_code = level_options.get(level, 'The provided value for \\'level\\' is not valid.')\n",
    "\n",
    "        # Select family_code that will be used to represent the variable name.\n",
    "        family_code = family_options.get(family, 'The provided value for \\'family\\' is not valid.')\n",
    "\n",
    "        # Build variable name string.\n",
    "        new_fs_name_list.append(family_code + level_code)\n",
    "\n",
    "    new_fs_name = ('_'.join(new_fs_name_list))\n",
    "\n",
    "    # Remove wildcard family, if present.\n",
    "    try:\n",
    "        selection = [i for i in selection if i != 'wildcard']\n",
    "    except ValueError:\n",
    "        None\n",
    "    \n",
    "    # Make new feature set.\n",
    "    new_fs_value = nafsm_family_membership[selection].all(True)\n",
    "\n",
    "    # Skip if non-informative.  \n",
    "    a = len(new_fs_value.value_counts())\n",
    "    if a < 2:\n",
    "        # Keep a list of the non-informative feature sets.\n",
    "        discarded_because_only_one_value3.append(new_fs_name)\n",
    "    else:\n",
    "    # Else, evaluate the feature set.\n",
    "        fs_eval = evaloutputs(new_fs_value, caseness_array.caseness_1isYes.astype(int))\n",
    "        ls_output3.append([new_fs_name] + list(fs_eval)[1:])\n",
    "\n",
    "print(f'It took {time.time() - t1} to process.')\n",
    "\n",
    "# Store the list of output evaluations because I need to append to this in the evaluation of other feature sets.\n",
    "#%store ls_output\n",
    "print(f'{len(ls_output3)} feature sets contained sufficient information to be evaluated.')\n",
    "print('These feature sets\\' evaluation statistics can be viewed in the `ls_output` variable.\\n')\n",
    "\n",
    "# Store the discard list because I need to append to this in the evaluation of other feature sets.\n",
    "#%store discarded_because_only_one_value\n",
    "print(f'{len(set(discarded_because_only_one_value3) ) } feature sets were discarded because they only presented one value for all patients.')\n",
    "print('These discards can be viewed in the `discarded_because_only_one_value` variable.')\n",
    "\n",
    "%store ls_output3 discarded_because_only_one_value3\n",
    "# Join to `feature_set_array`.\n",
    "# ## No join is necessary because the fetaure sets have been processed and evaluated here, rather than saved for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8d828194-f034-4d54-a742-956db1d2975a",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8a3a653e8b748cebdb64d3253763c4b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ? feature sets/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 6922.701315879822 to process.\n",
      "15505 feature sets contained sufficient information to be evaluated.\n",
      "These feature sets' evaluation statistics can be viewed in the `ls_output` variable.\n",
      "\n",
      "34495 feature sets were discarded because they only presented one value for all patients.\n",
      "These discards can be viewed in the `discarded_because_only_one_value` variable.\n",
      "Stored 'ls_output4' (list)\n",
      "Stored 'discarded_because_only_one_value4' (list)\n"
     ]
    }
   ],
   "source": [
    "ls_output4 = []\n",
    "discarded_because_only_one_value4 = []\n",
    "# Do work.\n",
    "t1 = time.time()\n",
    "# Create the names and values for the feature sets.\n",
    "for i_family_combins in tqdm.notebook.tqdm_notebook(family_combins[150000:200000], unit = \" feature sets\"):\n",
    "    selection = i_family_combins.copy()\n",
    "\n",
    "    # Get name of new feature set.\n",
    "    wildcard_idx = 0\n",
    "    new_fs_name_list = []\n",
    "    for i_family in i_family_combins:\n",
    "        # Extract the level and family.\n",
    "        try:\n",
    "            level, family = i_family.split('_')\n",
    "            wildcard_idx += 1 # This increment says 'There is no wildcard at this index location.'\n",
    "        except:\n",
    "            level = 'wildcard'\n",
    "            family = family_lookup_list[wildcard_idx]\n",
    "            wildcard_idx += 1\n",
    "\n",
    "        # Select `level_code` that will be used to represent the variable name.\n",
    "        level_code = level_options.get(level, 'The provided value for \\'level\\' is not valid.')\n",
    "\n",
    "        # Select family_code that will be used to represent the variable name.\n",
    "        family_code = family_options.get(family, 'The provided value for \\'family\\' is not valid.')\n",
    "\n",
    "        # Build variable name string.\n",
    "        new_fs_name_list.append(family_code + level_code)\n",
    "\n",
    "    new_fs_name = ('_'.join(new_fs_name_list))\n",
    "\n",
    "    # Remove wildcard family, if present.\n",
    "    try:\n",
    "        selection = [i for i in selection if i != 'wildcard']\n",
    "    except ValueError:\n",
    "        None\n",
    "    \n",
    "    # Make new feature set.\n",
    "    new_fs_value = nafsm_family_membership[selection].all(True)\n",
    "\n",
    "    # Skip if non-informative.  \n",
    "    a = len(new_fs_value.value_counts())\n",
    "    if a < 2:\n",
    "        # Keep a list of the non-informative feature sets.\n",
    "        discarded_because_only_one_value4.append(new_fs_name)\n",
    "    else:\n",
    "    # Else, evaluate the feature set.\n",
    "        fs_eval = evaloutputs(new_fs_value, caseness_array.caseness_1isYes.astype(int))\n",
    "        ls_output4.append([new_fs_name] + list(fs_eval)[1:])\n",
    "\n",
    "print(f'It took {time.time() - t1} to process.')\n",
    "\n",
    "# Store the list of output evaluations because I need to append to this in the evaluation of other feature sets.\n",
    "#%store ls_output\n",
    "print(f'{len(ls_output4)} feature sets contained sufficient information to be evaluated.')\n",
    "print('These feature sets\\' evaluation statistics can be viewed in the `ls_output` variable.\\n')\n",
    "\n",
    "# Store the discard list because I need to append to this in the evaluation of other feature sets.\n",
    "#%store discarded_because_only_one_value\n",
    "print(f'{len(set(discarded_because_only_one_value4) ) } feature sets were discarded because they only presented one value for all patients.')\n",
    "print('These discards can be viewed in the `discarded_because_only_one_value` variable.')\n",
    "\n",
    "%store ls_output4 discarded_because_only_one_value4\n",
    "# Join to `feature_set_array`.\n",
    "# ## No join is necessary because the fetaure sets have been processed and evaluated here, rather than saved for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0dcd81ed-9f8e-468e-a84a-263fb5ccb1cf",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a331f395fde54955a51081cea6407d51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ? feature sets/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 8858.332808971405 to process.\n",
      "39388 feature sets contained sufficient information to be evaluated.\n",
      "These feature sets' evaluation statistics can be viewed in the `ls_output` variable.\n",
      "\n",
      "10612 feature sets were discarded because they only presented one value for all patients.\n",
      "These discards can be viewed in the `discarded_because_only_one_value` variable.\n",
      "Stored 'ls_output5' (list)\n",
      "Stored 'discarded_because_only_one_value5' (list)\n"
     ]
    }
   ],
   "source": [
    "ls_output5 = []\n",
    "discarded_because_only_one_value5 = []\n",
    "# Do work.\n",
    "t1 = time.time()\n",
    "# Create the names and values for the feature sets.\n",
    "for i_family_combins in tqdm.notebook.tqdm_notebook(family_combins[200000:250000], unit = \" feature sets\"):\n",
    "    selection = i_family_combins.copy()\n",
    "\n",
    "    # Get name of new feature set.\n",
    "    wildcard_idx = 0\n",
    "    new_fs_name_list = []\n",
    "    for i_family in i_family_combins:\n",
    "        # Extract the level and family.\n",
    "        try:\n",
    "            level, family = i_family.split('_')\n",
    "            wildcard_idx += 1 # This increment says 'There is no wildcard at this index location.'\n",
    "        except:\n",
    "            level = 'wildcard'\n",
    "            family = family_lookup_list[wildcard_idx]\n",
    "            wildcard_idx += 1\n",
    "\n",
    "        # Select `level_code` that will be used to represent the variable name.\n",
    "        level_code = level_options.get(level, 'The provided value for \\'level\\' is not valid.')\n",
    "\n",
    "        # Select family_code that will be used to represent the variable name.\n",
    "        family_code = family_options.get(family, 'The provided value for \\'family\\' is not valid.')\n",
    "\n",
    "        # Build variable name string.\n",
    "        new_fs_name_list.append(family_code + level_code)\n",
    "\n",
    "    new_fs_name = ('_'.join(new_fs_name_list))\n",
    "\n",
    "    # Remove wildcard family, if present.\n",
    "    try:\n",
    "        selection = [i for i in selection if i != 'wildcard']\n",
    "    except ValueError:\n",
    "        None\n",
    "    \n",
    "    # Make new feature set.\n",
    "    new_fs_value = nafsm_family_membership[selection].all(True)\n",
    "\n",
    "    # Skip if non-informative.  \n",
    "    a = len(new_fs_value.value_counts())\n",
    "    if a < 2:\n",
    "        # Keep a list of the non-informative feature sets.\n",
    "        discarded_because_only_one_value5.append(new_fs_name)\n",
    "    else:\n",
    "    # Else, evaluate the feature set.\n",
    "        fs_eval = evaloutputs(new_fs_value, caseness_array.caseness_1isYes.astype(int))\n",
    "        ls_output5.append([new_fs_name] + list(fs_eval)[1:])\n",
    "\n",
    "print(f'It took {time.time() - t1} to process.')\n",
    "\n",
    "# Store the list of output evaluations because I need to append to this in the evaluation of other feature sets.\n",
    "#%store ls_output\n",
    "print(f'{len(ls_output5)} feature sets contained sufficient information to be evaluated.')\n",
    "print('These feature sets\\' evaluation statistics can be viewed in the `ls_output` variable.\\n')\n",
    "\n",
    "# Store the discard list because I need to append to this in the evaluation of other feature sets.\n",
    "#%store discarded_because_only_one_value\n",
    "print(f'{len(set(discarded_because_only_one_value5) ) } feature sets were discarded because they only presented one value for all patients.')\n",
    "print('These discards can be viewed in the `discarded_because_only_one_value` variable.')\n",
    "\n",
    "%store ls_output5 discarded_because_only_one_value5\n",
    "# Join to `feature_set_array`.\n",
    "# ## No join is necessary because the fetaure sets have been processed and evaluated here, rather than saved for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5261af9e-5244-4b8e-bd94-0bfab1de3866",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dbaec6740ee149b89cbdf3828f6f02ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ? feature sets/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 8618.000146865845 to process.\n",
      "38994 feature sets contained sufficient information to be evaluated.\n",
      "These feature sets' evaluation statistics can be viewed in the `ls_output` variable.\n",
      "\n",
      "11006 feature sets were discarded because they only presented one value for all patients.\n",
      "These discards can be viewed in the `discarded_because_only_one_value` variable.\n",
      "Stored 'ls_output6' (list)\n",
      "Stored 'discarded_because_only_one_value6' (list)\n"
     ]
    }
   ],
   "source": [
    "ls_output6 = []\n",
    "discarded_because_only_one_value6 = []\n",
    "# Do work.\n",
    "t1 = time.time()\n",
    "# Create the names and values for the feature sets.\n",
    "for i_family_combins in tqdm.notebook.tqdm_notebook(family_combins[250000:300000], unit = \" feature sets\"):\n",
    "    selection = i_family_combins.copy()\n",
    "\n",
    "    # Get name of new feature set.\n",
    "    wildcard_idx = 0\n",
    "    new_fs_name_list = []\n",
    "    for i_family in i_family_combins:\n",
    "        # Extract the level and family.\n",
    "        try:\n",
    "            level, family = i_family.split('_')\n",
    "            wildcard_idx += 1 # This increment says 'There is no wildcard at this index location.'\n",
    "        except:\n",
    "            level = 'wildcard'\n",
    "            family = family_lookup_list[wildcard_idx]\n",
    "            wildcard_idx += 1\n",
    "\n",
    "        # Select `level_code` that will be used to represent the variable name.\n",
    "        level_code = level_options.get(level, 'The provided value for \\'level\\' is not valid.')\n",
    "\n",
    "        # Select family_code that will be used to represent the variable name.\n",
    "        family_code = family_options.get(family, 'The provided value for \\'family\\' is not valid.')\n",
    "\n",
    "        # Build variable name string.\n",
    "        new_fs_name_list.append(family_code + level_code)\n",
    "\n",
    "    new_fs_name = ('_'.join(new_fs_name_list))\n",
    "\n",
    "    # Remove wildcard family, if present.\n",
    "    try:\n",
    "        selection = [i for i in selection if i != 'wildcard']\n",
    "    except ValueError:\n",
    "        None\n",
    "    \n",
    "    # Make new feature set.\n",
    "    new_fs_value = nafsm_family_membership[selection].all(True)\n",
    "\n",
    "    # Skip if non-informative.  \n",
    "    a = len(new_fs_value.value_counts())\n",
    "    if a < 2:\n",
    "        # Keep a list of the non-informative feature sets.\n",
    "        discarded_because_only_one_value6.append(new_fs_name)\n",
    "    else:\n",
    "    # Else, evaluate the feature set.\n",
    "        fs_eval = evaloutputs(new_fs_value, caseness_array.caseness_1isYes.astype(int))\n",
    "        ls_output6.append([new_fs_name] + list(fs_eval)[1:])\n",
    "\n",
    "print(f'It took {time.time() - t1} to process.')\n",
    "\n",
    "# Store the list of output evaluations because I need to append to this in the evaluation of other feature sets.\n",
    "#%store ls_output\n",
    "print(f'{len(ls_output6)} feature sets contained sufficient information to be evaluated.')\n",
    "print('These feature sets\\' evaluation statistics can be viewed in the `ls_output` variable.\\n')\n",
    "\n",
    "# Store the discard list because I need to append to this in the evaluation of other feature sets.\n",
    "#%store discarded_because_only_one_value\n",
    "print(f'{len(set(discarded_because_only_one_value6) ) } feature sets were discarded because they only presented one value for all patients.')\n",
    "print('These discards can be viewed in the `discarded_because_only_one_value` variable.')\n",
    "\n",
    "%store ls_output6 discarded_because_only_one_value6\n",
    "# Join to `feature_set_array`.\n",
    "# ## No join is necessary because the fetaure sets have been processed and evaluated here, rather than saved for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f004a5c2-4434-4284-8b9e-d4dc8ae86b67",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad1c7e15ad264d32b2b0b735b06c389d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ? feature sets/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 9072.722830533981 to process.\n",
      "40424 feature sets contained sufficient information to be evaluated.\n",
      "These feature sets' evaluation statistics can be viewed in the `ls_output` variable.\n",
      "\n",
      "9576 feature sets were discarded because they only presented one value for all patients.\n",
      "These discards can be viewed in the `discarded_because_only_one_value` variable.\n",
      "Stored 'ls_output7' (list)\n",
      "Stored 'discarded_because_only_one_value7' (list)\n"
     ]
    }
   ],
   "source": [
    "ls_output7 = []\n",
    "discarded_because_only_one_value7 = []\n",
    "# Do work.\n",
    "t1 = time.time()\n",
    "# Create the names and values for the feature sets.\n",
    "for i_family_combins in tqdm.notebook.tqdm_notebook(family_combins[300000:350000], unit = \" feature sets\"):\n",
    "    selection = i_family_combins.copy()\n",
    "\n",
    "    # Get name of new feature set.\n",
    "    wildcard_idx = 0\n",
    "    new_fs_name_list = []\n",
    "    for i_family in i_family_combins:\n",
    "        # Extract the level and family.\n",
    "        try:\n",
    "            level, family = i_family.split('_')\n",
    "            wildcard_idx += 1 # This increment says 'There is no wildcard at this index location.'\n",
    "        except:\n",
    "            level = 'wildcard'\n",
    "            family = family_lookup_list[wildcard_idx]\n",
    "            wildcard_idx += 1\n",
    "\n",
    "        # Select `level_code` that will be used to represent the variable name.\n",
    "        level_code = level_options.get(level, 'The provided value for \\'level\\' is not valid.')\n",
    "\n",
    "        # Select family_code that will be used to represent the variable name.\n",
    "        family_code = family_options.get(family, 'The provided value for \\'family\\' is not valid.')\n",
    "\n",
    "        # Build variable name string.\n",
    "        new_fs_name_list.append(family_code + level_code)\n",
    "\n",
    "    new_fs_name = ('_'.join(new_fs_name_list))\n",
    "\n",
    "    # Remove wildcard family, if present.\n",
    "    try:\n",
    "        selection = [i for i in selection if i != 'wildcard']\n",
    "    except ValueError:\n",
    "        None\n",
    "    \n",
    "    # Make new feature set.\n",
    "    new_fs_value = nafsm_family_membership[selection].all(True)\n",
    "\n",
    "    # Skip if non-informative.  \n",
    "    a = len(new_fs_value.value_counts())\n",
    "    if a < 2:\n",
    "        # Keep a list of the non-informative feature sets.\n",
    "        discarded_because_only_one_value7.append(new_fs_name)\n",
    "    else:\n",
    "    # Else, evaluate the feature set.\n",
    "        fs_eval = evaloutputs(new_fs_value, caseness_array.caseness_1isYes.astype(int))\n",
    "        ls_output7.append([new_fs_name] + list(fs_eval)[1:])\n",
    "\n",
    "print(f'It took {time.time() - t1} to process.')\n",
    "\n",
    "# Store the list of output evaluations because I need to append to this in the evaluation of other feature sets.\n",
    "#%store ls_output\n",
    "print(f'{len(ls_output7)} feature sets contained sufficient information to be evaluated.')\n",
    "print('These feature sets\\' evaluation statistics can be viewed in the `ls_output` variable.\\n')\n",
    "\n",
    "# Store the discard list because I need to append to this in the evaluation of other feature sets.\n",
    "#%store discarded_because_only_one_value\n",
    "print(f'{len(set(discarded_because_only_one_value7) ) } feature sets were discarded because they only presented one value for all patients.')\n",
    "print('These discards can be viewed in the `discarded_because_only_one_value` variable.')\n",
    "\n",
    "%store ls_output7 discarded_because_only_one_value7\n",
    "# Join to `feature_set_array`.\n",
    "# ## No join is necessary because the fetaure sets have been processed and evaluated here, rather than saved for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79cb5c76-2177-4900-ac0c-6d72d24d2116",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806ca116819e4e4488b2ec34bc373f92",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ? feature sets/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 8987.33467054367 to process.\n",
      "40331 feature sets contained sufficient information to be evaluated.\n",
      "These feature sets' evaluation statistics can be viewed in the `ls_output` variable.\n",
      "\n",
      "9669 feature sets were discarded because they only presented one value for all patients.\n",
      "These discards can be viewed in the `discarded_because_only_one_value` variable.\n",
      "Stored 'ls_output8' (list)\n",
      "Stored 'discarded_because_only_one_value8' (list)\n"
     ]
    }
   ],
   "source": [
    "ls_output8 = []\n",
    "discarded_because_only_one_value8 = []\n",
    "# Do work.\n",
    "t1 = time.time()\n",
    "# Create the names and values for the feature sets.\n",
    "for i_family_combins in tqdm.notebook.tqdm_notebook(family_combins[350000:400000], unit = \" feature sets\"):\n",
    "    selection = i_family_combins.copy()\n",
    "\n",
    "    # Get name of new feature set.\n",
    "    wildcard_idx = 0\n",
    "    new_fs_name_list = []\n",
    "    for i_family in i_family_combins:\n",
    "        # Extract the level and family.\n",
    "        try:\n",
    "            level, family = i_family.split('_')\n",
    "            wildcard_idx += 1 # This increment says 'There is no wildcard at this index location.'\n",
    "        except:\n",
    "            level = 'wildcard'\n",
    "            family = family_lookup_list[wildcard_idx]\n",
    "            wildcard_idx += 1\n",
    "\n",
    "        # Select `level_code` that will be used to represent the variable name.\n",
    "        level_code = level_options.get(level, 'The provided value for \\'level\\' is not valid.')\n",
    "\n",
    "        # Select family_code that will be used to represent the variable name.\n",
    "        family_code = family_options.get(family, 'The provided value for \\'family\\' is not valid.')\n",
    "\n",
    "        # Build variable name string.\n",
    "        new_fs_name_list.append(family_code + level_code)\n",
    "\n",
    "    new_fs_name = ('_'.join(new_fs_name_list))\n",
    "\n",
    "    # Remove wildcard family, if present.\n",
    "    try:\n",
    "        selection = [i for i in selection if i != 'wildcard']\n",
    "    except ValueError:\n",
    "        None\n",
    "    \n",
    "    # Make new feature set.\n",
    "    new_fs_value = nafsm_family_membership[selection].all(True)\n",
    "\n",
    "    # Skip if non-informative.  \n",
    "    a = len(new_fs_value.value_counts())\n",
    "    if a < 2:\n",
    "        # Keep a list of the non-informative feature sets.\n",
    "        discarded_because_only_one_value8.append(new_fs_name)\n",
    "    else:\n",
    "    # Else, evaluate the feature set.\n",
    "        fs_eval = evaloutputs(new_fs_value, caseness_array.caseness_1isYes.astype(int))\n",
    "        ls_output8.append([new_fs_name] + list(fs_eval)[1:])\n",
    "\n",
    "print(f'It took {time.time() - t1} to process.')\n",
    "\n",
    "# Store the list of output evaluations because I need to append to this in the evaluation of other feature sets.\n",
    "#%store ls_output\n",
    "print(f'{len(ls_output8)} feature sets contained sufficient information to be evaluated.')\n",
    "print('These feature sets\\' evaluation statistics can be viewed in the `ls_output` variable.\\n')\n",
    "\n",
    "# Store the discard list because I need to append to this in the evaluation of other feature sets.\n",
    "#%store discarded_because_only_one_value\n",
    "print(f'{len(set(discarded_because_only_one_value8) ) } feature sets were discarded because they only presented one value for all patients.')\n",
    "print('These discards can be viewed in the `discarded_because_only_one_value` variable.')\n",
    "\n",
    "%store ls_output8 discarded_because_only_one_value8\n",
    "# Join to `feature_set_array`.\n",
    "# ## No join is necessary because the fetaure sets have been processed and evaluated here, rather than saved for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c518ffa5-8233-486e-9c4d-a3004370276c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992f68bb0cf94a1fadebf6aeb471cf6d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/50000 [00:00<?, ? feature sets/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 7889.966617822647 to process.\n",
      "38211 feature sets contained sufficient information to be evaluated.\n",
      "These feature sets' evaluation statistics can be viewed in the `ls_output` variable.\n",
      "\n",
      "11789 feature sets were discarded because they only presented one value for all patients.\n",
      "These discards can be viewed in the `discarded_because_only_one_value` variable.\n",
      "Stored 'ls_output9' (list)\n",
      "Stored 'discarded_because_only_one_value9' (list)\n"
     ]
    }
   ],
   "source": [
    "ls_output9 = []\n",
    "discarded_because_only_one_value9 = []\n",
    "# Do work.\n",
    "t1 = time.time()\n",
    "# Create the names and values for the feature sets.\n",
    "for i_family_combins in tqdm.notebook.tqdm_notebook(family_combins[450000:500000], unit = \" feature sets\"):\n",
    "    selection = i_family_combins.copy()\n",
    "\n",
    "    # Get name of new feature set.\n",
    "    wildcard_idx = 0\n",
    "    new_fs_name_list = []\n",
    "    for i_family in i_family_combins:\n",
    "        # Extract the level and family.\n",
    "        try:\n",
    "            level, family = i_family.split('_')\n",
    "            wildcard_idx += 1 # This increment says 'There is no wildcard at this index location.'\n",
    "        except:\n",
    "            level = 'wildcard'\n",
    "            family = family_lookup_list[wildcard_idx]\n",
    "            wildcard_idx += 1\n",
    "\n",
    "        # Select `level_code` that will be used to represent the variable name.\n",
    "        level_code = level_options.get(level, 'The provided value for \\'level\\' is not valid.')\n",
    "\n",
    "        # Select family_code that will be used to represent the variable name.\n",
    "        family_code = family_options.get(family, 'The provided value for \\'family\\' is not valid.')\n",
    "\n",
    "        # Build variable name string.\n",
    "        new_fs_name_list.append(family_code + level_code)\n",
    "\n",
    "    new_fs_name = ('_'.join(new_fs_name_list))\n",
    "\n",
    "    # Remove wildcard family, if present.\n",
    "    try:\n",
    "        selection = [i for i in selection if i != 'wildcard']\n",
    "    except ValueError:\n",
    "        None\n",
    "    \n",
    "    # Make new feature set.\n",
    "    new_fs_value = nafsm_family_membership[selection].all(True)\n",
    "\n",
    "    # Skip if non-informative.  \n",
    "    a = len(new_fs_value.value_counts())\n",
    "    if a < 2:\n",
    "        # Keep a list of the non-informative feature sets.\n",
    "        discarded_because_only_one_value9.append(new_fs_name)\n",
    "    else:\n",
    "    # Else, evaluate the feature set.\n",
    "        fs_eval = evaloutputs(new_fs_value, caseness_array.caseness_1isYes.astype(int))\n",
    "        ls_output9.append([new_fs_name] + list(fs_eval)[1:])\n",
    "\n",
    "print(f'It took {time.time() - t1} to process.')\n",
    "\n",
    "# Store the list of output evaluations because I need to append to this in the evaluation of other feature sets.\n",
    "#%store ls_output\n",
    "print(f'{len(ls_output9)} feature sets contained sufficient information to be evaluated.')\n",
    "print('These feature sets\\' evaluation statistics can be viewed in the `ls_output` variable.\\n')\n",
    "\n",
    "# Store the discard list because I need to append to this in the evaluation of other feature sets.\n",
    "#%store discarded_because_only_one_value\n",
    "print(f'{len(set(discarded_because_only_one_value9) ) } feature sets were discarded because they only presented one value for all patients.')\n",
    "print('These discards can be viewed in the `discarded_because_only_one_value` variable.')\n",
    "\n",
    "%store ls_output9 discarded_because_only_one_value9\n",
    "# Join to `feature_set_array`.\n",
    "# ## No join is necessary because the fetaure sets have been processed and evaluated here, rather than saved for later evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dd6a6550-0492-4913-991f-13da0a24ad21",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "471d9b1ee6544b96805640af87739ad0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/59872 [00:00<?, ? feature sets/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "It took 9414.497598409653 to process.\n",
      "49618 feature sets contained sufficient information to be evaluated.\n",
      "These feature sets' evaluation statistics can be viewed in the `ls_output` variable.\n",
      "\n",
      "10254 feature sets were discarded because they only presented one value for all patients.\n",
      "These discards can be viewed in the `discarded_because_only_one_value` variable.\n",
      "Stored 'ls_output10' (list)\n",
      "Stored 'discarded_because_only_one_value10' (list)\n"
     ]
    }
   ],
   "source": [
    "ls_output10 = []\n",
    "discarded_because_only_one_value10 = []\n",
    "# Do work.\n",
    "t1 = time.time()\n",
    "# Create the names and values for the feature sets.\n",
    "for i_family_combins in tqdm.notebook.tqdm_notebook(family_combins[500000:], unit = \" feature sets\"):\n",
    "    selection = i_family_combins.copy()\n",
    "\n",
    "    # Get name of new feature set.\n",
    "    wildcard_idx = 0\n",
    "    new_fs_name_list = []\n",
    "    for i_family in i_family_combins:\n",
    "        # Extract the level and family.\n",
    "        try:\n",
    "            level, family = i_family.split('_')\n",
    "            wildcard_idx += 1 # This increment says 'There is no wildcard at this index location.'\n",
    "        except:\n",
    "            level = 'wildcard'\n",
    "            family = family_lookup_list[wildcard_idx]\n",
    "            wildcard_idx += 1\n",
    "\n",
    "        # Select `level_code` that will be used to represent the variable name.\n",
    "        level_code = level_options.get(level, 'The provided value for \\'level\\' is not valid.')\n",
    "\n",
    "        # Select family_code that will be used to represent the variable name.\n",
    "        family_code = family_options.get(family, 'The provided value for \\'family\\' is not valid.')\n",
    "\n",
    "        # Build variable name string.\n",
    "        new_fs_name_list.append(family_code + level_code)\n",
    "\n",
    "    new_fs_name = ('_'.join(new_fs_name_list))\n",
    "\n",
    "    # Remove wildcard family, if present.\n",
    "    try:\n",
    "        selection = [i for i in selection if i != 'wildcard']\n",
    "    except ValueError:\n",
    "        None\n",
    "    \n",
    "    # Make new feature set.\n",
    "    new_fs_value = nafsm_family_membership[selection].all(True)\n",
    "\n",
    "    # Skip if non-informative.  \n",
    "    a = len(new_fs_value.value_counts())\n",
    "    if a < 2:\n",
    "        # Keep a list of the non-informative feature sets.\n",
    "        discarded_because_only_one_value10.append(new_fs_name)\n",
    "    else:\n",
    "    # Else, evaluate the feature set.\n",
    "        fs_eval = evaloutputs(new_fs_value, caseness_array.caseness_1isYes.astype(int))\n",
    "        ls_output10.append([new_fs_name] + list(fs_eval)[1:])\n",
    "\n",
    "print(f'It took {time.time() - t1} to process.')\n",
    "\n",
    "# Store the list of output evaluations because I need to append to this in the evaluation of other feature sets.\n",
    "#%store ls_output\n",
    "print(f'{len(ls_output10)} feature sets contained sufficient information to be evaluated.')\n",
    "print('These feature sets\\' evaluation statistics can be viewed in the `ls_output` variable.\\n')\n",
    "\n",
    "# Store the discard list because I need to append to this in the evaluation of other feature sets.\n",
    "#%store discarded_because_only_one_value\n",
    "print(f'{len(set(discarded_because_only_one_value10) ) } feature sets were discarded because they only presented one value for all patients.')\n",
    "print('These discards can be viewed in the `discarded_because_only_one_value` variable.')\n",
    "\n",
    "%store ls_output10 discarded_because_only_one_value10\n",
    "# Join to `feature_set_array`.\n",
    "# ## No join is necessary because the fetaure sets have been processed and evaluated here, rather than saved for later evaluation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "222b0395-5274-4f76-9686-2ee26959dc21",
   "metadata": {},
   "source": [
    "#### Write lists to file, just in case there are more problems with the store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8ea248bd-b109-41fd-9e30-3bccb630eb12",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'ls_output_family_combins' (list)\n",
      "Stored 'discarded_because_only_one_value' (set)\n"
     ]
    }
   ],
   "source": [
    "# Extend the lists to be saved.\n",
    "# ## I had some service issuse that meant I had to chunk my processing. If you have to do the same, the script below combines your chunks into a single variable\n",
    "# ## that will then be saved.\n",
    "ls_output = []\n",
    "discarded_because_only_one_value = []\n",
    "[ls_output.extend(l) for l in (ls_output1, ls_output2, ls_output3, ls_output4, ls_output5, ls_output6, ls_output7, ls_output8, ls_output9, ls_output10)]\n",
    "[discarded_because_only_one_value.extend(l) for l in (discarded_because_only_one_value1, discarded_because_only_one_value2, discarded_because_only_one_value3, discarded_because_only_one_value4,\n",
    "                                                      discarded_because_only_one_value5, discarded_because_only_one_value6, discarded_because_only_one_value7,\n",
    "                                                      discarded_because_only_one_value8, discarded_because_only_one_value9, discarded_because_only_one_value10)]\n",
    "\n",
    "ls_output_family_combins = ls_output.copy()\n",
    "discarded_because_only_one_value = set(discarded_because_only_one_value).copy()\n",
    "\n",
    "\n",
    "# Write lists to file.\n",
    "with open('ls_output_family_combins.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(ls_output_family_combins)\n",
    "\n",
    "%store ls_output_family_combins\n",
    "    \n",
    "with open('discarded_because_only_one_value.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerows(discarded_because_only_one_value)\n",
    "%store discarded_because_only_one_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31128542-2f41-4e26-ae38-eeff9f2b591d",
   "metadata": {},
   "source": [
    "### Multiprocessing implementation.\n",
    "\n",
    "I had some issues with the multiprocessing implementation and I also found that it didn't speed things up very much, for my particular problem. I leave my scripts below for your interest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00bfdea6-d5cc-44ee-b5b4-5e20f76a7874",
   "metadata": {},
   "source": [
    "#### Define functions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "72ffd289-2ee8-4eaf-ad03-86d1f7f136a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "# initialize worker processes\n",
    "def init_worker(shared_queue):\n",
    "    # declare scope of a new global variable\n",
    "    global queue\n",
    "    # store argument in the global variable for this process\n",
    "    queue = shared_queue\n",
    "    \n",
    "# consume work\n",
    "def return_callback(result):\n",
    "    return result\n",
    "        \n",
    "    \n",
    "def createfamilycombinfs(family_combin, level_options, family_options, family_combin_names_processed, nafsm_family_membership):\n",
    "    # Do work\n",
    "    selection = family_combin.copy()\n",
    "    #print('selection defined.')\n",
    "    \n",
    "    # Get name of new feature set.\n",
    "    wildcard_idx = 0\n",
    "    new_fs_name_list = []\n",
    "    for i_family in family_combin:\n",
    "        # Extract the level and family.\n",
    "        try:\n",
    "            level, family = i_family.split('_')\n",
    "            wildcard_idx += 1\n",
    "        except:\n",
    "            level = 'wildcard'\n",
    "            family = family_lookup_list[wildcard_idx]\n",
    "\n",
    "        # Select `level_code` that will be used to represent the variable name.\n",
    "        level_code = level_options.get(level, 'The provided value for \\'level\\' is not valid.')\n",
    "        #print('level_code obtained')\n",
    "\n",
    "        # Select family_code that will be used to represent the variable name.\n",
    "        family_code = family_options.get(family, 'The provided value for \\'family\\' is not valid.')\n",
    "        #print('family_code obtained')\n",
    "        \n",
    "        # Build variable name string.\n",
    "        new_fs_name_list.append(family_code + level_code)\n",
    "    \n",
    "    # Define new feature set's name.\n",
    "    new_fs_name = ('_'.join(new_fs_name_list))\n",
    "    #print('new_fs_name defined.')\n",
    "        \n",
    "    # Mark family combination as processed.\n",
    "    family_combin_names_processed.append(family_combin)\n",
    "    \n",
    "    # Remove wildcard family, if present.\n",
    "    try:\n",
    "        selection = [i for i in selection if i != 'wildcard']\n",
    "    except ValueError:\n",
    "        None\n",
    "    \n",
    "    # Make new feature set's value.\n",
    "    new_fs_value = nafsm_family_membership[selection].all(True)\n",
    "    #print('new_fs_value defined.')\n",
    "\n",
    "    # Append new feature set to the storage dictionary.\n",
    "    ##new_fs_dict[new_fs_name] = new_fs_value\n",
    "    \n",
    "    # declare scope of shared queue\n",
    "    ###global queue\n",
    "    # send result using shared queue\n",
    "    queue_value = [new_fs_name, new_fs_value]\n",
    "    #print(sys.getsizeof(queue_value))\n",
    "    #print(f'queue_value ready to be placed in queue')\n",
    "    ####queue.put(queue_value)\n",
    "    #print('queue_value placed in queue')\n",
    "        \n",
    "    return queue_value"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7402080d-c383-4fe3-b9a5-70e2005b11ee",
   "metadata": {},
   "source": [
    "#### Do work."
   ]
  },
  {
   "cell_type": "raw",
   "id": "6d977329-9829-4249-97a4-6c1480086117",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "source": [
    "#######################\n",
    "# The prallel version.#\n",
    "#######################\n",
    "# All combinations of the non-mutually-exclusive columns. For example, mutually-exclusive columns would be the\n",
    "# 'None' and 'Many' antecedent columns, which can't both be true.\n",
    "# The `family_combins` list that is created does not cantain CB's 'regardless' combinations, i.e. combinations\n",
    "# of subsets of non-mutually-exclusive columns but where some levels of families aren't specified. For example,\n",
    "# with 'x' = 'regardless, a feature set entitled `fs_A0C1SxTxKxPxRxYx` represents the patients that have no\n",
    "# antecedent feature sets (A0), not-none concurrent feature sets (C1), and any count, zero of otherwise, from\n",
    "# the other families feature sets.\n",
    "\n",
    "# Set iterator.\n",
    "ls_family_combin_names = []\n",
    "A_cols = list(nafsm_family_membership.filter(regex='antecedent').columns) + ['wildcard']\n",
    "C_cols = list(nafsm_family_membership.filter(regex='concurrent').columns) + ['wildcard']\n",
    "S_cols = list(nafsm_family_membership.filter(regex='serviceUse').columns) + ['wildcard']\n",
    "T_cols = list(nafsm_family_membership.filter(regex='treatment').columns) + ['wildcard']\n",
    "K_cols = list(nafsm_family_membership.filter(regex='inconsistency').columns) + ['wildcard']\n",
    "P_cols = list(nafsm_family_membership.filter(regex='patternsOfPrescription').columns) + ['wildcard']\n",
    "R_cols = list(nafsm_family_membership.filter(regex='relevantPrescriptions').columns) + ['wildcard']\n",
    "Y_cols = list(nafsm_family_membership.filter(regex='antipsychotics').columns) + ['wildcard']\n",
    "for i_antecedent in A_cols:\n",
    "    for i_concurrent in C_cols:\n",
    "        for i_serviceUse in S_cols:\n",
    "            for i_treatment in T_cols:\n",
    "                for i_chaotic in K_cols:\n",
    "                    for i_patternsOfPrescriptions in P_cols:\n",
    "                        for i_relevantPrescriptions in R_cols:\n",
    "                            for i_antipsychotics in Y_cols:\n",
    "                                ls_family_combin_names.append([i_antecedent, i_concurrent, i_serviceUse, i_treatment, i_inconsistency,\n",
    "                                                       i_patternsOfPrescriptions, i_relevantPrescriptions, i_antipsychotics])\n",
    "\n",
    "# \n",
    "try:\n",
    "    if len(family_combin_names_processed) > 0:\n",
    "        # The following code only runs if `pid_processed` exists and is loaded.\n",
    "        ls_family_combin_names = list(set(ls_family_combin_names).difference(set(family_combin_names_processed)))\n",
    "        print(\"\\nSome family combinations have already been processed. The `ls_family_combin_names` iterator will be shortened accordingly.\\n\")\n",
    "    else:\n",
    "        # If `pid_processed` does not exist, then I am starting from scratch\n",
    "        family_combin_names_processed = []\n",
    "        print(\"\\nNo family combinations have already been processed. All family combinations will be processed.\\n\")\n",
    "except:\n",
    "    # If `pid_processed` does not exist, then I am starting from scratch\n",
    "    family_combin_names_processed = []\n",
    "    print(\"\\nNo family combinations have already been processed. All family combinations will be processed.\\n\")\n",
    "\n",
    "# Set lookup of family names.\n",
    "family_lookup_list = ['antecedent', 'concurrent', 'serviceUse', 'treatment', 'inconsistency', 'patternsOfPrescription', 'relevantPrescriptions', 'antipsychoticsPrescription']\n",
    "\n",
    "# Set mapping dictionary for count level to count-level code.\n",
    "level_options = \\\n",
    "        {'none' : '0',\n",
    "         'notNone'  : '1',\n",
    "         'few'  : '2',\n",
    "         'some' : '3',\n",
    "         'many' : '4',\n",
    "         'wildcard' : 'x'\n",
    "        }\n",
    "\n",
    "# Set mapping dictionary for family name to family code.\n",
    "family_options = \\\n",
    "            {'antecedent' : 'A',\n",
    "             'concurrent'  : 'C',\n",
    "             'serviceUse'  : 'S',\n",
    "             'treatment' : 'T',\n",
    "             'inconsistency' : 'K',\n",
    "             'patternsOfPrescription' : 'P',\n",
    "             'relevantPrescriptions' : 'R',\n",
    "             'antipsychoticsPrescription' : 'Y'\n",
    "            }    \n",
    "    \n",
    "# Set storage.\n",
    "global ls_family_combin_values\n",
    "ls_family_combin_values = []\n",
    "\n",
    "ls_output = []\n",
    "\n",
    "# Set number of workers.\n",
    "n_workers = 8\n",
    "\n",
    "t1 = time.time()\n",
    "# Do the main job of assessing the feature sets.\n",
    "if __name__ ==  '__main__':\n",
    "    # Create a shared queue\n",
    "    shared_queue = SimpleQueue()\n",
    "     \n",
    "    print('Parallel processing to calculate the family-combination feature sets has begun...')\n",
    "    \n",
    "    # create and configure the process pool\n",
    "    with Pool(processes = n_workers, initializer = init_worker, initargs = (shared_queue,) ) as pool:\n",
    "       \n",
    "        # Issue tasks into the process pool\n",
    "        ## starmap version \n",
    "        ###_ = pool.starmap(createfamilycombinfs, zip(ls_family_combin_names[0:3],\n",
    "        ###                                           itertools.repeat(level_options), itertools.repeat(family_options),\n",
    "        ###                                           itertools.repeat(family_combin_names_processed), itertools.repeat(nafsm_family_membership)\n",
    "        ###                                         )\n",
    "        ###)\n",
    "        ### Read results (https://superfastpython.com/multiprocessing-pool-shared-global-variables/).\n",
    "        ###for i in range(len(ls_family_combin_names[0:3])):\n",
    "        ###    print(shared_queue.get())#ls_family_combin_values.append(shared_queue.get() )\n",
    "        \n",
    "        ## map_async vesion\n",
    "        #results = pool.map_async(createfamilycombinfs, zip(ls_family_combin_names[0:3],\n",
    "        #                                           itertools.repeat(level_options), itertools.repeat(family_options),\n",
    "        #                                           itertools.repeat(family_combin_names_processed), itertools.repeat(nafsm_family_membership)\n",
    "        #                                         )\n",
    "        #                )\n",
    "        \n",
    "        ## apply_async version.\n",
    "        results = [pool.apply_async(createfamilycombinfs, (i, level_options, family_options, family_combin_names_processed, nafsm_family_membership)) for i in ls_family_combin_names]\n",
    "\n",
    "       \n",
    "        ls_family_combin_values = [res.get() for res in results]\n",
    "    # Close down the pool to release resources. https://superfastpython.com/shutdown-the-multiprocessing-pool-in-python/\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    \n",
    "    print('\\t...parallel processing to calculate the entropy-based feature sets has ended.')\n",
    "print(f'It took {time.time() - t1} to process using {n_workers} workers.')\n",
    "\n",
    "### Format the output\n",
    "col_headers = []\n",
    "col_values = []\n",
    "for i in range(len(ls_family_combin_values)):\n",
    "    col_headers.append(ls_family_combin_values[i][0])\n",
    "    col_values.append(ls_family_combin_values[i][1])\n",
    "family_combin_fs = pandas.concat(col_values, axis = 1)\n",
    "family_combin_fs.columns = col_headers\n",
    "\n",
    "%store family_combin_fs"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-2.m111",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-2:m111"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
