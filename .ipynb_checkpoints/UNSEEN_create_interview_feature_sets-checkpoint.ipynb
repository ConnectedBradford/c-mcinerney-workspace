{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7ef62a66-4956-4591-b43a-bc3ee30745ab",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Interview feature sets\n",
    "The purpose of this notebook is to create the array of feature sets suggested by the interviews with GPs.\n",
    "\n",
    "### Imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "b044e55d-64e9-43d6-92e9-6a2ca43295cc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%run 'UNSEEN_helper_functions.ipynb'\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be69d2ce-8c42-4879-b232-a480e500679d",
   "metadata": {},
   "source": [
    "## Load codelist CSV files.\n",
    "We used opencodelist.org to define codelists that define the set of SNOMED-CT codes used to identify patients based on various attributes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "437c8f34-fdc7-4343-a685-afdbf53f8096",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Instaniate BigQuery client.\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Set folder location.\n",
    "folder_loc = os.path.dirname(os.path.abspath(\"UNSEEN create clinician feature sets.ipynb\"))\n",
    "folder = folder_loc + '/codelists/'\n",
    "\n",
    "# Clinical codes of interest.\n",
    "codes_to_query_DNA = pandas.read_csv(folder + \"ciaranmci-did-not-attend-098119da.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bfdf4e2-64c4-40a3-965d-14632520023f",
   "metadata": {},
   "source": [
    "## Load prerequisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "34bff67f-edf7-462b-943b-a6909b113e94",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "if 'caseness_array' not in globals():\n",
    "    print(\"not here\")\n",
    "    %run ./\"UNSEEN_create_caseness_variables.ipynb\"\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14bf4787-ac83-477f-a2d1-e6cf91669f26",
   "metadata": {},
   "source": [
    "## Query database for base feature sets\n",
    "The code below returns `fs_interview` that contains the base feature sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abafa2e-a030-400c-85ce-3e34fc96e5ab",
   "metadata": {},
   "source": [
    "### Feature sets potentially indicative of \"chaotic life\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a32702f-2120-4e0d-b88f-9202ae26f070",
   "metadata": {},
   "source": [
    "#### Counts and ratios of appointments and did-not-attend events"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f1c9401d-9ee0-489e-984a-593c21dc8b00",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_CTEs_top = \"\"\"\n",
    "WITH\n",
    "# The first CTE will specify the 'spine' of the data table by selecting the unique list of person IDs.\n",
    "tbl_persons AS (\n",
    "    SELECT\n",
    "        DISTINCT person_id\n",
    "        ,year_of_birth\n",
    "    FROM\n",
    "        \"\"\" + server_id + \"\"\".\"\"\" + database_id + \"\"\".person\n",
    "    # Limiting to age range 18-70.\n",
    "    WHERE\n",
    "        (EXTRACT(YEAR FROM CURRENT_DATE()) - year_of_birth) BETWEEN 18 AND 70\n",
    ")\n",
    "\n",
    "# The following CTEs extract each clinical codelist into a SQL table before querying the person_ID \n",
    "# associated with the clinical codes.\n",
    "#\n",
    "\"\"\"\n",
    "\n",
    "sql_CTEs_body = \\\n",
    "\"\"\"\n",
    "#  ## Count of appointments in the previous year.\n",
    ",tbl_countAppointmentsPreviousYear_persons AS ( \n",
    "    SELECT \n",
    "        DISTINCT person_id\n",
    "        ,COUNT( DISTINCT EXTRACT(DATE FROM datestart) ) AS countAppointmentsPreviousYear\n",
    "    FROM\n",
    "        \"\"\" + server_id + \"\"\".\"\"\" + database_id + \"\"\".tbl_srappointment\n",
    "    WHERE\n",
    "        DATE_DIFF(CURRENT_DATE(), datestart, YEAR) <= 1\n",
    "    GROUP BY\n",
    "        person_id\n",
    "    ORDER BY\n",
    "        person_id\n",
    ")\n",
    "# ## Median annual count of appointments.\n",
    ",tbl_annualCountOfAppointments AS (\n",
    "    SELECT \n",
    "        DISTINCT person_id\n",
    "        ,EXTRACT(YEAR FROM datestart) AS year_appointment\n",
    "        ,COUNT( DISTINCT EXTRACT(DATE FROM datestart) ) AS countAppointmentsPerYear\n",
    "    FROM\n",
    "        \"\"\" + server_id + \"\"\".\"\"\" + database_id + \"\"\".tbl_srappointment\n",
    "    GROUP BY\n",
    "        person_id\n",
    "        ,year_appointment\n",
    "    ORDER BY\n",
    "        person_id\n",
    "        ,year_appointment\n",
    ")\n",
    ",tbl_medianAnnualCountAppointments_persons AS (\n",
    "    SELECT\n",
    "        DISTINCT person_id\n",
    "        ,PERCENTILE_DISC(countAppointmentsPerYear, 0.5) OVER(PARTITION BY person_id) AS medianAnnualCountAppointments\n",
    "    FROM\n",
    "        tbl_annualCountOfAppointments\n",
    "    ORDER BY\n",
    "        person_id\n",
    ")\n",
    "#  ## Count of Did-Not-Attend (DNA) in the previous year.\n",
    ",tbl_DNAcodes AS ( \n",
    "    SELECT\n",
    "        snomedcode\n",
    "    FROM\n",
    "        UNNEST([\n",
    "                '\"\"\" + '\\', \\''.join(map(str, codes_to_query_DNA[\"code\"].tolist())) + \"\"\"'\n",
    "                ]) AS snomedcode\n",
    ")\n",
    ",tbl_countDNAsPreviousYear_persons AS ( \n",
    "    SELECT \n",
    "      DISTINCT a.person_id\n",
    "     ,COUNT(person_id) AS countDNAsPreviousYear\n",
    "    FROM\n",
    "      \"\"\" + server_id + \"\"\".\"\"\" + database_id + \"\"\".tbl_srcode AS a, tbl_DNAcodes\n",
    "    WHERE\n",
    "      a.snomedcode IN (tbl_DNAcodes.snomedcode)\n",
    "      AND DATE_DIFF(CURRENT_DATE(), dateevent, YEAR) <= 1\n",
    "    GROUP BY\n",
    "        person_id\n",
    ")\n",
    "# ## Median annual count of Did-Not-Attend (DNA).\n",
    ",tbl_annualCountOfDNAs AS ( \n",
    "    SELECT \n",
    "        DISTINCT a.person_id\n",
    "        ,EXTRACT(YEAR FROM dateevent) AS year_DNA\n",
    "        ,COUNT( DISTINCT EXTRACT(DATE FROM dateevent) ) AS countDNAsPerYear\n",
    "    FROM\n",
    "        \"\"\" + server_id + \"\"\".\"\"\" + database_id + \"\"\".tbl_srcode AS a, tbl_DNAcodes\n",
    "    WHERE\n",
    "        a.snomedcode IN (tbl_DNAcodes.snomedcode)\n",
    "    GROUP BY\n",
    "        person_id\n",
    "        ,year_DNA\n",
    ")\n",
    ",tbl_medianAnnualCountDNAs_persons AS (\n",
    "    SELECT\n",
    "        DISTINCT tbl_annualCountOfDNAs.person_id\n",
    "        ,PERCENTILE_DISC(countDNAsPerYear, 0.5) OVER(PARTITION BY person_id) AS medianAnnualCountDNAs\n",
    "    FROM\n",
    "        tbl_annualCountOfDNAs\n",
    "    ORDER BY\n",
    "        person_id\n",
    ")\n",
    "# ## Ratio of annual counts of Did-Not-Attend (DNA) to appointment, in the previous year.\n",
    ",tbl_ratioDNAtoAppointmentPreviousYear_persons AS (\n",
    "    SELECT\n",
    "        DISTINCT tbl_countDNAsPreviousYear_persons.person_id\n",
    "        ,(countDNAsPreviousYear / countAppointmentsPreviousYear) AS ratioDNAtoAppointmentPreviousYear\n",
    "    FROM\n",
    "        tbl_countDNAsPreviousYear_persons\n",
    "    LEFT OUTER JOIN tbl_countAppointmentsPreviousYear_persons ON tbl_countDNAsPreviousYear_persons.person_id = tbl_countAppointmentsPreviousYear_persons.person_id\n",
    "        \n",
    ")\n",
    "# ## Median annual ratio of DNA to appointments\n",
    ",tbl_medianAnnualRatioDNAtoAppointment_persons AS (\n",
    "    SELECT\n",
    "        DISTINCT tbl_annualCountOfDNAs.person_id\n",
    "        ,PERCENTILE_DISC( (countDNAsPerYear / countAppointmentsPerYear), 0.5) OVER(PARTITION BY tbl_annualCountOfDNAs.person_id) AS medianAnnualRatioDNAtoAppointment\n",
    "    FROM\n",
    "        tbl_annualCountOfDNAs\n",
    "    LEFT OUTER JOIN\n",
    "        tbl_annualCountOfAppointments\n",
    "        ON\n",
    "        (\n",
    "        tbl_annualCountOfDNAs.person_id = tbl_annualCountOfAppointments.person_id\n",
    "        AND tbl_annualCountOfDNAs.year_DNA = tbl_annualCountOfAppointments.year_appointment\n",
    "        )\n",
    ")\n",
    "\n",
    "#######################################################\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "sql_final_select = \\\n",
    "\"\"\"\n",
    "# Finally, we use the above CTEs to define a table with one row per patient and one column for each\n",
    "# feature set. The feature-set columns are populated by interger values with '1' indicating that the\n",
    "# feature set is satisfied and '0' indicating otherwise.\n",
    "SELECT\n",
    "    DISTINCT tbl_persons.person_id\n",
    "    ,countAppointmentsPreviousYear\n",
    "    ,medianAnnualCountAppointments\n",
    "    ,countDNAsPreviousYear\n",
    "    ,medianAnnualCountDNAs\n",
    "    ,ratioDNAtoAppointmentPreviousYear\n",
    "    ,medianAnnualRatioDNAtoAppointment\n",
    "    \n",
    "FROM tbl_persons\n",
    "LEFT OUTER JOIN tbl_countAppointmentsPreviousYear_persons ON tbl_persons.person_id = tbl_countAppointmentsPreviousYear_persons.person_id\n",
    "LEFT OUTER JOIN tbl_medianAnnualCountAppointments_persons ON tbl_persons.person_id = tbl_medianAnnualCountAppointments_persons.person_id\n",
    "LEFT OUTER JOIN tbl_countDNAsPreviousYear_persons ON tbl_persons.person_id = tbl_countDNAsPreviousYear_persons.person_id\n",
    "LEFT OUTER JOIN tbl_medianAnnualCountDNAs_persons ON tbl_persons.person_id = tbl_medianAnnualCountDNAs_persons.person_id\n",
    "LEFT OUTER JOIN tbl_ratioDNAtoAppointmentPreviousYear_persons ON tbl_persons.person_id = tbl_ratioDNAtoAppointmentPreviousYear_persons.person_id\n",
    "LEFT OUTER JOIN tbl_medianAnnualRatioDNAtoAppointment_persons ON tbl_persons.person_id = tbl_medianAnnualRatioDNAtoAppointment_persons.person_id\n",
    "\n",
    "ORDER BY tbl_persons.person_id\n",
    "\"\"\"\n",
    "\n",
    "fs_interview = client.query(sql_CTEs_top + sql_CTEs_body + sql_final_select).to_dataframe().fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "397b3461-4cd0-4dfb-874b-7239d191ad59",
   "metadata": {
    "tags": []
   },
   "source": [
    "#### Entropy-based feature sets potentially indicative of \"chaotic life\"\n",
    "\n",
    "The data from BigQuery needs to be appointments and DNAs tallied in three-month blocks, per person. Specifically, I use BigQuery's built-in `QUARTER()` function for which Q1 = Jan-Mar, Q2 = Apr-Jun, etc.  The query will only return data for quarters in which there was an appointment or a DNA. Each patient's data will be processed in Python to fill in the missing quarters' counts with 0 before calculating the values of the entropy-based feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "29379469-2835-40a6-abf7-78ff56dfe7f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_CTEs_body = \\\n",
    "\"\"\"\n",
    "WITH\n",
    "#  ## Count of appointments, per quarter.\n",
    "tbl_countAppointmentsPerQuarter AS (\n",
    "    SELECT\n",
    "        DISTINCT person_id\n",
    "        ,EXTRACT(YEAR FROM datestart) AS year_appointment\n",
    "        ,EXTRACT(QUARTER FROM datestart) AS quarter_appointment\n",
    "        ,COUNT(DISTINCT datestart) AS countAppointmentsPerQuarter\n",
    "    FROM\n",
    "        \"\"\" + server_id + \"\"\".\"\"\" + database_id + \"\"\".tbl_srappointment\n",
    "    GROUP BY\n",
    "         person_id\n",
    "        ,year_appointment\n",
    "        ,quarter_appointment\n",
    "\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sql_final_select = \\\n",
    "\"\"\"\n",
    "SELECT\n",
    "    person_id\n",
    "    ,year_appointment\n",
    "    ,quarter_appointment\n",
    "    ,countAppointmentsPerQuarter\n",
    "FROM\n",
    "    tbl_countAppointmentsPerQuarter\n",
    "ORDER BY\n",
    "    person_id\n",
    "    ,year_appointment\n",
    "    ,quarter_appointment\n",
    "\"\"\"\n",
    "\n",
    "bq_countAppointmentsPerQuarter = client.query(sql_CTEs_body + sql_final_select).to_dataframe().fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "007263b2-6816-4de9-889b-8d194cb3c9fc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_CTEs_body = \\\n",
    "\"\"\"\n",
    "WITH\n",
    "#  ## Count of did-not-attend (DNA), per quarter.\n",
    "tbl_DNAcodes AS ( \n",
    "    SELECT\n",
    "        snomedcode\n",
    "    FROM\n",
    "        UNNEST([\n",
    "                '\"\"\" + '\\', \\''.join(map(str, codes_to_query_DNA[\"code\"].tolist())) + \"\"\"'\n",
    "                ]) AS snomedcode\n",
    ")\n",
    ",tbl_countDNAsPerQuarter AS ( \n",
    "    SELECT \n",
    "        DISTINCT a.person_id\n",
    "        ,EXTRACT(YEAR FROM dateevent) AS year_DNA\n",
    "        ,EXTRACT(QUARTER FROM dateevent) AS quarter_DNA\n",
    "        ,COUNT( DISTINCT EXTRACT(DATE FROM dateevent) ) AS countDNAsPerQuarter\n",
    "    FROM\n",
    "        \"\"\" + server_id + \"\"\".\"\"\" + database_id + \"\"\".tbl_srcode AS a, tbl_DNAcodes\n",
    "    WHERE\n",
    "        a.snomedcode IN (tbl_DNAcodes.snomedcode)\n",
    "    GROUP BY\n",
    "        person_id\n",
    "        ,year_DNA\n",
    "        ,quarter_DNA\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sql_final_select = \\\n",
    "\"\"\"\n",
    "SELECT\n",
    "    person_id\n",
    "    ,year_DNA\n",
    "    ,quarter_DNA\n",
    "    ,countDNAsPerQuarter\n",
    "FROM\n",
    "    tbl_countDNAsPerQuarter\n",
    "ORDER BY\n",
    "    person_id\n",
    "    ,year_DNA\n",
    "    ,quarter_DNA\n",
    "\"\"\"\n",
    "\n",
    "bq_countDNAsPerQuarter = client.query(sql_CTEs_body + sql_final_select).to_dataframe().fillna(0).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "c552bd3e-9f05-4ec3-9166-335e27b3ea60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/EntropyHub/_SpecEn.py:74: RuntimeWarning: invalid value encountered in true_divide\n",
      "  Pxx = Pt[:Fx]/sum(Pt[:Fx])\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Length of values (6) does not match length of index (38)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_4065/109026038.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;31m# Convert the nested list into a pandas.DataFrame.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0mentropyBasedFS\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mls_entropyBased_fs_appts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mls_entropyBased_fs_appts\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 48\u001b[0;31m \u001b[0mentropyBasedFS\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'person_id'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbq_countAppointmentsPerQuarter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mperson_id\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0mentropyBasedFS_DNAs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mls_entropyBased_fs_DNAs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mls_entropyBased_fs_DNAs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36minsert\u001b[0;34m(self, loc, column, value, allow_duplicates)\u001b[0m\n\u001b[1;32m   4416\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loc must be int\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4417\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4418\u001b[0;31m         \u001b[0mvalue\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sanitize_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4419\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_mgr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minsert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4420\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m_sanitize_column\u001b[0;34m(self, value)\u001b[0m\n\u001b[1;32m   4507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4508\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_list_like\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4509\u001b[0;31m             \u001b[0mcom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequire_length_match\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   4510\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msanitize_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mallow_2d\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4511\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/conda/lib/python3.7/site-packages/pandas/core/common.py\u001b[0m in \u001b[0;36mrequire_length_match\u001b[0;34m(data, index)\u001b[0m\n\u001b[1;32m    530\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    531\u001b[0m         raise ValueError(\n\u001b[0;32m--> 532\u001b[0;31m             \u001b[0;34m\"Length of values \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    533\u001b[0m             \u001b[0;34mf\"({len(data)}) \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    534\u001b[0m             \u001b[0;34m\"does not match length of index \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Length of values (6) does not match length of index (38)"
     ]
    }
   ],
   "source": [
    "# Make a FOR loop that will loop through patients to 1) produce their timeline, 2) join their actual appointment/DNA data, \n",
    "# and 3) create the entropy-based feature sets.\n",
    "# This for loop will later be parallelised using the `multiprocessing` library.\n",
    "#\n",
    "# Set storage.\n",
    "ls_entropyBased_fs_appts = [['activeInformation', 'entropyRate', 'spectralEntropy', 'sampleEntropy', 'eoe', 'averageEntropy', 'bubbleEntropy']]\n",
    "ls_entropyBased_fs_DNAs = [['activeInformation', 'entropyRate', 'spectralEntropy', 'sampleEntropy', 'eoe', 'averageEntropy', 'bubbleEntropy']]\n",
    "ls_pids = set(numpy.concatenate((bq_countAppointmentsPerQuarter.head(100).person_id.unique(), bq_countDNAsPerQuarter.head(100).person_id.unique())))\n",
    "\n",
    "for pid in ls_pids:\n",
    "    # Extract this particular patient's range of active years.\n",
    "    pt_years = \\\n",
    "        bq_countAppointmentsPerQuarter.loc[bq_countAppointmentsPerQuarter.person_id == pid, 'year_appointment'].append(\n",
    "         bq_countDNAsPerQuarter.loc[bq_countDNAsPerQuarter.person_id == pid, 'year_DNA'])\n",
    "    \n",
    "    pt_years_lsrange =  pandas.DataFrame(\n",
    "        data = { 'year' : list( range( min(pt_years), max(pt_years) ) ) }\n",
    "        )\n",
    "    # Create a timeline of years and quarters for this particular patient.\n",
    "    pt_quarters = pandas.DataFrame( data = {'qtr': [1,2,3,4]} )\n",
    "    pt_timeline = pt_years_lsrange.merge(pt_quarters, how = 'cross')\n",
    "    \n",
    "    # Join the patient's actual count of appointments-per-quarter-per-year to their timeline.\n",
    "    pt_appts = bq_countAppointmentsPerQuarter.loc[bq_countAppointmentsPerQuarter.person_id == pid, :]\n",
    "    pt_timeline_appts = \\\n",
    "        pandas.merge(pt_timeline, pt_appts, how = 'left',\n",
    "                     left_on = ['year', 'qtr'],\n",
    "                     right_on = ['year_appointment',\n",
    "                                 'quarter_appointment']).loc[:,'countAppointmentsPerQuarter'].fillna(0).astype(int)\n",
    "    \n",
    "    # Repeat for did-not-attend events.\n",
    "    pt_DNAs = bq_countDNAsPerQuarter.loc[bq_countDNAsPerQuarter.person_id == pid, :]\n",
    "    pt_timeline_DNAs = \\\n",
    "        pandas.merge(pt_timeline, pt_DNAs, how = 'left',\n",
    "                     left_on = ['year', 'qtr'],\n",
    "                     right_on = ['year_DNA',\n",
    "                                 'quarter_DNA']).loc[:,'countDNAsPerQuarter'].fillna(0).astype(int)\n",
    "\n",
    "    # Create the entropy-based feature sets.\n",
    "    # ...\n",
    "    pt_entropyStats_appts = chaoticlifeentropyfs(pt_timeline_appts)\n",
    "    ls_entropyBased_fs_appts.append(pt_entropyStats_appts)\n",
    "    pt_entropyStats_DNAs = chaoticlifeentropyfs(pt_timeline_DNAs)\n",
    "    ls_entropyBased_fs_DNAs.append(pt_entropyStats_DNAs)\n",
    "\n",
    "# Convert the nested list into a pandas.DataFrame.\n",
    "entropyBasedFS = pandas.DataFrame(ls_entropyBased_fs_appts[1:], columns = ls_entropyBased_fs_appts[0])\n",
    "entropyBasedFS.insert(0, 'person_id', bq_countAppointmentsPerQuarter.head(100).person_id.unique())\n",
    "\n",
    "entropyBasedFS_DNAs = pandas.DataFrame(ls_entropyBased_fs_DNAs[1:], columns = ls_entropyBased_fs_DNAs[0])\n",
    "entropyBasedFS_DNAs.insert(0, 'person_id', bq_countDNAsPerQuarter.head(100).person_id.unique())\n",
    "\n",
    "entropyBasedFS.merge(entropyBasedFS_DNAs\n",
    "                    ,how = 'outer'\n",
    "                    ,on = 'person_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aeb7559d-1c96-4c81-a33e-b36a3c4dabef",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "\n",
       "## Filter feature sets not within bounds\n",
       "\n",
       "All feature sets must have a prevalence (or count) between the prevalence (or count) bounds defined in `UNSEEN_feature_sets_prevalence_bounds.ipynb`,\n",
       "i.e. a feature set's count must satisfy:\n",
       "- $18,950\\le$ $count\\ of\\ patients_{feature\\ set_{i}}$ $\\le37,890$, for 'Possible caseness'\n",
       "- $770\\le$ $count\\ of\\ patients_{feature\\ set_{i}}$ $\\le1,530$, for 'Definite caseness'\n",
       "- $19,710\\le$ $count\\ of\\ patients_{feature\\ set_{i}}$ $\\le39,430$, for 'Multinomial caseness'\n",
       "- $770\\le$ $count\\ of\\ patients_{feature\\ set_{i}}$ $\\le1,530$, for 'Possible-vs-Definite caseness'\n"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display message.\n",
    "display(\n",
    "    Markdown(\n",
    "f\"\"\"\n",
    "## Filter feature sets not within bounds\n",
    "\n",
    "All feature sets must have a prevalence (or count) between the prevalence (or count) bounds defined in `UNSEEN_feature_sets_prevalence_bounds.ipynb`,\n",
    "i.e. a feature set's count must satisfy:\n",
    "- ${int(possibleCaseness_count_LB):,}\\le$ $count\\ of\\ patients_{{feature\\ set_{{i}}}}$ $\\le{int(possibleCaseness_count_UB):,}$, for 'Possible caseness'\n",
    "- ${int(definiteCaseness_count_LB):,}\\le$ $count\\ of\\ patients_{{feature\\ set_{{i}}}}$ $\\le{int(definiteCaseness_count_UB):,}$, for 'Definite caseness'\n",
    "- ${int(multinomialCaseness_count_LB):,}\\le$ $count\\ of\\ patients_{{feature\\ set_{{i}}}}$ $\\le{int(multinomialCaseness_count_UB):,}$, for 'Multinomial caseness'\n",
    "- ${int(possdefCaseness_count_LB):,}\\le$ $count\\ of\\ patients_{{feature\\ set_{{i}}}}$ $\\le{int(possdefCaseness_count_UB):,}$, for 'Possible-vs-Definite caseness'\n",
    "\"\"\"\n",
    "       )\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9921532f-455c-44e3-8818-3b779d015100",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Filtering complete for 'Possible caseness'...\n",
      "\t 1  feature sets remain.\n",
      "\t 4  feature sets removed, in total.\n",
      "\t 4  feature sets removed because of low prevalence.\n",
      "\t 0  feature sets removed because of high prevalence.\n",
      "\n",
      "The final list of feature sets from this source is:\n",
      " ['sleepDisturbance']\n",
      "\n",
      " Filtering complete for 'Definite caseness'...\n",
      "\t 0  feature sets remain.\n",
      "\t 5  feature sets removed, in total.\n",
      "\t 1  feature sets removed because of low prevalence.\n",
      "\t 4  feature sets removed because of high prevalence.\n",
      "\n",
      "The final list of feature sets from this source is:\n",
      " []\n",
      "\n",
      " Filtering complete for 'Multi caseness'...\n",
      "\t 1  feature sets remain.\n",
      "\t 4  feature sets removed, in total.\n",
      "\t 4  feature sets removed because of low prevalence.\n",
      "\t 0  feature sets removed because of high prevalence.\n",
      "\n",
      "The final list of feature sets from this source is:\n",
      " ['sleepDisturbance']\n",
      "\n",
      " Filtering complete for 'Possible-vs-Definite caseness'...\n",
      "\t 0  feature sets remain.\n",
      "\t 5  feature sets removed, in total.\n",
      "\t 1  feature sets removed because of low prevalence.\n",
      "\t 4  feature sets removed because of high prevalence.\n",
      "\n",
      "The final list of feature sets from this source is:\n",
      " []\n",
      "Stored 'fs_literature_filteredPossible' (DataFrame)\n",
      "Stored 'fs_literature_filteredDefinite' (DataFrame)\n",
      "Stored 'fs_literature_filteredMulti' (DataFrame)\n",
      "Stored 'fs_literature_filteredPossDef' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "fs_interview_filteredPossible = boundaryfilter(my_featureSet_array = fs_interview, caseness = 'possible', verbose = True)[0]\n",
    "print(\"\\nThe final list of feature sets from this source is:\\n\", fs_interview_filteredPossible.columns.values[1:])\n",
    "fs_interview_filteredDefinite = boundaryfilter(my_featureSet_array = fs_interview, caseness = 'definite', verbose = True)[0]\n",
    "print(\"\\nThe final list of feature sets from this source is:\\n\", fs_interview_filteredDefinite.columns.values[1:])\n",
    "fs_interview_filteredMulti = boundaryfilter(my_featureSet_array = fs_interview, caseness = 'multi', verbose = True)[0]\n",
    "print(\"\\nThe final list of feature sets from this source is:\\n\", fs_interview_filteredMulti.columns.values[1:])\n",
    "fs_interview_filteredPossDef = boundaryfilter(my_featureSet_array = fs_interview, caseness = 'possdef', verbose = True)[0]\n",
    "print(\"\\nThe final list of feature sets from this source is:\\n\", fs_interview_filteredPossDef.columns.values[1:])\n",
    "\n",
    "\n",
    "\n",
    "# Store clinician feature sets for use in other notebooks.\n",
    "%store fs_interview_filteredPossible fs_interview_filteredDefinite fs_interview_filteredMulti fs_interview_filteredPossDef"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c5539d6-31f2-4642-87d4-43585448ba2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ---------------------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-1.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-1:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
