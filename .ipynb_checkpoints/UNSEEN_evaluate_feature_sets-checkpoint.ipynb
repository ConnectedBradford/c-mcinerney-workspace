{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c117869-3004-4f7a-a0aa-2848bb353739",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Evaluating feature sets\n",
    "\n",
    "This notebook contains the subprotocol for evaluating feature sets that have already been filtered in previous stages of the overall protocol. The following are the expected outputs for each feature set:\n",
    "\n",
    "1. A prevalence value per 1,000.\n",
    "2. A [class balance accuracy value](http://search.proquest.com/docview/1500559170?accountid=37552).\n",
    "3. An odds ratio value.\n",
    "4. A positive predictive value.\n",
    "5. A negative predictive value.\n",
    "\n",
    "We also provide the counts that make up the contingency table so that readers can calculate their own evaluation statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e5c0d0-5fa1-4ea5-8d34-f29320a6633f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports and helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "b044e55d-64e9-43d6-92e9-6a2ca43295cc",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('/home/jupyter/UNSEEN/c-mcinerney-workspace')\n",
    "%run 'UNSEEN_helper_functions.ipynb'\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec74e8f-a984-4f7b-8e33-a56ccd8c1559",
   "metadata": {},
   "source": [
    "### Retrieve the retriebable parts of the output table from existing files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e47d7ef6-9221-4791-8baa-968208552220",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "order_options = ['Individuals', 'Pairs', 'Triplets']\n",
    "data = []\n",
    "for i_order in order_options:\n",
    "    fs_order = i_order\n",
    "    data_dir = pathlib.Path(os.getcwd() + '/Mutual information saves/' + fs_order)\n",
    "    globbed_files = data_dir.glob(\"*.parquet\") #creates a list of all csv files\n",
    "    for parquet_file in globbed_files:\n",
    "        read_list = pandas.read_parquet(parquet_file).values.tolist() # Read parquet to list.\n",
    "        read_list = numpy.array(list((map(list, zip(*read_list))))) # Transpose to make list for feature set and list for mutual information.\n",
    "        len_read_list = numpy.shape(read_list)[1]\n",
    "        file_source = os.path.basename(parquet_file)\n",
    "        fs_source, fs_casenessType, fs_representation, dump = file_source.split(\"_\")\n",
    "        temp_list = \\\n",
    "            list(\n",
    "                zip(numpy.repeat(file_source, len_read_list),\n",
    "                    numpy.repeat(fs_source, len_read_list),\n",
    "                    numpy.repeat(fs_order, len_read_list),\n",
    "                    numpy.repeat(fs_casenessType, len_read_list),\n",
    "                    numpy.repeat(fs_representation, len_read_list),\n",
    "                    numpy.array(read_list[:][0]),\n",
    "                    numpy.array(read_list[:][1])\n",
    "                   )\n",
    "        ) # A zip colwise concat of lists.\n",
    "        data.append(temp_list)      \n",
    "        \n",
    "flat_metadata = \\\n",
    "    pandas.DataFrame([item for sublist in data for item in sublist],\n",
    "                     columns = ['file_source',\n",
    "                                'Source',\n",
    "                                'Order',\n",
    "                                'Caseness_type',\n",
    "                                'Representation',\n",
    "                                'Feature_set',\n",
    "                                'Mutual_information']\n",
    "                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4241c931-954b-4872-a11b-2f1c38061bb7",
   "metadata": {},
   "source": [
    "### Check that the required fs_* dataframes exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5b73c712-18d2-4008-b71a-89761ee85050",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "ls_fs_sources = flat_metadata['Source'].unique().tolist()\n",
    "for i_fs_source in ls_fs_sources:\n",
    "    if 'fs_' + i_fs_source not in globals():\n",
    "        script_to_run = \"\\\"./UNSEEN_create_\" + i_fs_source + \"_feature_sets.ipynb\\\"\"\n",
    "        print(script_to_run)\n",
    "        %run $script_to_run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f1004a-4f27-45e1-adae-ab2a902b261a",
   "metadata": {},
   "source": [
    "### For each feature set in 'flat_metadata', extract and append evaluation statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "1c9b1204-8cee-491a-a59d-c8e408009f40",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation statistics saved.\n"
     ]
    }
   ],
   "source": [
    "ls_output = []\n",
    "for i_fs in range(len(flat_metadata)):\n",
    "    # Choose the caseness variable of interest.\n",
    "    caseness_type = flat_metadata['Caseness_type'][i_fs]\n",
    "    if caseness_type == 'multinomial':\n",
    "        vec_caseness = caseness_array['CMHD']\n",
    "    elif caseness_type == 'definite':\n",
    "        vec_caseness = caseness_array['CMHD_dx_and_rx']\n",
    "    elif caseness_type == 'possible':\n",
    "        vec_caseness = caseness_array['CMHD_rx_not_dx']\n",
    "    elif caseness_type == 'control':\n",
    "        vec_caseness = caseness_array['CMHD_control']\n",
    "\n",
    "    \n",
    "    # Choose the feature-set components for the feature set of interest.\n",
    "    fs_components_names = flat_metadata['Feature_set'][i_fs].split(\"-\")\n",
    "    fs_components = fs_literature[fs_components_names]\n",
    "    \n",
    "    # Choose the representation for the feature set of interest.\n",
    "    representation = flat_metadata['Representation'][i_fs]\n",
    "    if representation == 'ALL':\n",
    "        vec_featureSet = fs_components.all(True)\n",
    "    elif representation == \"MULTI\":\n",
    "        vec_featureSet, dump = mutlinomRepresentation(fs_components)\n",
    "    \n",
    "    \n",
    "    # Pass the caseness and the feature-set vectors to evaloutputs().\n",
    "    ls_output.append(evaloutputs(vec_featureSet,\n",
    "                                 vec_caseness)\n",
    "                )\n",
    "# Flatten the appended list of evaluation statistics.\n",
    "flat_output = \\\n",
    "    pandas.DataFrame(ls_output,\n",
    "                     columns = ['prevalence per thousand',\n",
    "                                'cba',\n",
    "                                'odds ratio',\n",
    "                                'ppv',\n",
    "                                'npv',\n",
    "                                'tn',\n",
    "                                'fn',\n",
    "                                'fp',\n",
    "                                'tp']\n",
    "                    )\n",
    "\n",
    "# Append the evaluation statistics to the metadata about the feature set.\n",
    "evaluation_dataframe = pandas.concat([flat_metadata, flat_output], axis=1, join='inner')\n",
    "#display(evaluation_dataframe)\n",
    "\n",
    "# Rename column names for saving.\n",
    "evaluation_dataframe.rename = \\\n",
    "                    ['Order',\n",
    "                     'Caseness_type',\n",
    "                     'Representation',\n",
    "                     'Feature_set',\n",
    "                     'Mutual_information',\n",
    "                     'FeatureSet_prevalence_per_thousand',\n",
    "                     'Class_balance_acccuracy',\n",
    "                     'Odds_ratio',\n",
    "                     'Positive_predictive_value',\n",
    "                     'Negative_predictive_value',\n",
    "                     'True_positive_count',\n",
    "                     'True_negativecount',\n",
    "                     'False_positive_count',\n",
    "                     'False_negative_count']\n",
    "\n",
    "# Save evaluation outputs.\n",
    "savelocation = \"Evaluation/\"\n",
    "evaluation_dataframe.to_csv(savelocation + datetime.strftime(datetime.now(), '%Y_%m_%d_%H:%M:%S') + \"_Evaluation statistics.csv\", index = False)\n",
    "evaluation_dataframe.astype(str).to_parquet(savelocation + datetime.strftime(datetime.now(), '%Y_%m_%d_%H:%M:%S') + \"_Evaluation statistics.parquet\", index = False)\n",
    "print(\"\\nEvaluation statistics saved.\")"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-1.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-1:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
