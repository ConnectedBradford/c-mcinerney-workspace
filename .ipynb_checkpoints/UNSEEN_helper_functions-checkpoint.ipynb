{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fd3604-adb9-4e79-881c-9e18988d2e64",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "\n",
    "This notebook collates all the functions that help the other notebooks do their thing, without clogging the other notebooks with function definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad1828-3272-4b6b-b8c4-7557b24bfe80",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ece23826-2a4d-451b-8002-501a226f745c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy\n",
    "import pandas\n",
    "pandas.set_option('display.max_colwidth', None)\n",
    "from datetime import date, datetime\n",
    "import itertools\n",
    "import scipy.stats\n",
    "import sklearn.metrics\n",
    "import math\n",
    "import os\n",
    "import matplotlib.pyplot\n",
    "from google.cloud import bigquery\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from tqdm import tqdm\n",
    "import pyarrow.parquet\n",
    "import pathlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b06dd66-0f5e-414f-ac99-7a918e2009d3",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a3885-d79d-4a12-be5a-63ef2e2a9981",
   "metadata": {},
   "source": [
    "### Compute and present the entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e423811f-5800-42c5-9f0e-a3ddd889fbc3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to compute and present the entropy of a column in a pandas.Dataframe.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. my_df_column:            A column from a pandas.Dataframe containing the variable\n",
    "#                             for which entropy needs calculating.\n",
    "#\n",
    "# RETURNS\n",
    "# 1. entropy_caseness_scaled: Entropy scaled to the theoretical maximum for a binary variable.\n",
    "# 2. entropy_caseness:        Entropy in nats.\n",
    "#\n",
    "\n",
    "def entropy_output(my_df_column):\n",
    "    entropy_caseness = scipy.stats.entropy(my_df_column.value_counts(), base = math.e)\n",
    "    entropy_caseness_scaled = round(entropy_caseness / math.log(2, math.e) * 100, 3)\n",
    "    if entropy_caseness < 0.001:\n",
    "        print(\"\\t Caseness variable entropy < 0.001 nats\")\n",
    "    else:\n",
    "        print(\"\\t Caseness variable entropy =\", round(entropy_caseness, 3), \"nats\")\n",
    "    if entropy_caseness < 0.001:\n",
    "        print(\"\\t Caseness variable scaled entropy < 0.001 %\")\n",
    "    else:\n",
    "        print(\"\\t Caseness variable scaled entropy =\", entropy_caseness_scaled, \"%\")\n",
    "    \n",
    "    return entropy_caseness, entropy_caseness_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2798124-2ccd-4129-9c23-1b8afcb61ea0",
   "metadata": {},
   "source": [
    "### Compute and present the hit rate - a.k.a. (mis)classification error - of a caseness variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3db7dc06-6195-4223-80e2-2ad15604f81e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to compute and present the hitrate - a.k.a. (mis)classification\n",
    "# error - of a caseness variable.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. my_caseness_variable:  A column from a pandas.Dataframe containing patients'\n",
    "#                           caseness values.\n",
    "#\n",
    "# RETURNS\n",
    "# 1. hitRate_none:          The hitrate assuming no one demonstrates the caseness.\n",
    "# 2. hitRate_all:           The hitrate assuming everyone demonstrates the caseness.\n",
    "#\n",
    "\n",
    "def hitrate_output(my_caseness_variable):\n",
    "    numerator = (my_caseness_variable != 0).sum()\n",
    "    denominator = len(my_caseness_variable)\n",
    "    hitRate_all = (numerator / denominator) * 100\n",
    "    hitRate_none = 100 - hitRate_all\n",
    "    Odds_noYes = hitRate_none / hitRate_all\n",
    "    if hitRate_all < 0.001:\n",
    "        print(\"\\t Hit rate (all) < 0.001 %\")\n",
    "        print(\"\\t Hit rate (none) \\u2248 100 %\")\n",
    "        print(\"\\t Odds (No CMHD : CMHD) \\u2248 infitely-times less likely to have CMHD than to have it.\")\n",
    "    elif hitRate_all:\n",
    "        print(\"\\t Hit rate (all) =\", round(hitRate_all, 3), \"%\")\n",
    "        print(\"\\t Hit rate (none) =\", round(hitRate_none, 3), \"%\")\n",
    "        print(\"\\t Odds (No CMHD : CMHD) =\", f'{int(Odds_noYes):,}', \"-times less likely to have CMHD than to have it.\")\n",
    "    else:\n",
    "        print(\"\\t Hit rate (all) =\", round(hitRate_all, 3), \"%\")\n",
    "        print(\"\\t Hit rate (none) =\", round(hitRate_none, 3), \"%\")\n",
    "        print(\"\\t Odds (No CMHD : CMHD) =\", f'{int(Odds_noYes):,}', \"-times less likely to have CMHD than to have it.\")\n",
    "    \n",
    "    return hitRate_none, hitRate_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f9b2573-22d8-4151-8f79-16ba39b60b80",
   "metadata": {},
   "source": [
    "### Filter out feature sets that are not within the prevalence bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29853444-5d86-4d07-bf2f-26b174093610",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to filter out feature sets that are not within the prevlance bounds.\n",
    "#\n",
    "# The function counts the non-zero elements of each feature set can compares it \n",
    "# to the minimum and maximum count criteria.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. my_featureSet_array:       An n-by-fs pandas.Datqframe of n patients represented\n",
    "#                               by rows and fs features represented by columns.\n",
    "# 2. verboase:                  An optional argument to indicate whether the user \n",
    "#                               wants feedback on how many feature sets were removed.\n",
    "#\n",
    "# RETURNS\n",
    "# 1. filtered_featureSet_array: The inputted feature-set array but with the prevlance\n",
    "#                               violating feature sets removed.\n",
    "# 2. fs_removed_lower:          A numpy array of the names of feature sets removed \n",
    "#                               because their prevalence was too low.\n",
    "# 3. n_fs_removed_lower:        The count of feature sets removed for high low\n",
    "#                               prevalence.\n",
    "# 4. fs_removed_upper:          A numpy array of the namesof feature sets removed \n",
    "#                               because their prevalence was too high.\n",
    "# 5. n_fs_removed_upper:        The count of feature sets removed for having high\n",
    "#                               prevalence.\n",
    "#\n",
    "\n",
    "def boundaryfilter(my_featureSet_array, verbose = True):\n",
    "    # Identify the feature sets that are too few, and extract the column names.\n",
    "    fs_removed_lower = \\\n",
    "        my_featureSet_array.loc[:, \n",
    "        numpy.insert(\n",
    "        ((my_featureSet_array.loc[:, my_featureSet_array.columns != 'person_id'] != 0).sum(axis=0) < min_criterion_count).values\n",
    "            ,0\n",
    "            ,False)\n",
    "                   ].columns\n",
    "    # Extract the count of feature sets that are too few.\n",
    "    n_fs_removed_lower = len(fs_removed_lower)\n",
    "\n",
    "    # Identify the feature sets that are too many, and extract the column names.\n",
    "    fs_removed_upper = \\\n",
    "        my_featureSet_array.loc[:, \n",
    "        numpy.insert(\n",
    "        ((my_featureSet_array.loc[:, my_featureSet_array.columns != 'person_id'] !=0).sum(axis=0) > max_criterion_count).values\n",
    "            ,0\n",
    "            ,False)\n",
    "                   ].columns\n",
    "    # Extract the count of feature sets that are too many.\n",
    "    n_fs_removed_upper = len(fs_removed_upper)\n",
    "\n",
    "    # Remove the feature sets that are no within the prevalence bounds.\n",
    "    filtered_featureSet_array = \\\n",
    "        pandas.DataFrame(my_featureSet_array.drop(columns = numpy.insert(fs_removed_lower, 0, fs_removed_upper)))\n",
    "    \n",
    "    # Print message if arg{verbose} = True.\n",
    "    if verbose == True:\n",
    "        print(\"\\n Filtering complete...\")\n",
    "        print(\"\\t\", len(filtered_featureSet_array.columns)-1, \" feature sets remain.\")\n",
    "        print(\"\\t\", n_fs_removed_lower+n_fs_removed_upper, \" feature sets removed, in total.\")\n",
    "        print(\"\\t\", n_fs_removed_lower, \" feature sets removed because of low prevalence.\")\n",
    "        print(\"\\t\", n_fs_removed_upper, \" feature sets removed because of high prevalence.\")\n",
    "    \n",
    "    # Return outputs\n",
    "    return [filtered_featureSet_array, fs_removed_lower, n_fs_removed_lower, fs_removed_upper, n_fs_removed_upper]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c65f056-f60b-4246-ae69-4f706b110e93",
   "metadata": {},
   "source": [
    "### Compute the evaluation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9c87092f-8174-4f8d-8c34-90aa46f626a6",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to compute the evalaution outputs. The function automatically saves the\n",
    "# contingency table and also returns it.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. vec_featureSet:      A column from a pandas.Dataframe containing the feature set\n",
    "#                         that needs evaluating.\n",
    "# 2. vec_caseness:        A column from a pandas.Dataframe containing the caseness\n",
    "#                         variable of interest.\n",
    "# 3. savelocation:        The folder location where the output should be saved.\n",
    "#\n",
    "# RETURNS\n",
    "# 1. prevalence:          The proportion of patients satisfying the definition of the\n",
    "#                         feature set.\n",
    "# 2. cba:                 Class balanced accuracy - the lower bound of the average\n",
    "#                         sensitivity and average positive predictive value\n",
    "#                         (a.k.a. precision) for all caseness values.\n",
    "# 3. oddsRatio:           The ratio of the odds of caseness given the presence of feature\n",
    "#                         set, to the odds of CMHD given the absence of the feature set.\n",
    "#                         It can also be thought of as the multiplicative difference\n",
    "#                         between correct and incorrect classification.\n",
    "# 4. ppv:                 The proportion of patients satisfying the definition of the\n",
    "#                         feature set who satisfy the caseness.\n",
    "# 5. npv:                 The proportion of patients who do not satisfy the definition\n",
    "#                         of the feature set who do not satisfy the caseness.\n",
    "# 6. tn:                  The count of true negatives, i.e. the count of patients whose \n",
    "#                         feature-set value and caseness value are both zero.\n",
    "# 7. fn:                  The count of false negatives, i.e. the count of patients whose \n",
    "#                         feature-set value is zero but whose caseness value is one.\n",
    "# 8. fp:                  The count of false positives, i.e. the count of patients whose \n",
    "#                         feature-set value is one but whose caseness value is zero.\n",
    "# 9. tp:                  The count of true positives, i.e. the count of patients whose \n",
    "#                         feature-set value and caseness value are both one.\n",
    "#\n",
    "def evaloutputs(vec_featureSet,\n",
    "                vec_caseness):\n",
    "    # ## Assess argument validty.\n",
    "    \n",
    "    # Check that both vectors are the same length.\n",
    "    if len(vec_featureSet) != len(vec_caseness):\n",
    "        print(\"\\n**\",\n",
    "              \"Feature-set and caseness vectors are of different lengths.\",\n",
    "             \"**\\n\")\n",
    "        return\n",
    "    \n",
    "    # ## Contingency table.\n",
    "    # Make contingency table.\n",
    "    contingencyTable = \\\n",
    "        pandas.crosstab(\n",
    "            index = vec_featureSet,\n",
    "            columns = vec_caseness\n",
    "    )\n",
    "    \n",
    "    # Extract components of contingency table\n",
    "    tn = contingencyTable.loc[0,0]\n",
    "    fn = contingencyTable.loc[0,1]\n",
    "    fp = contingencyTable.loc[1,0]\n",
    "    tp = contingencyTable.loc[1,1]\n",
    "    \n",
    "    # ## Compute outputs.\n",
    "    \n",
    "    # Prevalence value per 1,000.\n",
    "    #\n",
    "    # I use 1 minus the prevalence of zeros because that\n",
    "    # combines all the possibly-many values that indicate\n",
    "    # the presence of the feature set.\n",
    "    prevalence = \\\n",
    "        (1 - (sum(vec_featureSet == 0) / len(vec_featureSet))) * 10\n",
    "    if prevalence < 0.01:\n",
    "         prevalence = '< 0.01'\n",
    "    else:\n",
    "         prevalence = round(prevalence, 2)\n",
    "    \n",
    "    # Class balance accuracy.\n",
    "    cba = \\\n",
    "        round( 0.5 * \\\n",
    "              ( (tp / max( (tp + fn), (tp + fp) ) ) + \\\n",
    "               (tn / max( (tn + fp), (tn +fn) ) ) ), 2)\n",
    "    if cba < 0.01:\n",
    "        cba = '< 0.01'\n",
    "    \n",
    "    # Odds ratio.\n",
    "    if min( (tp * tn) , (fp * fn) ) == 0:\n",
    "        oddsRatio = 'Undefined: One of the odds is zero.'\n",
    "    else:\n",
    "        oddsRatio = round( (tp * tn) / (fp * fn), 2)\n",
    "    \n",
    "    # Positive predictive value.\n",
    "    ppv = 0.00 if (tp + fp) == 0 else tp / (tp + fp)\n",
    "    if ppv > 0 and ppv < 0.01:\n",
    "        ppv = '< 0.01'\n",
    "    elif ppv < 1 and ppv > 0.999:\n",
    "        ppv = '\\u2248 1.00'\n",
    "    else:\n",
    "         ppv = round(ppv, 2)\n",
    "         \n",
    "    # Negative predictive value.\n",
    "    npv = 0.00 if (tn + fn) == 0 else tn / (tn + fn)\n",
    "    if npv > 0 and npv < 0.01:\n",
    "        npv = '< 0.01'\n",
    "    elif npv < 1 and npv > 0.999:\n",
    "        npv = '\\u2248 1.00'\n",
    "    else:\n",
    "         npv = round(npv, 2)\n",
    "    \n",
    "    \n",
    "    \n",
    "    return prevalence, cba, oddsRatio, ppv, npv, tn, fn, fp, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b961cd6f-146b-44d2-b653-ccc0dfacfb65",
   "metadata": {},
   "source": [
    "### Iterate through the three caseness variables with a given feature set, and call the evalaution function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4613c195-2446-4784-beaa-4bf934be918f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to iterate through the three caseness variables with a given feature set\n",
    "# and call the evaluation function, evaloutputs.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. vec_featureSet:      A column from a pandas.Dataframe containing the feature set\n",
    "#                         that needs evaluating.\n",
    "# 2. vec_caseness:        A column from a pandas.Dataframe containing the caseness\n",
    "#                         variable of interest.\n",
    "# 3. savelocation:        The folder location where the output should be saved.\n",
    "#\n",
    "# RETURNS\n",
    "# n/a\n",
    "#\n",
    "\n",
    "def evaleachcaseness(vec_featureSet,\n",
    "                     array_caseness,\n",
    "                     savelocation = None):\n",
    "    counter = 0\n",
    "    for vec_caseness in array_caseness[[\"CMHD_dx_and_rx\", \"CMHD_rx_not_dx\",\"CMHD_control\"]]:\n",
    "        contingencyTable,\n",
    "        prevalence[counter],\n",
    "        cba[counter],\n",
    "        oddsRatio[counter],\n",
    "        ppv[counter],\n",
    "        npv[counter] = \\\n",
    "            evaloutputs(vec_featureSet,\n",
    "                        vec_caseness,\n",
    "                        savelocation = None)\n",
    "        counter = counter + 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fc34d6a-9745-4b0c-902e-0f6d478fd3ea",
   "metadata": {},
   "source": [
    "### Compute the multinomial representation of a feature set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "acf2b93b-bbc0-4d3a-995d-9d75763f2bee",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to compute the multinomial representation of a feature set.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. var_vals:      An n-by-fs pandas.Dataframe of n patients and fs\n",
    "#                   features, containing the feature sets that we want to\n",
    "#                   compress into a single, multinomial representation.\n",
    "#\n",
    "# RETURNS\n",
    "# 1. featureSet:    An n-by-1 pandas.Dataframe containing a multinomial\n",
    "#                   representation of the inputted feature sets.\n",
    "# 2. next_iter:     An indicator variable that is used by the parent\n",
    "#                   function featuresetmi().\n",
    "#\n",
    "def mutlinomRepresentation(var_vals):\n",
    "    # Check that the variables have more than three values and\n",
    "    # only progress if False.\n",
    "    for i_col in range(var_vals.shape[1]-1):\n",
    "        unique_feature_vals = var_vals.iloc[:, i_col].drop_duplicates()\n",
    "        if (len(unique_feature_vals) > 3):\n",
    "            print(\"\\n** Error: At least one of the\",\n",
    "                  \"component features has more than\",\n",
    "                  \"three values so the multinomial\",\n",
    "                  \"representation will not be computed.**\")\n",
    "            print(\"Offending variable:\", var_vals.columns.values[i_col],\n",
    "                 \":\", var_vals.columns.values[i_col].unique())\n",
    "            unique_feature_vals\n",
    "            next_iter = True\n",
    "            return 0, next_iter\n",
    "\n",
    "    # Get all combinations of values of the component features\n",
    "    # and define feature set values for each multinomial combination.\n",
    "    feature_combins = var_vals.drop_duplicates()\n",
    "    feature_combins =\\\n",
    "        pandas.DataFrame(data = feature_combins, columns = var_vals.columns)\\\n",
    "        .reset_index()\\\n",
    "        .drop(['index'], axis = 1)\n",
    "    feature_combins['multinom_vals'] = feature_combins.index\n",
    "\n",
    "\n",
    "    # Define a vector indicating the feature set value.\n",
    "    myMerge =\\\n",
    "        pandas.merge(\n",
    "            var_vals,\n",
    "            feature_combins,\n",
    "            how = 'left',\n",
    "            on = list(var_vals.columns.values)\n",
    "    )\n",
    "\n",
    "    # Extract multinomial representation as output variable.\n",
    "    featureSet = myMerge['multinom_vals']\n",
    "    next_iter = False\n",
    "    return featureSet, next_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1871c6b3-0549-44fc-97f0-65a86505ca58",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate two-way mutual information for a features of order m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc083a26-9003-4931-89bc-3f6c81516b21",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to calculate the two-way mutual information for a feature set.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. featureSet_array:   An n-by-fs pandas.Dataframe of n patients and fs feature\n",
    "#                        sets, or an fs-by-1 pandas.Dataframe containing the names\n",
    "#                        of feature sets. If the fs-by-1 dataframe, then it is\n",
    "#                        assumed that the feature sets are SNOMED-CT codes that \n",
    "#                        can be queried in the WHERE clause of BigQuery syntax.\n",
    "# 2. casenessVector:     A column from a pandas.Dataframe containing the caseness\n",
    "#                        variable of interest.\n",
    "# 3. m:                  The order of feature set to be tested. 1 = Individuals, \n",
    "#                        2 = Pairs, 3 = Triplets.\n",
    "# 4. savelocation:       The folder location where the output should be saved.\n",
    "# 5. representation:     A choice of {'all', 'multi'} where 'all' = the feature set\n",
    "#                        value is 1 when all components are 1, or 0 otherwise, and\n",
    "#                        'multi' = the feature set values represent each combination\n",
    "#                        of components' values.\n",
    "# 6. source:             The source of the feature set: {'database', 'clinicial',\n",
    "#                        'literature', 'interviews', 'PPI'}.\n",
    "# 7. verbose:            An optional argument to indicate whether the user wants\n",
    "#                        feedback on how many feature sets were removed and saved.\n",
    "#\n",
    "# RETURNS\n",
    "# n/a\n",
    "#\n",
    "def featuresetmi(featureSet_array,\n",
    "                 casenessVector,\n",
    "                 m = None,\n",
    "                 savelocation = None,\n",
    "                 representation = None,\n",
    "                 source = None,\n",
    "                 verbose = False):\n",
    "    # ## Assess argument validty.\n",
    "    \n",
    "    # Check order of feature set. If not provided,\n",
    "    # default to m = 1.    \n",
    "    if m == None:\n",
    "        order_int = 1\n",
    "        order_label = \"Individuals\"\n",
    "        print(\"\\nNo value for m provided.\" +\n",
    "              \"\\n...Default value of m = 1 will be used.\")\n",
    "    elif m == 1:\n",
    "        order_int = m\n",
    "        order_label = \"Individuals\"\n",
    "    elif m == 2:\n",
    "        order_int = m\n",
    "        order_label = \"Pairs\"\n",
    "    elif m == 3:\n",
    "        order_int = m\n",
    "        order_label = \"Triplets\"\n",
    "    else:\n",
    "        print(\"\\n** Error: Integer value between 1\",\n",
    "              \"and 3 not supplied for m.**\\n\")\n",
    "        return\n",
    "\n",
    "    # Check and set save location.\n",
    "    if savelocation == None:\n",
    "        savelocation = \\\n",
    "           (\"Mutual information saves/\"+\\\n",
    "            order_label)\n",
    "        print(\"\\nNo save location provided.\" +\n",
    "              \"\\n...Defaulting to ~/\" + savelocation)    \n",
    "\n",
    "    # ## Check encoding. If not provided, \n",
    "    # ## default to OR encoding.\n",
    "    if representation == None:\n",
    "        representation_label = \"ALL\"\n",
    "        print(\"\\nNo representation provided.\" +\n",
    "              \"\\n...Defaulting to '\" + representation_label + \"' representation.\")\n",
    "    elif representation == \"all\":\n",
    "        representation_label = \"ALL\"\n",
    "    elif representation == \"multi\":\n",
    "        representation_label = \"MULTI\"\n",
    "    else:\n",
    "        print(\"\\n** Error: Representation value from \",\n",
    "              \"{'and', 'multi'} not provided.**\\n\")\n",
    "        return\n",
    "    \n",
    "    # ## Check the source argument is provided.\n",
    "    if source == None:\n",
    "        print(\"\\n** Error: No source argument provided.\",\n",
    "              \"**\\n\")\n",
    "        return\n",
    "    \n",
    "    # ## Set save string for particular caseness variable.\n",
    "    caseness_type = casenessVector.columns.values[-1]\n",
    "    if caseness_type == 'CMHD': \n",
    "        caseness_label = 'multinomial'\n",
    "    elif caseness_type == 'CMHD_dx_and_rx': \n",
    "        caseness_label = 'definite'\n",
    "    elif caseness_type == 'CMHD_rx_not_dx': \n",
    "        caseness_label = 'possible'\n",
    "    elif caseness_type == 'CMHD_control': \n",
    "        caseness_label = 'control'\n",
    "    \n",
    "    print(\"\\n\\n\\n****************************************\")  \n",
    "    print(\"Calculating mutual information values...\")\n",
    "    \n",
    "    # Check if the supplied feature set array is an n-by-fs array\n",
    "    # of feature sets, or an fs-by-1 vector of feature-set names.\n",
    "    # If it is the fs-by-1 vector, then pass all arguments to the\n",
    "    # appropriate function; else, continue with the code.\n",
    "    if featureSet_array.shape[1] == 1:\n",
    "        featuresetmi_database(featureSet_vector_of_names = featureSet_array,\n",
    "                              casenessVector = casenessVector,\n",
    "                              m = m,\n",
    "                              representation_label = representation_label,\n",
    "                              verbose = verbose)\n",
    "    else:\n",
    "        # Instantiate specific storage for mutual information.\n",
    "        featureSet_MI = \\\n",
    "            pandas.DataFrame(columns = ['Feature_set', 'Mutual_information'])\n",
    "\n",
    "        # Instantiate batch number.\n",
    "        batch = 0\n",
    "\n",
    "        # Instantiate tally of feature sets that are dropped due to low entropy.\n",
    "        drop_tally = 0\n",
    "\n",
    "        # Define entropy of the particular caseness variable.\n",
    "        entropy_caseness = \\\n",
    "            scipy.stats.entropy(casenessVector.iloc[:,-1].value_counts(),\n",
    "                                base = math.e)\n",
    "        if verbose == True:\n",
    "            print(\"Threshold f_MI is\",entropy_caseness)\n",
    "            \n",
    "        # Ensure feature-set and casenesss values are matched for person_id.\n",
    "        full_array = pandas.merge(casenessVector,\n",
    "                                  featureSet_array,\n",
    "                                  on = 'person_id',\n",
    "                                  how = 'left')  \n",
    "\n",
    "        # Define the m-way tuples of features sets as a numpy array. We will loop\n",
    "        # through the rows of this array to create the feature sets.\n",
    "        combins = \\\n",
    "            numpy.asarray(\n",
    "                list(\n",
    "                    itertools.combinations(\n",
    "                        featureSet_array.columns[featureSet_array.columns != 'person_id'],\n",
    "                        order_int)\n",
    "                    )\n",
    "                )\n",
    "\n",
    "        # ## loop through the feature sets.\n",
    "        for i_fs in tqdm(range(len(combins))):\n",
    "            \n",
    "            # Define an array indicating the feature set value.\n",
    "            var_vals = full_array[combins[i_fs]]\n",
    "\n",
    "            # Name the feature set.\n",
    "            name_var = '-'.join(var_vals.columns.values)\n",
    "\n",
    "            if representation_label == \"ALL\":\n",
    "                fs_val = var_vals.all(True)\n",
    "            elif representation_label == \"MULTI\":\n",
    "                fs_val, next_iter = mutlinomRepresentation(var_vals)\n",
    "                if next_iter:\n",
    "                    drop_tally += 1\n",
    "                    if verbose == True:\n",
    "                        print(\"Dropped\", name_var, \"because at least one\",\n",
    "                              \"component features has more than three values\")\n",
    "                    continue\n",
    "\n",
    "\n",
    "            # Calculate the mutual information between the feature set and\n",
    "            # caseness variable.\n",
    "            f_MI = sklearn.metrics.mutual_info_score(fs_val, full_array.iloc[:,-1])\n",
    "\n",
    "            if f_MI < entropy_caseness:\n",
    "                drop_tally += 1\n",
    "                if verbose == True:\n",
    "                    print(\"Dropped\",name_var,\"because f_MI =\", f_MI)\n",
    "                continue\n",
    "            else:            \n",
    "                # Store the name and mutual information value.\n",
    "                if verbose == True:\n",
    "                    print(\"\\tSaved\",name_var,\"because f_MI =\", f_MI)\n",
    "                featureSet_MI.loc[len(featureSet_MI),:] = name_var, f_MI\n",
    "\n",
    "            if len(featureSet_MI) > 9:\n",
    "                    # Increment batch.\n",
    "                    batch += 1\n",
    "\n",
    "                    # Make an interim save of results.\n",
    "                    current_dir = os.getcwd()\n",
    "                    print(current_dir + savelocation)\n",
    "                    os.chdir(current_dir + \"/\" + savelocation)\n",
    "                    pyarrow.parquet.write_table(pyarrow.Table.from_pandas(featureSet_MI),   \n",
    "                                                source + \"_\" +\n",
    "                                                caseness_label + \"_\" +\n",
    "                                                representation_label + \"_\" + \n",
    "                                                \"batch\" + \n",
    "                                                str(batch) + \n",
    "                                                \".parquet\")\n",
    "                    os.chdir(current_dir)\n",
    "                    featureSet_MI.to_csv(savelocation + \"/\" +\n",
    "                                         source + \"_\" +\n",
    "                                         caseness_label + \"_\" + \n",
    "                                         representation_label + \"_\" + \n",
    "                                         \"batch\" + \n",
    "                                         str(batch) + \n",
    "                                         \".csv\", index = False)\n",
    "                    print(\"\\nInterim save made\")\n",
    "                    # Instantiate new storage.\n",
    "                    featureSet_MI = \\\n",
    "                        pandas.DataFrame(columns = ['Feature_set', 'Mutual_information'])\n",
    "                    \n",
    "    # Increment counter.\n",
    "    batch += 1\n",
    "    # Final save.\n",
    "    if len(featureSet_MI) != 0:\n",
    "        current_dir = os.getcwd()\n",
    "        print(current_dir + savelocation)\n",
    "        os.chdir(current_dir + \"/\" + savelocation)\n",
    "        pyarrow.parquet.write_table(pyarrow.Table.from_pandas(featureSet_MI),\n",
    "                                    source + \"_\" +\n",
    "                                    caseness_label + \"_\" +\n",
    "                                    representation_label + \"_\" + \n",
    "                                    \"batch\" + \n",
    "                                    str(batch) + \n",
    "                                    \".parquet\")\n",
    "        os.chdir(current_dir)\n",
    "        featureSet_MI.to_csv(savelocation + \"/\" +\n",
    "                             source + \"_\" +\n",
    "                             caseness_label + \"_\" +\n",
    "                             representation_label + \"_\" +\n",
    "                             \"batch\" + \n",
    "                             str(batch) + \n",
    "                             \".csv\", index = False)\n",
    "\n",
    "    # Feedback messages.\n",
    "    print(\"...\\n\")\n",
    "    print(str(batch), \"batch(es) of feature sets processed.\")\n",
    "    print(str(drop_tally), \"/\",\n",
    "          str(len(combins)),\n",
    "          \"feature sets dropped due to low entropy.\")\n",
    "    print(\"****************************************\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f639109-1435-4cda-9578-c42428d0938c",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Calculate two-way mutual information for a *database* features of order m."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c313de-ba6f-40d1-ae6c-da7ca7257fdd",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to calculate the two-way mutual information for the database feature sets.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. featureSet_vector_of_names:   An fs-by-1 pandas.Dataframe containing the names\n",
    "#                                  of feature sets.\n",
    "# 2. casenessVector:               A column from a pandas.Dataframe containing the caseness\n",
    "#                                  variable of interest.\n",
    "# 3. m:                            The order of feature set to be tested. 1 = Individuals, \n",
    "#                                  2 = Pairs, 3 = Triplets.\n",
    "# 4. representation_label:         A choice of {'all', 'multi'} where 'all' = the feature set\n",
    "#                                  value is 1 when all components are 1, or 0 otherwise, and\n",
    "#                                  'multi' = the feature set values represent each combination\n",
    "#                                  of components' values.\n",
    "# 5. verbose:                      An optional argument to indicate whether the user wants\n",
    "#                                  feedback on how many feature sets were removed and saved.\n",
    "#\n",
    "# RETURNS\n",
    "# n/a\n",
    "#\n",
    "def featuresetmi_database(featureSet_vector_of_names,\n",
    "                          casenessVector,\n",
    "                          m,\n",
    "                          representation_label,\n",
    "                          source,\n",
    "                          verbose):\n",
    "    # ## Assess argument validity.\n",
    "    # Argument validty was already assessed in parent function.\n",
    "\n",
    "    \n",
    "    # Instantiate specific storage for mutual information.\n",
    "    featureSet_MI = \\\n",
    "        pandas.DataFrame(columns = ['Feature_set', 'Mutual_information'])\n",
    "    \n",
    "    # Instantiate batch number.\n",
    "    batch = 0\n",
    "\n",
    "    # Instantiate tally of feature sets that are dropped due to low entropy.\n",
    "    drop_tally = 0\n",
    "    \n",
    "    # Define entropy of the particular caseness variable.\n",
    "    entropy_caseness = \\\n",
    "        scipy.stats.entropy(casenessVector.iloc[:,-1].value_counts(),\n",
    "                            base = math.e)\n",
    "    if verbose == True:\n",
    "        print(\"Threshold f_MI is\",entropy_caseness)\n",
    "        \n",
    "    # Define the feature-set combinations that should be considered.\n",
    "    fs_1_indices = range(len(featureSet_vector_of_names))\n",
    "    if m == 2:\n",
    "        fs_2_indices = fs_1_indices[1:]\n",
    "    elif m == 3:\n",
    "        fs_2_indices = fs_1_indices[1:]\n",
    "        fs_3_indices = fs_1_indices[2:]     \n",
    "        \n",
    "    # ## loop through the feature sets.\n",
    "    for i_fs_1 in tqdm(fs_1_indices, desc = \"First feature-set list\", unit = \"feature set\"): # We know that at least the Individual iterable is needed.\n",
    "        \n",
    "        # This code block defines the featureSet_array, depending on the order of feature set required\n",
    "        ##########################################################################\n",
    "        if m != 1: # If Pairs or Triplets are desired...\n",
    "            for i_fs_2 in tqdm(fs_2_indices, desc = \"Second feature-set list\", unit = \"feature set\"): # ...we infer that at least Pairs are desired so we create the Pairs iterable.\n",
    "                if m != 3: # If Triplets are not desired, we can infer that Pairs are desired, so we run the Pairs code.\n",
    "                    ##############\n",
    "                    # PAIRS CODE #\n",
    "                    ##############\n",
    "                    # Define the list of SNOMED-CT codes to be queried.\n",
    "                    fs_df = featureSet_vector_of_names.iloc[[i_fs_1, i_fs_2]]\n",
    "                    # Retrieve a feastureSet_array\n",
    "                    featureSet_array = getfsarray(fs_df)\n",
    "                    \n",
    "                    ##########################################################################\n",
    "                    # Ensure feature-set and casenesss values are matched for person_id.\n",
    "                    full_array = pandas.merge(casenessVector,\n",
    "                                              featureSet_array,\n",
    "                                              on = 'person_id',\n",
    "                                              how = 'left').fillna(0).astype(int)\n",
    "\n",
    "                    # Define an array indicating the feature set value.\n",
    "                    var_vals = \\\n",
    "                        full_array[\n",
    "                            ('_'+',_'.join(map(str, fs_df['src_snomedcode'].to_list()))).split(sep=',')\n",
    "                        ]\n",
    "\n",
    "                    # Name the feature set.\n",
    "                    name_var = '-'.join(var_vals.columns.values)\n",
    "\n",
    "                    # Set the representation.\n",
    "                    if representation_label == \"ALLrepresentation\":\n",
    "                        fs_val = var_vals.all(True)\n",
    "                    elif representation_label == \"MULTIrepresentation\":\n",
    "                        fs_val, next_iter = mutlinomRepresentation(var_vals)\n",
    "                        if next_iter:\n",
    "                            drop_tally += 1\n",
    "                            if verbose == True:\n",
    "                                print(\"Dropped\", name_var, \"because at least one\",\n",
    "                                      \"component features has more than three values\")\n",
    "                            continue\n",
    "\n",
    "                    # Calculate the mutual information between the feature set and\n",
    "                    # caseness variable.\n",
    "                    f_MI = sklearn.metrics.mutual_info_score(fs_val, full_array.iloc[:,-1])\n",
    "\n",
    "                    if f_MI < entropy_caseness:\n",
    "                        drop_tally += 1\n",
    "                        if verbose == True:\n",
    "                            print(\"Dropped\",name_var,\"because f_MI =\", f_MI)\n",
    "                        continue\n",
    "                    else:            \n",
    "                        # Store the name and mutual information value.\n",
    "                        if verbose == True:\n",
    "                            print(\"\\tSaved\",name_var,\"because f_MI =\", f_MI, \"is >\", entropy_caseness, \"(i.e. caseness entropy)\")\n",
    "                        featureSet_MI.loc[len(featureSet_MI),:] = name_var, f_MI\n",
    "\n",
    "                    if len(featureSet_MI) > 9:\n",
    "                            # Increment batch.\n",
    "                            batch += 1\n",
    "\n",
    "                            # Make an interim save of results.\n",
    "                            current_dir = os.getcwd()\n",
    "                            print(current_dir + savelocation)\n",
    "                            os.chdir(current_dir + \"/\" + savelocation)\n",
    "                            pyarrow.parquet.write_table(pyarrow.Table.from_pandas(featureSet_MI),\n",
    "                                                        source + \"_\" +\n",
    "                                                        caseness_label + \"_\" +\n",
    "                                                        representation_label + \"_\" + \n",
    "                                                        \"batch\" + \n",
    "                                                        str(batch) + \n",
    "                                                        \".parquet\")\n",
    "                            os.chdir(current_dir)\n",
    "                            featureSet_MI.to_csv(savelocation + \"/\" +\n",
    "                                                 source + \"_\" +\n",
    "                                                 caseness_label + \"_\" +\n",
    "                                                 representation_label + \"_\" +\n",
    "                                                 \"batch\" + \n",
    "                                                 str(batch) + \n",
    "                                                 \".csv\", index = False)\n",
    "                            print(\"\\nInterim save made\")\n",
    "                            # Instantiate new storage.\n",
    "                            featureSet_MI = \\\n",
    "                                pandas.DataFrame(columns = ['Feature_set', 'Mutual_information'])\n",
    "                    ##########################################################################\n",
    "        \n",
    "        \n",
    "                else: # We can infer that Triplets are desired so we create the Triplets iterable and run the Triplets code.\n",
    "                    for i_fs_3 in tqdm(fs_3_indices, desc = \"Third feature-set list\", unit = \"feature set\"):\n",
    "                        #################\n",
    "                        # TRIPLETS CODE #\n",
    "                        #################\n",
    "                        # Define the list of SNOMED-CT codes to be queried.\n",
    "                        fs_df = featureSet_vector_of_names.iloc[[i_fs_1, i_fs_2, i_fs_3]]\n",
    "                        # Retrieve a feastureSet_array\n",
    "                        featureSet_array = getfsarray(fs_df)\n",
    "\n",
    "                        ##########################################################################\n",
    "                        # Ensure feature-set and casenesss values are matched for person_id.\n",
    "                        full_array = pandas.merge(casenessVector,\n",
    "                                                  featureSet_array,\n",
    "                                                  on = 'person_id',\n",
    "                                                  how = 'left').fillna(0).astype(int)\n",
    "\n",
    "                        # Define an array indicating the feature set value.\n",
    "                        var_vals = \\\n",
    "                            full_array[\n",
    "                                ('_'+',_'.join(map(str, fs_df['src_snomedcode'].to_list()))).split(sep=',')\n",
    "                            ]\n",
    "\n",
    "                        # Name the feature set.\n",
    "                        name_var = '-'.join(var_vals.columns.values)\n",
    "\n",
    "                        # Set the representation.\n",
    "                        if representation_label == \"ALLrepresentation\":\n",
    "                            fs_val = var_vals.all(True)\n",
    "                        elif representation_label == \"MULTIrepresentation\":\n",
    "                            fs_val, next_iter = mutlinomRepresentation(var_vals)\n",
    "                            if next_iter:\n",
    "                                drop_tally += 1\n",
    "                                if verbose == True:\n",
    "                                    print(\"Dropped\", name_var, \"because at least one\",\n",
    "                                          \"component features has more than three values\")\n",
    "                                continue\n",
    "\n",
    "                        # Calculate the mutual information between the feature set and\n",
    "                        # caseness variable.\n",
    "                        f_MI = sklearn.metrics.mutual_info_score(fs_val, full_array.iloc[:,-1])\n",
    "\n",
    "                        if f_MI < entropy_caseness:\n",
    "                            drop_tally += 1\n",
    "                            if verbose == True:\n",
    "                                print(\"Dropped\",name_var,\"because f_MI =\", f_MI)\n",
    "                            continue\n",
    "                        else:            \n",
    "                            # Store the name and mutual information value.\n",
    "                            if verbose == True:\n",
    "                                print(\"\\tSaved\",name_var,\"because f_MI =\", f_MI, \"is >\", entropy_caseness, \"(i.e. caseness entropy)\")\n",
    "                            featureSet_MI.loc[len(featureSet_MI),:] = name_var, f_MI\n",
    "\n",
    "                        if len(featureSet_MI) > 9:\n",
    "                                # Increment batch.\n",
    "                                batch += 1\n",
    "\n",
    "                                # Make an interim save of results.\n",
    "                                current_dir = os.getcwd()\n",
    "                                print(current_dir + savelocation)\n",
    "                                os.chdir(current_dir + \"/\" + savelocation)\n",
    "                                pyarrow.parquet.write_table(pyarrow.Table.from_pandas(featureSet_MI),\n",
    "                                                            source + \"_\" +\n",
    "                                                            caseness_label + \"_\" +\n",
    "                                                            representation_label + \"_\" + \n",
    "                                                            \"batch\" + \n",
    "                                                            str(batch) + \n",
    "                                                            \".parquet\")\n",
    "                                os.chdir(current_dir)\n",
    "                                featureSet_MI.to_csv(savelocation + \"/\" +\n",
    "                                                     source + \"_\" +\n",
    "                                                     caseness_label + \"_\" +\n",
    "                                                     representation_label + \"_\" +\n",
    "                                                     \"batch\" + \n",
    "                                                     str(batch) + \n",
    "                                                     \".csv\", index = False)\n",
    "                                print(\"\\nInterim save made\")\n",
    "                                # Instantiate new storage.\n",
    "                                featureSet_MI = \\\n",
    "                                    pandas.DataFrame(columns = ['Feature_set', 'Mutual_information'])\n",
    "                        ##########################################################################\n",
    "\n",
    "\n",
    "        else: # We conclude that Individuals are desired and we run the Individuals code.            \n",
    "            ####################\n",
    "            # INDIVIDUALS CODE #\n",
    "            ####################          \n",
    "            # Define the list of SNOMED-CT codes to be queried.\n",
    "            fs_df = featureSet_vector_of_names.iloc[[i_fs_1]]\n",
    "            # Retrieve a feastureSet_array\n",
    "            featureSet_array = getfsarray(fs_df)\n",
    "           \n",
    "            \n",
    "            \n",
    "        #\n",
    "        # End of code block\n",
    "        ##########################################################################\n",
    "        \n",
    "        ##########################################################################\n",
    "        # Ensure feature-set and casenesss values are matched for person_id.\n",
    "        full_array = pandas.merge(casenessVector,\n",
    "                                  featureSet_array,\n",
    "                                  on = 'person_id',\n",
    "                                  how = 'left').fillna(0).astype(int)\n",
    "            \n",
    "        # Define an array indicating the feature set value.\n",
    "        var_vals = \\\n",
    "            full_array[\n",
    "                ('_'+',_'.join(map(str, fs_df['src_snomedcode'].to_list()))).split(sep=',')\n",
    "            ]\n",
    "\n",
    "        # Name the feature set.\n",
    "        name_var = '-'.join(var_vals.columns.values)\n",
    "        \n",
    "        # Set the representation.\n",
    "        if representation_label == \"ALLrepresentation\":\n",
    "            fs_val = var_vals.all(True)\n",
    "        elif representation_label == \"MULTIrepresentation\":\n",
    "            fs_val, next_iter = mutlinomRepresentation(var_vals)\n",
    "            if next_iter:\n",
    "                drop_tally += 1\n",
    "                if verbose == True:\n",
    "                    print(\"Dropped\", name_var, \"because at least one\",\n",
    "                          \"component features has more than three values\")\n",
    "                continue\n",
    "\n",
    "        # Calculate the mutual information between the feature set and\n",
    "        # caseness variable.\n",
    "        f_MI = sklearn.metrics.mutual_info_score(fs_val, full_array.iloc[:,-1])\n",
    "\n",
    "        if f_MI < entropy_caseness:\n",
    "            drop_tally += 1\n",
    "            if verbose == True:\n",
    "                print(\"Dropped\",name_var,\"because f_MI =\", f_MI)\n",
    "            continue\n",
    "        else:            \n",
    "            # Store the name and mutual information value.\n",
    "            if verbose == True:\n",
    "                print(\"\\tSaved\",name_var,\"because f_MI =\", f_MI, \"is >\", entropy_caseness, \"(i.e. caseness entropy)\")\n",
    "            featureSet_MI.loc[len(featureSet_MI),:] = name_var, f_MI\n",
    "\n",
    "        if len(featureSet_MI) > 9:\n",
    "                # Increment batch.\n",
    "                batch += 1\n",
    "\n",
    "                # Make an interim save of results.\n",
    "                current_dir = os.getcwd()\n",
    "                print(current_dir + savelocation)\n",
    "                os.chdir(current_dir + \"/\" + savelocation)\n",
    "                pyarrow.parquet.write_table(pyarrow.Table.from_pandas(featureSet_MI),\n",
    "                                            source + \"_\" +\n",
    "                                            caseness_label + \"_\" +\n",
    "                                            representation_label + \"_\" + \n",
    "                                            \"batch\" + \n",
    "                                            str(batch) + \n",
    "                                            \".parquet\")\n",
    "                os.chdir(current_dir)\n",
    "                featureSet_MI.to_csv(savelocation + \"/\" +\n",
    "                                     source + \"_\" +\n",
    "                                     caseness_label + \"_\" +\n",
    "                                     representation_label + \"_\" +\n",
    "                                     \"batch\" + \n",
    "                                     str(batch) + \n",
    "                                     \".csv\", index = False)\n",
    "                print(\"\\nInterim save made\")\n",
    "                # Instantiate new storage.\n",
    "                featureSet_MI = \\\n",
    "                    pandas.DataFrame(columns = ['Feature_set', 'Mutual_information'])\n",
    "                ##########################################################################\n",
    "    \n",
    "    # Increment counter.\n",
    "    batch += 1\n",
    "    # Final save.\n",
    "    if len(featureSet_MI) != 0:\n",
    "        current_dir = os.getcwd()\n",
    "        print(current_dir + savelocation)\n",
    "        os.chdir(current_dir + \"/\" + savelocation)\n",
    "        pyarrow.parquet.write_table(pyarrow.Table.from_pandas(featureSet_MI),\n",
    "                                    source + \"_\" +\n",
    "                                    caseness_label + \"_\" +\n",
    "                                    representation_label + \"_\" + \n",
    "                                    \"batch\" + \n",
    "                                    str(batch) + \n",
    "                                    \".parquet\")\n",
    "        os.chdir(current_dir)\n",
    "        featureSet_MI.to_csv(savelocation + \"/\" +\n",
    "                             source + \"_\" +\n",
    "                             caseness_label + \"_\" +\n",
    "                             representation_label + \"_\" +\n",
    "                             \"batch\" + \n",
    "                             str(batch) + \n",
    "                             \".csv\", index = False)\n",
    "\n",
    "    # Feedback messages.\n",
    "    print(\"...\\n\")\n",
    "    print(str(batch), \"batch(es) of feature sets processed.\")\n",
    "    print(str(drop_tally), \"/\",\n",
    "          str(len(combins)),\n",
    "          \"feature sets dropped due to low entropy.\")\n",
    "    print(\"****************************************\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6338baa9-6297-451f-a8b8-b377f6ca282c",
   "metadata": {},
   "source": [
    "### Produce a database feature set for submission to `featuresetmi()` function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4b430c11-3ed6-418d-982f-ad2c9faa2d70",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def getfsarray(fs_df):\n",
    "    \n",
    "    # Instantiate a Google BigQuery client.\n",
    "    client = bigquery.Client()\n",
    "    \n",
    "    # Define the BigQuery syntax.\n",
    "\n",
    "    sql_CTEs_body = \\\n",
    "    \"\"\"\n",
    "    # Make a table of person ID and their SNOMED-CT codes from the list of codes of interest.\n",
    "    WITH tbl_persons_and_codes AS\n",
    "    (\n",
    "    SELECT\n",
    "        DISTINCT person_id\n",
    "        ,src_snomedcode\n",
    "    FROM\n",
    "        yhcr-prd-phm-bia-core.CY_FDM_PrimaryCare_v5.tbl_SRCode\n",
    "    WHERE\n",
    "        src_snomedcode IN ('\"\"\" + '\\', \\''.join(map(str, fs_df['src_snomedcode'].to_list())) + \"\"\"')\n",
    "    )\n",
    "    \"\"\"\n",
    "\n",
    "    sql_pivot = \\\n",
    "    \"\"\"\n",
    "        SELECT\n",
    "        CONCAT(\"SELECT person_id,\"\n",
    "               , STRING_AGG(CONCAT(\"CASE WHEN src_snomedcode='\",src_snomedcode,\"' THEN 1 ELSE 0 END AS `_\",src_snomedcode,\"`\")), \n",
    "            \" FROM `tbl_persons_and_codes`\",\n",
    "            \" GROUP BY person_id, src_snomedcode ORDER BY person_id\")\n",
    "    FROM (  SELECT DISTINCT src_snomedcode FROM `tbl_persons_and_codes` ORDER BY src_snomedcode  )\n",
    "    \"\"\"\n",
    "    sql = client.query(sql_CTEs_body + sql_pivot).to_dataframe()['f0_'].iloc[0]\n",
    "    \n",
    "    feastureSet_array = \\\n",
    "        client.query(sql_CTEs_body +\n",
    "                     sql).to_dataframe()\n",
    "    \n",
    "    return feastureSet_array"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6085ccba-bc80-46aa-876d-713eb04bc686",
   "metadata": {
    "tags": []
   },
   "source": [
    "### NOT IN USE Do the work of calculating the MI between a given feature set and caseness variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a29e471c-97a9-4685-b584-d9058d7d8757",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def calculatemi(var_vals,\n",
    "                name_var,\n",
    "                representation_label,\n",
    "                drop_tally,\n",
    "                verbose):\n",
    "    \n",
    "    # Formulate the representation.\n",
    "    if representation_label == \"ALLrepresentation\":\n",
    "        fs_val = var_vals.all(True)\n",
    "    elif representation_label == \"MULTIrepresentation\":\n",
    "        fs_val, next_iter = mutlinomRepresentation(var_vals)\n",
    "        if next_iter:\n",
    "            return next_iter\n",
    "\n",
    "    # Calculate the mutual information between the feature set and\n",
    "    # caseness variable.\n",
    "    f_MI = sklearn.metrics.mutual_info_score(fs_val, full_array.iloc[:,-1])\n",
    "\n",
    "    if f_MI < entropy_caseness:\n",
    "        drop_tally += 1\n",
    "        if verbose == True:\n",
    "            print(\"Dropped\",name_var,\"because f_MI =\", f_MI)\n",
    "    else:            \n",
    "        # Store the name and mutual information value.\n",
    "        if verbose == True:\n",
    "            print(\"\\tSaved\",name_var,\"because f_MI =\", f_MI)\n",
    "        featureSet_MI.loc[len(featureSet_MI),:] = name_var, f_MI\n",
    "\n",
    "    if len(featureSet_MI) > 9:\n",
    "            # Increment batch.\n",
    "            batch += 1\n",
    "\n",
    "            # Make an interim save of results.\n",
    "            featureSet_MI.to_csv(savelocation +\n",
    "                              representation_label + \"_\" +\n",
    "                              \"_batch\" + \\\n",
    "                              str(batch) + \\\n",
    "                              \".csv\", index = False)\n",
    "            print(\"\\nInterim save made\")\n",
    "            # Instantiate new storage.\n",
    "            featureSet_MI = \\\n",
    "                pandas.DataFrame(columns = ['Feature set', 'Mutual information'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b757a54-41af-4ca5-89fa-8614e08513d9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-1.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-1:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
