{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e6d5d9a-a6b3-4471-82c7-317ece56bbfa",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Filtering feature sets.\n",
    "\n",
    "Candidate feature sets are filtered based on their entropy. Should they demonstrate sufficient entropy, we calculate their mutual information with the 'active' and 'possible' caseness variables, for further review.\n",
    "\n",
    "The purpose of this script is to calculate the entropy and mutual information.\n",
    "\n",
    "This Jupyter notebook calls a different filter notebook for each source of feature sets:\n",
    "- database feature sets\n",
    "- clinician feature sets\n",
    "- literature feature sets\n",
    "- PPI feature sets\n",
    "- interview feature sets\n",
    "\n",
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "b5e89744-b7fb-42f2-b718-503cdf46688c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47159b04-f348-4084-ad6a-f698384b3979",
   "metadata": {},
   "source": [
    "### Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc3c16ee-fa5b-46af-8f2f-f14f04b44c85",
   "metadata": {},
   "source": [
    "#### Load the caseness variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f085b218-7849-419d-848b-2d42afef00f4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run ./\"UNSEEN create caseness variables.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9d6190-7235-4e4d-919b-a3827a47c79c",
   "metadata": {},
   "source": [
    "#### Define a function to calculate the multinomial representation of a feature set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "f7a66a26-c8d6-4ad8-8be6-ae0139a12aa5",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define function that will calculate the multinomial\n",
    "# representation of a feature set.\n",
    "#\n",
    "# The function takes an n-by-m array of n patients and m features\n",
    "# and produces an n-by-1 array indicating the multinomial category\n",
    "# to which each patient record belongs.\n",
    "def mutlinomRepresentation(var_vals):\n",
    "    # Check that the variables have two or fewer values and\n",
    "    # only progress if True.\n",
    "    for i_col in range(var_vals.shape[1]-1):\n",
    "        unique_feature_vals = var_vals.iloc[:, i_col].drop_duplicates()\n",
    "        if (len(unique_feature_vals) > 2):\n",
    "            print(\"\\n** Error: At least one of the\",\n",
    "                  \"component features has more than\",\n",
    "                  \"two values so the multinomial\",\n",
    "                  \"representation will not be computed.**\\n\")\n",
    "            print(i_col, \"th variable:\", var_vals.columns.values[i_col])\n",
    "            unique_feature_vals\n",
    "            next_iter = True\n",
    "            return 0, next_iter\n",
    "\n",
    "    # Get all combinations of values of the component features\n",
    "    # and define feature set values for each multinomial combination.\n",
    "    feature_combins = var_vals.drop_duplicates()\n",
    "    feature_combins =\\\n",
    "        pandas.DataFrame(data = feature_combins, columns = var_vals.columns)\\\n",
    "        .reset_index()\\\n",
    "        .drop(['index'], axis = 1)\n",
    "    feature_combins['multinom_vals'] = feature_combins.index\n",
    "    \n",
    "    \n",
    "    # Define a vector indicating the feature set value.\n",
    "    myMerge =\\\n",
    "        pandas.merge(\n",
    "            var_vals,\n",
    "            feature_combins,\n",
    "            how = 'left',\n",
    "            on = list(var_vals.columns.values)\n",
    "    )\n",
    "    \n",
    "    # Extract multinomial representation as output variable.\n",
    "    featureSet = myMerge['multinom_vals']\n",
    "    next_iter = False\n",
    "    return featureSet, next_iter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeebb93-fa36-4838-a11c-f1bb06fe43be",
   "metadata": {},
   "source": [
    "#### Define a function to calculate the mutual information between feature sets and the caseness variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "59330a6f-e936-4e96-b070-18e7116a14d4",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define function that will calculate two-way mutual\n",
    "# information for the features of order m.\n",
    "def featuresetmi(featureSet_array,\n",
    "                 casenessVector,\n",
    "                 m = None,\n",
    "                 savelocation = None,\n",
    "                 representation = None):\n",
    "    # ## Assess argument validty.\n",
    "    \n",
    "    # Check order of feature set. If not provided,\n",
    "    # default to m = 1.    \n",
    "    if m == None:\n",
    "        order_int = 1\n",
    "        order_label = \"Individuals\"\n",
    "        print(\"\\nNo value for m provided.\" +\n",
    "              \"\\n...Default value of m = 1 will be used.\")\n",
    "    elif m == 1:\n",
    "        order_int = m\n",
    "        order_label = \"Individuals\"\n",
    "    elif m == 2:\n",
    "        order_int = m\n",
    "        order_label = \"Pairs\"\n",
    "    elif m == 3:\n",
    "        order_int = m\n",
    "        order_label = \"Triplets\"\n",
    "    else:\n",
    "        print(\"\\n** Error: Integer value between 1\",\n",
    "              \"and 3 not supplied for m.**\\n\")\n",
    "        return\n",
    "            \n",
    "    # Check and set save location.\n",
    "    if savelocation == None:\n",
    "        savelocation = \\\n",
    "           (\"Mutual information saves/\"+\\\n",
    "            order_label)\n",
    "        print(\"\\nNo save location provided.\" +\n",
    "              \"\\n...Defaulting to ~/\" + savelocation)    \n",
    "    \n",
    "    # ## Check encoding. If not provided, \n",
    "    # ## default to OR encoding.\n",
    "    if representation == None:\n",
    "        representation_label = \"ALLrepresentation\"\n",
    "        print(\"\\nNo representation provided.\" +\n",
    "              \"\\n...Defaulting to '\" + representation_label + \"' representation.\")\n",
    "    elif representation == \"all\":\n",
    "        representation_label = \"ALLrepresentation\"\n",
    "    elif representation == \"multi\":\n",
    "        representation_label = \"MULTIrepresentation\"\n",
    "    else:\n",
    "        print(\"\\n** Error: Representation value from \",\n",
    "              \"{'or', 'and', 'multi'} not provided.**\\n\")\n",
    "        return\n",
    "    \n",
    "    print(\"\\n\\n\\n****************************************\")  \n",
    "    print(\"Calculating mutual information values...\")\n",
    "    # Define the m-way tuples of features sets as a numpy array. We will loop\n",
    "    # through the rows of this array to create the feature sets.\n",
    "    combins = \\\n",
    "        numpy.asarray(\n",
    "            list(\n",
    "                itertools.combinations(\n",
    "                    featureSet_array.columns[featureSet_array.columns != 'person_id'],\n",
    "                    order_int)\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # Ensure feature-set and casenesss values are matched for person_id.\n",
    "    full_array = featureSet_array.merge(casenessVector, on = 'person_id')\n",
    "    \n",
    "    # Instantiate specific storage for mutual information.\n",
    "    featureSet_MI = \\\n",
    "        pandas.DataFrame(columns = ['Feature set', 'Mutual information'])\n",
    "    \n",
    "    # Instantiate batch number.\n",
    "    batch = 0\n",
    "    \n",
    "    # Instantiate tally of feature sets that are dropped due to low entropy.\n",
    "    drop_tally = 0\n",
    "    \n",
    "    # Define entropy of the particular caseness variable.\n",
    "    entropy_caseness = \\\n",
    "        scipy.stats.entropy(casenessVector.iloc[:,-1].value_counts(),\n",
    "                            base = math.e)\n",
    "    \n",
    "    # ## loop through the feature sets.\n",
    "    for i_fs in range(len(combins)):\n",
    "                \n",
    "        # Define a vector indicating the feature set value.\n",
    "        var_vals = full_array[combins[i_fs]]\n",
    "        if representation_label == \"ALLrepresentation\":\n",
    "            fs_val = var_vals.all(True)\n",
    "        elif representation_label == \"MULTIrepresentation\":\n",
    "            fs_val, next_iter = mutlinomRepresentation(var_vals)\n",
    "            if next_iter:\n",
    "                continue\n",
    "        \n",
    "        \n",
    "        # Calculate the mutual information between the feature set and\n",
    "        # caseness variable.\n",
    "        f_MI = sklearn.metrics.mutual_info_score(fs_val, full_array.iloc[:,-1])\n",
    "        \n",
    "        if f_MI < entropy_caseness:\n",
    "            drop_tally += 1\n",
    "            continue\n",
    "        else:\n",
    "            # Name the feature set.\n",
    "            # ...\n",
    "            # Store the name and mutual information value.\n",
    "            featureSet_MI.loc[len(featureSet_MI)] = name_var, f_MI\n",
    "\n",
    "        if len(featureSet_MI) > 9:\n",
    "                # Increment batch.\n",
    "                batch += 1\n",
    "\n",
    "                # Make an interim save of results.\n",
    "                featureSet_MI.to_csv(savelocation +\n",
    "                                  order_label + \"_\" +\n",
    "                                  representation_label + \"_\" +\n",
    "                                  \"_batch\" + \\\n",
    "                                  str(batch) + \\\n",
    "                                  \".csv\", index = False)\n",
    "                # Instantiate new storage.\n",
    "                featureSet_MI = \\\n",
    "                    pandas.DataFrame(columns = ['Feature set', 'Mutual information'])\n",
    "\n",
    "\n",
    "    # Increment counter.\n",
    "    batch += 1\n",
    "\n",
    "    # Final save.\n",
    "    if len(featureSet_MI) != 0:\n",
    "        featureSet_MI.to_csv(savelocation +\n",
    "                          order_label + \"_\" +\n",
    "                          representation_label + \"_\" +\n",
    "                          \"_batch\" + \\\n",
    "                          str(batch) + \\\n",
    "                          \".csv\", index = False)\n",
    "\n",
    "    # Feedback messages.\n",
    "    print(\"...\\n\")\n",
    "    print(str(batch), \"batch(es) of feature sets processed.\")\n",
    "    print(str(drop_tally), \"/\",\n",
    "          str(len(combins)),\n",
    "          \"feature sets dropped due to low entropy.\")\n",
    "    print(\"****************************************\")  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7319adb-f1af-4489-84b8-ddfee2b637ac",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Calculate the entropy and two-way mutual information of the feature sets and the caseness.\n",
    "\n",
    "Our focus is mutual information but I check the entropy of features so that I don't calculate the mutual information for any features whose entropy is less than that of the caseness variable. Justification for this action is based on the fact that the mutual information between the caseness variable and any feature will be less than or equal to the lesser entropy of the caseness or feature, i.e. $I(X_{i};CMHD) ≤ min\\{H(X_{i}), H(CMHD)\\}$. We don’t want any feature set that is worse than no feature set (i.e. having only the caseness prevalence to predict a random outcome value) so we don’t bother with any feature set that will lower the possible mutual information. Two-way mutual information* will not be calculated for those features whose entropy is less than the caseness variable's entropy. The dropped variables are indicated in the `f_to_drop` pandas.DataFrame.\n",
    "\n",
    "First, two-way mutual information will be calculated between the caseness variable and individual feature sets. Secondly, two-way mutual information will be calculated between the caseness variable and pair-composites of feature sets. These pair composites are individual feature sets that amalgamate two feature sets into a new binary definition, where values are `0` if both component feature sets are zero and `1` otherwise**. More-complicated encoding is possible, e.g. a different level for every combination of values from each component feature set. Further code extends these feature-set compositions up to quintuplet composites (i.e. amalgamating five feature sets into a single binary variable).\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "__\\*__ _Initially, the plan was to use $k$-way mutual information for $k>2$ but the meaning of these mutual information values is controversial at best. I side with [Krippendorf's assessment](https://sci-hub.wf/10.1080/03081070902993160), which renders 3-way mutual information interpretable but not any higher-order mutual information statistics (yet?). I decided to stick with two-way mutual information using composite feature sets so that I am comparing the same statistic across individual and composite feature sets._\n",
    "\n",
    "__\\**__ _Other encodings will be trialled at a later date. If a feature set contains more than one feature, then it will be represented in two ways: all-present and multinomial. The all-present (alternative called the all-present representation) representation is a binary variable with a value of `1` if all component features are one, and `0` otherwise. The multinomial representation is a multinomial variable with values for each of the possible combinations of component features’ values. For example, given a feature set of two features $A=\\{0,1\\}$ and $B=\\{0,1\\}$, their multinomial feature-set representation would be $C=\\{0,1,2,3\\}$, where $C=0=(A=0\\   AND\\   B=0)$, $C=1=(A=0\\   AND\\   B=1)$, $C=2=(A=1\\  AND\\  B=0)$, and $C=3=(A=1\\   AND\\   B=1)$. The multinomial representation will only be applied to binary variables._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67e6752e-c972-4937-85d6-a36bd3def4da",
   "metadata": {},
   "source": [
    "## Filter database feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8fb5a25e-7c32-4e98-8d81-467b36b53042",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run ./\"UNSEEN_filter_database_feature_sets.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54eb6f54-36dc-4cd2-b734-de53f3f94dd0",
   "metadata": {},
   "source": [
    "## Filter clinician feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a195f838-8fd7-4d86-bed5-260a7b91b7d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run ./\"UNSEEN_filter_clinician_feature_sets.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c746f299-5453-40c1-b9de-cf5fc70c8469",
   "metadata": {},
   "source": [
    "## Filter literature feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "00297b63-5762-4c24-8389-9e246442deb2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run ./\"UNSEEN_filter_literature_feature_sets.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e51366-f71c-482e-83d9-a55fd3cce7d6",
   "metadata": {},
   "source": [
    "## Filter PPI feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5db33eb0-02f3-43c1-b9e1-7be4c4078fde",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run ./\"UNSEEN_filter_PPI_feature_sets.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "882ad32b-3b7f-4dd8-ae23-c92a32d30b92",
   "metadata": {},
   "source": [
    "## Filter interview feature sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97eb877c-5ca6-412a-af27-7dd6159ed6e6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "%run ./\"UNSEEN_filter_interview_feature_sets.ipynb\""
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.4-1.m95",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/r-cpu.4-1:m95"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
