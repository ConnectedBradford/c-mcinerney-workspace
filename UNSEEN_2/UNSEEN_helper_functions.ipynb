{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "57fd3604-adb9-4e79-881c-9e18988d2e64",
   "metadata": {},
   "source": [
    "# Helper functions\n",
    "\n",
    "This notebook collates all the functions that help the other notebooks do their thing, without clogging the other notebooks with function definitions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ad1828-3272-4b6b-b8c4-7557b24bfe80",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ece23826-2a4d-451b-8002-501a226f745c",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "import csv\n",
    "\n",
    "import datetime\n",
    "\n",
    "#!pip install EntropyHub\n",
    "import EntropyHub\n",
    "\n",
    "from IPython.display import display, Markdown, Latex\n",
    "from IPython import get_ipython\n",
    "import itertools\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot\n",
    "from multiprocessing.pool import Pool\n",
    "from multiprocessing import SimpleQueue, Process\n",
    "\n",
    "import nest_asyncio \n",
    "nest_asyncio.apply()\n",
    "import numpy\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas\n",
    "#!pip install pandas_gbq\n",
    "import pandas_gbq\n",
    "pandas.set_option('display.max_colwidth', None)\n",
    "import pathlib\n",
    "from PIL import Image\n",
    "import plotly\n",
    "#!pip install pyinform\n",
    "import pyinform\n",
    "    \n",
    "import re\n",
    "import rpy2.ipython\n",
    "\n",
    "import scipy.stats\n",
    "import scipy.special\n",
    "import sklearn.metrics\n",
    "import sklearn.neighbors\n",
    "import sklearn.utils\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "import statistics\n",
    "import statsmodels.formula.api\n",
    "\n",
    "import time\n",
    "import tqdm\n",
    "\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b06dd66-0f5e-414f-ac99-7a918e2009d3",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e28a3885-d79d-4a12-be5a-63ef2e2a9981",
   "metadata": {},
   "source": [
    "### `entropy_output()`: Compute and present the entropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e423811f-5800-42c5-9f0e-a3ddd889fbc3",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to compute and present the entropy of a column in a pandas.Dataframe.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. my_df_column:            A column from a pandas.Dataframe containing the variable\n",
    "#                             for which entropy needs calculating.\n",
    "#\n",
    "# RETURNS\n",
    "# 1. entropy_caseness:        Entropy in nats.\n",
    "# 2. entropy_caseness_scaled: Entropy scaled to the theoretical maximum for a binary variable.\n",
    "#\n",
    "\n",
    "def entropy_output(my_df_column):\n",
    "    my_df_column.dropna(inplace = True)\n",
    "    entropy_caseness = scipy.stats.entropy(my_df_column.value_counts().astype(int), base = math.e)\n",
    "    \n",
    "    entropy_caseness_scaled = round(entropy_caseness / math.log(len(my_df_column.unique()), math.e) * 100, 1)\n",
    "    if entropy_caseness < 0.001:\n",
    "        print('\\t Caseness variable entropy < 0.001 nats')\n",
    "    else:\n",
    "        print(f'\\t Caseness variable entropy = {round(entropy_caseness, 3)} nats')\n",
    "    if entropy_caseness < 0.001:\n",
    "        print(f'\\t The caseness variable\\'s entropy is < 0.001 % its theoretical maximum\\n')\n",
    "    else:\n",
    "        print(f'\\t The caseness variable\\'s entropy is {entropy_caseness_scaled} % of its theoretical maximum\\n')\n",
    "    \n",
    "    return entropy_caseness, entropy_caseness_scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2798124-2ccd-4129-9c23-1b8afcb61ea0",
   "metadata": {},
   "source": [
    "### `hitrate_output()`: Compute and present the hit rate - a.k.a. (mis)classification error - of a caseness variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3db7dc06-6195-4223-80e2-2ad15604f81e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# A function to compute and present the hitrate - a.k.a. (mis)classification\n",
    "# error - of a caseness variable.\n",
    "#\n",
    "# ARGUMENTS\n",
    "# 1. my_caseness_variable:  A column from a pandas.Dataframe containing patients'\n",
    "#                           caseness values.\n",
    "#\n",
    "# RETURNS\n",
    "# 1. hitRate_none:          The hitrate assuming no one demonstrates the caseness.\n",
    "# 2. hitRate_all:           The hitrate assuming everyone demonstrates the caseness.\n",
    "#\n",
    "\n",
    "def hitrate_output(my_caseness_variable):\n",
    "    \n",
    "    # Calculations.\n",
    "    my_caseness_variable.dropna(inplace = True)\n",
    "    numerator = my_caseness_variable.astype(bool).sum()\n",
    "    denominator = len(my_caseness_variable)\n",
    "    hitRate_all = (numerator / denominator) * 100\n",
    "    hitRate_none = 100 - hitRate_all\n",
    "    Odds_noYes = hitRate_none / hitRate_all\n",
    "    \n",
    "    # Message to user.\n",
    "    if hitRate_all < 0.001:\n",
    "        print(f'\\t Hit rate (all) < 0.001 %')\n",
    "        print(f'\\t Hit rate (none) \\u2248 100 %')\n",
    "        print(f'\\t Odds (No : Yes) \\u2248 infitely-times less likely to demonstrate caseness than to not.')\n",
    "    else:\n",
    "        print(f'\\t Hit rate (all) = {round(hitRate_all, 3)} %')\n",
    "        print(f'\\t Hit rate (none) = {round(hitRate_none, 3)} %')\n",
    "        print(f'\\t Odds (No : Yes) = {int(Odds_noYes):,}-times less likely to demonstrate caseness than to not.')\n",
    "    \n",
    "    return hitRate_none, hitRate_all"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c65f056-f60b-4246-ae69-4f706b110e93",
   "metadata": {},
   "source": [
    "### `evaloutputs()`: Compute the evaluation outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fae1469-7871-49e5-ae9d-1a317db4ae7f",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaloutputs(vec_featureSet,\n",
    "                vec_caseness):\n",
    "    '''\n",
    "     A function to compute the evaluation statistics for submitted features that have more than\n",
    "     one value.\n",
    "\n",
    "     ARGUMENTS\n",
    "     1. vec_featureSet:      A column from a pandas.Dataframe containing the feature set\n",
    "                             that needs evaluating.\n",
    "     2. vec_caseness:        A column from a pandas.Dataframe containing the caseness\n",
    "                             variable of interest.\n",
    "\n",
    "     RETURNS\n",
    "     1. evalStats:           A list containing the evaluation statistics:\n",
    "                             feature_set: A string of the feature set's name, as per the column\n",
    "                                          heading of `vec_featureSet`.\n",
    "                             fs_data_type: A datatype object indicating the data type of the\n",
    "                                           feature set, as per `vec_featureSet`.\n",
    "                             sMI: The mutual information between `vec_featureSet` and\n",
    "                                  `vec_caseness`, scaled to the entropy of `vec_caseness`. \n",
    "                                   This particular scaled mutual information is the\n",
    "                                   proportional improvement in certainty about the caseness\n",
    "                                   variable. For example, a sMI = 0.05 means that the feature\n",
    "                                   set improves our certainty about whether the person \n",
    "                                   demonstrates the caseness of CMHD by 5%.\n",
    "                             prevalence: The proportion of patients satisfying the definition\n",
    "                                         of the feature set, per thousand. This is only\n",
    "                                         calculated if the feature set is binary.\n",
    "                             mean: The arithmetic mean of the feature set values. This is only\n",
    "                                   calculated if the feature set is continuous. It is intended\n",
    "                                   as an alternative for the prevalence statistic, for \n",
    "                                   continuous feature sets. I reason that the prevalence gives\n",
    "                                   an expectation of an occurrence variable, and the mean gives\n",
    "                                   an expectation of a continuous variable.\n",
    "                             mode: The mode of the feature set values. This is only calculated\n",
    "                                   if the feature set is count data. It is intended as an\n",
    "                                   alternative for the prevalence statistic, for count feature\n",
    "                                   sets. I reason that the prevalence gives an expectation of\n",
    "                                   an occurrence variable, and the mean gives an expectation of\n",
    "                                   a continuous variable.\n",
    "                             cba: Class balanced accuracy - the lower bound of the average\n",
    "                                  sensitivity and average positive predictive value (a.k.a.\n",
    "                                  precision) for all caseness values. This is only calculated\n",
    "                                  if the feature set is binary.\n",
    "                             oddsRatio: The ratio of the odds of caseness given the presence\n",
    "                                        of feature set, to the odds of CMHD given the absence\n",
    "                                        of the feature set. It can also be thought of as the\n",
    "                                        multiplicative difference between correct and incorrect\n",
    "                                        classification.\n",
    "                             ppv: The proportion of patients satisfying the definition of the\n",
    "                                  feature set who satisfy the caseness. This is only calculated\n",
    "                                  if the feature set is binary.\n",
    "                             npv: The proportion of patients who do not satisfy the definition\n",
    "                                  of the feature set who do not satisfy the caseness. This is\n",
    "                                  only calculated if the feature set is binary.\n",
    "                             tn: The count of true negatives, i.e. the count of patients whose \n",
    "                                 feature-set value and caseness value are both zero. This is\n",
    "                                 only calculated if the feature set is binary.\n",
    "                             fn: The count of false negatives, i.e. the count of patients whose \n",
    "                                 feature-set value is zero but whose caseness value is one. This\n",
    "                                 is only calculated if the feature set is binary.\n",
    "                             fp: The count of false positives, i.e. the count of patients whose \n",
    "                                 feature-set value is one but whose caseness value is zero. This\n",
    "                                 is only calculated if the feature set is binary.\n",
    "                             tp: The count of true positives, i.e. the count of patients whose \n",
    "                                 feature-set value and caseness value are both one. This is only\n",
    "                                 calculated if the feature set is binary.\n",
    "    '''\n",
    "    ## ## Assess argument validty.\n",
    "    #if len(vec_featureSet.value_counts()) < 2:\n",
    "    #    print(f\"**Feature-set {vec_featureSet.name} only has one value.**\")\n",
    "    #    return None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
    "    \n",
    "    # Check that both vectors are the same length.\n",
    "    if len(vec_featureSet) != len(vec_caseness):\n",
    "        print(\"Feature-set and caseness vectors are of different lengths.\")\n",
    "        return None, None, None, None, None, None, None, None, None, None, None, None, None, None\n",
    "    \n",
    "    # Change the data type to suit the `statsmodel` function.\n",
    "    if vec_featureSet.dtype == 'int64':\n",
    "        vec_featureSet = vec_featureSet.astype(int)\n",
    "    elif vec_featureSet.dtype == 'boolean':\n",
    "        vec_featureSet = vec_featureSet.astype(bool)\n",
    "    \n",
    "    # Calculate the entropy of the caseness variable.\n",
    "    pk = vec_caseness.value_counts() / len(vec_caseness)\n",
    "    entropy_caseness = -numpy.sum(pk * numpy.log(pk))\n",
    "    \n",
    "    # Check what dtype the feature set is because float64-dtype feature sets need\n",
    "    # to be processed differently to the categorical ones.\n",
    "    if vec_featureSet.dtype == 'float64' or len(vec_featureSet.value_counts()) > 3:\n",
    "        tn = tp = fn = fp = None\n",
    "        # ## Compute outputs.\n",
    "        #\n",
    "        # Mutual information\n",
    "        # ## The `mutual_info_regression` function doesn't handle NaNs so I will remove those patients from the function\n",
    "        # ## arguments. I need to remove the same rows from both vectors.\n",
    "        #\n",
    "        # ## Remove rows that have NaNs in vec_featureSet, from both vec_caseness and vec_featureSet.\n",
    "        vec_caseness = vec_caseness[~vec_featureSet.isna().reindex(vec_caseness.index, fill_value=False)]\n",
    "        vec_featureSet = vec_featureSet[~vec_featureSet.isna()]\n",
    "        # ## Remove rows that have inf in vec_featureSet, from both vec_caseness and vec_featureSet.\n",
    "        vec_caseness = vec_caseness[~numpy.isinf(vec_featureSet).reindex(vec_caseness.index, fill_value=False)]\n",
    "        vec_featureSet = vec_featureSet[~numpy.isinf(vec_featureSet)]\n",
    "        # Calculate the mutual information.\n",
    "        MI_runs = []\n",
    "        for i_runs in range(20):\n",
    "            MI_runs.append(mutual_info_regression(vec_featureSet.to_numpy().reshape(-1,1), vec_caseness, n_neighbors = 2)[0])\n",
    "        MI = numpy.mean(MI_runs)\n",
    "        sMI = MI / entropy_caseness \n",
    "\n",
    "        # Prevalence value per 1,000.\n",
    "        prevalence = None\n",
    "        if vec_featureSet.dtype == 'float64':\n",
    "            mean = round(numpy.mean(vec_featureSet), 2)\n",
    "            mode = None\n",
    "        else:\n",
    "            mean = None\n",
    "            mode = scipy.stats.mode(vec_featureSet)[0][0]\n",
    "        \n",
    "        # Class balance accuracy.\n",
    "        cba = None\n",
    "        \n",
    "        # Odds ratio.\n",
    "        # ## Create the required dataframe.\n",
    "        df = pandas.DataFrame({'feature_set' : vec_featureSet.astype('float64'), 'caseness' : vec_caseness.astype(int)})\n",
    "        # ## Build regression model.\n",
    "        log_reg = statsmodels.formula.api.logit(\"caseness ~ feature_set\", data = df).fit(disp=0)\n",
    "        # ## Extract odds ratio.\n",
    "        oddsRatio = round(numpy.exp(log_reg.params)[1], 2)\n",
    "        \n",
    "        # Positive predictive value.\n",
    "        ppv = None\n",
    "        \n",
    "        # Negative predictive value.\n",
    "        npv = None\n",
    "        \n",
    "    else:\n",
    "        # ## Contingency table.\n",
    "        # Make contingency table.\n",
    "        contingencyTable = \\\n",
    "            pandas.crosstab(\n",
    "                index = vec_featureSet,\n",
    "                columns = vec_caseness\n",
    "        )\n",
    "\n",
    "        # Extract components of contingency table\n",
    "        tn = contingencyTable.iloc[0,0]\n",
    "        fn = contingencyTable.iloc[0,1]\n",
    "        fp = contingencyTable.iloc[1,0]\n",
    "        tp = contingencyTable.iloc[1,1]\n",
    "    \n",
    "        # ## Compute outputs.\n",
    "        #\n",
    "        # Scaled mutual information.\n",
    "        MI = sklearn.metrics.mutual_info_score(vec_featureSet, vec_caseness)\n",
    "        sMI = MI / entropy_caseness\n",
    "\n",
    "        # Prevalence value per 1,000.\n",
    "        #\n",
    "        # I use 1 minus the prevalence of zeros because that\n",
    "        # combines all the possibly-many values that indicate\n",
    "        # the presence of the feature set.\n",
    "        prevalence = \\\n",
    "            (1 - (sum(vec_featureSet == 0) / len(vec_featureSet))) * 1000\n",
    "        if prevalence < 1:\n",
    "             prevalence = '< 1'\n",
    "        else:\n",
    "             prevalence = round(prevalence, 2)\n",
    "        mean = None\n",
    "        mode = None\n",
    "        \n",
    "        # Class balance accuracy.\n",
    "        cba = \\\n",
    "            round( 0.5 * \\\n",
    "                  ( (tp / max( (tp + fn), (tp + fp) ) ) + \\\n",
    "                   (tn / max( (tn + fp), (tn +fn) ) ) ), 2)\n",
    "        if cba < 0.01:\n",
    "            cba = '< 0.01'\n",
    "\n",
    "        # Odds ratio.\n",
    "        if min( (tp * tn) , (fp * fn) ) == 0:\n",
    "            oddsRatio = 'Undefined'\n",
    "        else:\n",
    "            oddsRatio = round( (tp * tn) / (fp * fn), 2)\n",
    "\n",
    "        # Positive predictive value.\n",
    "        ppv = 0.00 if (tp + fp) == 0 else tp / (tp + fp)\n",
    "        if ppv > 0 and ppv < 0.01:\n",
    "            ppv = '< 0.01'\n",
    "        elif ppv < 1 and ppv > 0.999:\n",
    "            ppv = '\\u2248 1.00'\n",
    "        else:\n",
    "             ppv = round(ppv, 2)\n",
    "\n",
    "        # Negative predictive value.\n",
    "        npv = 0.00 if (tn + fn) == 0 else tn / (tn + fn)\n",
    "        if npv > 0 and npv < 0.01:\n",
    "            npv = '< 0.01'\n",
    "        elif npv < 1 and npv > 0.999:\n",
    "            npv = '\\u2248 1.00'\n",
    "        else:\n",
    "             npv = round(npv, 2)\n",
    "    \n",
    "    \n",
    "    return vec_featureSet.name, vec_featureSet.dtype, round(sMI, 6), prevalence, mean, mode, cba, oddsRatio, ppv, npv, tn, fn, fp, tp"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5696f968-e444-480e-851e-c470f213cd0d",
   "metadata": {},
   "source": [
    "### `chaoticlifeentropyfs()`: Calculate the entropy-based statistics for a patient's timeline of events."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbe3873-1692-4d0d-9932-07b9be77b82e",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def chaoticlifeentropyfs(pt_timeline):\n",
    "    '''\n",
    "    There are two categories of entropy-based feature sets for both appointments and did-not-attends:\n",
    "    Sequential\n",
    "    1.\tactiveInformation\n",
    "    2.\tentropyRate\n",
    "    Summative\n",
    "    3.\tspectralEntropy\n",
    "    4.\tsampleEntropy\n",
    "    5.\teoe (entropy of entropy)\n",
    "    6.\taverageEntropy\n",
    "    7.\tbubbleEntropy\n",
    "    Use the following parameters for all summative entropy statistics other than spectral entropy, which doesn't require them:\n",
    "    -\tobs = three-monthly count, enough to amass a period of use.\n",
    "    -\twindow breath (\"embedding dimension\") = 4, to indicate a year's worth of appointments.\n",
    "    -\twindow shift (\"embedding time delay\") = 1, to be sensitive to quarterly changes in behaviour.\n",
    "    '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Set parameters.\n",
    "    # ## Set warnings parameter to handle divide-by-zero issues with spectral entropy.\n",
    "    warnings.filterwarnings(\"error\")\n",
    "    # ## Window breath (\"embedding dimension\") = 4, to indicate a year's worth of appointments.\n",
    "    embeddingDimension = 4\n",
    "    # ## Window shift (\"embedding time delay\") = 1, to be sensitive to quarterly changes in behaviour.\n",
    "    embeddingTimeDelay = 1\n",
    "    # ## Length of the patient's timeline.\n",
    "    len_timeline = len(pt_timeline)\n",
    "    # Convert pt_timeline into a numpy.array.\n",
    "    pt_timeline = numpy.array(pt_timeline)\n",
    "    \n",
    "    # activeInformation\n",
    "    # ...\n",
    "    if len_timeline <= embeddingDimension:\n",
    "        activeInformation = None\n",
    "    else:\n",
    "        try:\n",
    "            activeInformation = \\\n",
    "                pyinform.activeinfo.active_info(pt_timeline, k = embeddingDimension)\n",
    "        except:\n",
    "            activeInformation = None\n",
    "    \n",
    "    # entropyRate\n",
    "    # ...\n",
    "    if len_timeline <= embeddingDimension:\n",
    "        entropyRate = None\n",
    "    else:\n",
    "        try:\n",
    "            entropyRate = \\\n",
    "                pyinform.entropyrate.entropy_rate(pt_timeline, k = embeddingDimension)\n",
    "        except:\n",
    "            entropyRate = None\n",
    "    \n",
    "    # spectralEntropy\n",
    "    # ...\n",
    "    if len_timeline <= 10:\n",
    "        spectralEntropy = None\n",
    "    else:\n",
    "        try:\n",
    "            spectralEntropy, _ = EntropyHub.SpecEn(pt_timeline)\n",
    "        except RuntimeWarning:\n",
    "            spectralEntropy = None\n",
    "    \n",
    "    # sampleEntropy\n",
    "    # ...\n",
    "    if len_timeline <= 10:\n",
    "        sampleEntropy = None\n",
    "    else:\n",
    "        try:\n",
    "            sampleEntropy, _, _ = \\\n",
    "                EntropyHub.SampEn(pt_timeline, m = embeddingDimension, tau = embeddingTimeDelay)\n",
    "            sampleEntropy = sampleEntropy[-1]\n",
    "        except:\n",
    "            sampleEntropy = None\n",
    "\n",
    "    # eoe and averageEntropy\n",
    "    # ...\n",
    "    if len_timeline <= 10:\n",
    "        eoe = None\n",
    "        averageEntropy = None\n",
    "    else:\n",
    "        try:\n",
    "            eoe, averageEntropy, _ = \\\n",
    "                EntropyHub.EnofEn(pt_timeline, tau = embeddingDimension, S = math.floor(len_timeline / 4) )\n",
    "        except:\n",
    "            eoe = None\n",
    "            averageEntropy = None\n",
    "\n",
    "    # bubbleEntropy\n",
    "    # ...\n",
    "    if len_timeline <= 10:\n",
    "        bubbleEntropy = None\n",
    "    else:\n",
    "        try:\n",
    "            bubbleEntropy, _ = EntropyHub.BubbEn(pt_timeline, m = embeddingDimension, tau = embeddingTimeDelay)\n",
    "            bubbleEntropy = bubbleEntropy[-1]\n",
    "        except:\n",
    "            bubbleEntropy = None\n",
    "    \n",
    "    # Package the output.\n",
    "    ls_entropyBasedFeatureSets = \\\n",
    "        [\n",
    "        activeInformation\n",
    "        ,entropyRate\n",
    "        ,spectralEntropy\n",
    "        ,sampleEntropy\n",
    "        ,eoe\n",
    "        ,averageEntropy\n",
    "        ,bubbleEntropy\n",
    "        ]\n",
    "    \n",
    "    return ls_entropyBasedFeatureSets"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": ".m115",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/:m115"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
