{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "17fa9c5d-6626-4c1e-b228-3ff73515057a",
   "metadata": {},
   "source": [
    "# Add feature sets of entropy statistics.\n",
    "\n",
    "The purpose of this notebook is to append feature sets that are defined by the value of entropy statistics.\n",
    "\n",
    "This notebook is expected to be called by its parent `UNSEEN_create_feature_sets_base.ipynb`. It will not run without the requisite loaded during the parent notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3918947a-258d-4d7c-be1c-61d6074d7301",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Refresh store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d5f3fbae-a50f-4e68-8c4f-61185b2e16c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get helper functions.\n",
    "%run 'UNSEEN_helper_functions.ipynb'\n",
    "# Refresh stored variables, if they are present.\n",
    "%store -r"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5db18df-415b-4edf-9be5-c248c215ca62",
   "metadata": {},
   "source": [
    "## Load requisites"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "512dd55e-626e-426a-86e2-ca49e3effdac",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set folder location.\n",
    "folder_loc = os.path.dirname(os.path.abspath(\"UNSEEN_create_clinician_feature_sets.ipynb\"))\n",
    "folder = folder_loc + '/codelists/'\n",
    "\n",
    "# Clinical codes of interest.\n",
    "codes_to_query_DNA = set( pandas.read_csv(folder + \"ciaranmci-did-not-attend-098119da.csv\")[\"code\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53148fa-f9aa-4fb5-a222-8fae0ccccefe",
   "metadata": {},
   "source": [
    "### Define functions."
   ]
  },
  {
   "cell_type": "raw",
   "id": "db859ee9-e699-425c-b9c2-03fa7c36f0e7",
   "metadata": {
    "tags": []
   },
   "source": [
    "# initialize worker processes\n",
    "def init_worker(shared_queue):\n",
    "    # declare scope of a new global variable\n",
    "    global queue\n",
    "    # store argument in the global variable for this process\n",
    "    queue = shared_queue\n",
    "    \n",
    "    \n",
    "def calculateentropybasedfs(pid):\n",
    "    # Extract this particular patient's range of active years.\n",
    "    pt_years = \\\n",
    "        bq_countAppointmentsPerQuarter.loc[bq_countAppointmentsPerQuarter.person_id == pid, 'year_appointment'].append(\n",
    "         bq_countDNAsPerQuarter.loc[bq_countDNAsPerQuarter.person_id == pid, 'year_DNA'])\n",
    "\n",
    "    pt_years_lsrange =  pandas.DataFrame(\n",
    "        data = { 'year' : list( range( min(pt_years), max(pt_years) ) ) }\n",
    "        )\n",
    "    # Create a timeline of years and quarters for this particular patient.\n",
    "    pt_quarters = pandas.DataFrame( data = {'qtr': [1,2,3,4]} )\n",
    "    pt_timeline = pt_years_lsrange.merge(pt_quarters, how = 'cross')\n",
    "\n",
    "    # Join the patient's actual count of appointments-per-quarter-per-year to their timeline.\n",
    "    pt_appts = bq_countAppointmentsPerQuarter.loc[bq_countAppointmentsPerQuarter.person_id == pid, :]\n",
    "    pt_timeline_appts = \\\n",
    "        pandas.merge(pt_timeline, pt_appts, how = 'left',\n",
    "                     left_on = ['year', 'qtr'],\n",
    "                     right_on = ['year_appointment',\n",
    "                                 'quarter_appointment']).loc[:,'countAppointmentsPerQuarter'].fillna(0).astype(int)\n",
    "\n",
    "    # Repeat for did-not-attend events.\n",
    "    pt_DNAs = bq_countDNAsPerQuarter.loc[bq_countDNAsPerQuarter.person_id == pid, :]\n",
    "    pt_timeline_DNAs = \\\n",
    "        pandas.merge(pt_timeline, pt_DNAs, how = 'left',\n",
    "                     left_on = ['year', 'qtr'],\n",
    "                     right_on = ['year_DNA',\n",
    "                                 'quarter_DNA']).loc[:,'countDNAsPerQuarter'].fillna(0).astype(int)\n",
    "\n",
    "    # Create the entropy-based feature sets.\n",
    "    pt_entropyStats_appts = chaoticlifeentropyfs(pt_timeline_appts)\n",
    "    pt_entropyStats_DNAs = chaoticlifeentropyfs(pt_timeline_DNAs) \n",
    "        \n",
    "    # Mark pid as processed.\n",
    "    pid_processed.append(pid)\n",
    "    \n",
    "    # declare scope of shared queue\n",
    "    global queue\n",
    "    # send result using shared queue\n",
    "    queue.put([pid] + pt_entropyStats_appts + pt_entropyStats_DNAs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e363f8f0-59e5-4188-b9c9-c73d03a56695",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Entropy-based feature sets potentially indicative of \"chaotic life\"\n",
    "\n",
    "The data from BigQuery results needs to be appointments and did-not-attend (DNA) tallied in three-month blocks, per person. Specifically, I use BigQuery's built-in `QUARTER()` function for which Q1 = Jan-Mar, Q2 = Apr-Jun, etc.  The query will only return data for quarters in which there was an appointment or a DNA. Each patient's data will be processed in Python to fill in the missing quarters' counts with 0 before calculating the values of the entropy-based feature sets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5559a9e4-8c6b-48ae-8e93-f145156f6858",
   "metadata": {},
   "source": [
    "##### SQL synax for appointments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82287c33-73ae-45be-8454-43fe1c953184",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sql_CTEs_body = \\\n",
    "\"\"\"\n",
    "#  ## Count of appointments, per quarter.\n",
    ",tbl_countAppointmentsPerQuarter AS (\n",
    "    SELECT\n",
    "        DISTINCT person_id\n",
    "        ,EXTRACT(YEAR FROM datestart) AS year_appointment\n",
    "        ,EXTRACT(QUARTER FROM datestart) AS quarter_appointment\n",
    "        ,COUNT(DISTINCT datestart) AS countAppointmentsPerQuarter\n",
    "    FROM\n",
    "        \"\"\" + server_id + \"\"\".\"\"\" + database_id + \"\"\".tbl_srappointment\n",
    "    GROUP BY\n",
    "         person_id\n",
    "        ,year_appointment\n",
    "        ,quarter_appointment\n",
    "\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sql_final_select = \\\n",
    "\"\"\"\n",
    "SELECT\n",
    "    tbl_studyPopulation_no_caseness.person_id\n",
    "    ,year_appointment\n",
    "    ,quarter_appointment\n",
    "    ,countAppointmentsPerQuarter\n",
    "FROM\n",
    "    tbl_studyPopulation_no_caseness\n",
    "LEFT JOIN\n",
    "    tbl_countAppointmentsPerQuarter\n",
    "    ON tbl_studyPopulation_no_caseness.person_id = tbl_countAppointmentsPerQuarter.person_id\n",
    "ORDER BY\n",
    "    person_id\n",
    "    ,year_appointment\n",
    "    ,quarter_appointment\n",
    "\"\"\"\n",
    "\n",
    "global bq_countAppointmentsPerQuarter\n",
    "bq_countAppointmentsPerQuarter = pandas.read_gbq(sql_declarations + sql_studyPopulation + sql_CTEs_body + sql_final_select).fillna(0).astype(int)\n",
    "# Double-check that all exclusions have been applied.\n",
    "bq_countAppointmentsPerQuarter.drop(bq_countAppointmentsPerQuarter[~bq_countAppointmentsPerQuarter.person_id.isin(caseness_array.person_id)].index, inplace=True)\n",
    "# Remove rows where a patient does not have any did-not-attends.\n",
    "bq_countAppointmentsPerQuarter = bq_countAppointmentsPerQuarter[bq_countAppointmentsPerQuarter.year_appointment != 0]\n",
    "%store bq_countAppointmentsPerQuarter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689c7463-f7bc-4fd2-b659-b26da728612d",
   "metadata": {},
   "source": [
    "##### SQL synax for did-not-attends (DNAs)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "580cf5bf-38c8-45fb-9e95-bc8e39ddf70c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'bq_countDNAsPerQuarter' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "sql_CTEs_body = \\\n",
    "\"\"\"\n",
    "#  ## Count of did-not-attend (DNA), per quarter.\n",
    ",tbl_DNAcodes AS ( \n",
    "    SELECT\n",
    "        snomedcode\n",
    "    FROM\n",
    "        UNNEST([\n",
    "                '\"\"\" + '\\', \\''.join(map(str, codes_to_query_DNA[\"code\"].tolist())) + \"\"\"'\n",
    "                ]) AS snomedcode\n",
    ")\n",
    ",tbl_countDNAsPerQuarter AS ( \n",
    "    SELECT \n",
    "        DISTINCT person_id\n",
    "        ,EXTRACT(YEAR FROM dateevent) AS year_DNA\n",
    "        ,EXTRACT(QUARTER FROM dateevent) AS quarter_DNA\n",
    "        ,COUNT( DISTINCT EXTRACT(DATE FROM dateevent) ) AS countDNAsPerQuarter\n",
    "    FROM\n",
    "        \"\"\" + server_id + \"\"\".\"\"\" + database_id + \"\"\".tbl_srcode AS a, tbl_DNAcodes\n",
    "    WHERE\n",
    "        a.snomedcode IN (tbl_DNAcodes.snomedcode)\n",
    "    GROUP BY\n",
    "        person_id\n",
    "        ,year_DNA\n",
    "        ,quarter_DNA\n",
    ")\n",
    "\"\"\"\n",
    "\n",
    "sql_final_select = \\\n",
    "\"\"\"\n",
    "SELECT\n",
    "    tbl_studyPopulation_no_caseness.person_id\n",
    "    ,year_DNA\n",
    "    ,quarter_DNA\n",
    "    ,countDNAsPerQuarter\n",
    "FROM\n",
    "    tbl_studyPopulation_no_caseness\n",
    "LEFT JOIN\n",
    "    tbl_countDNAsPerQuarter\n",
    "    ON tbl_studyPopulation_no_caseness.person_id = tbl_countDNAsPerQuarter.person_id\n",
    "ORDER BY\n",
    "    person_id\n",
    "    ,year_DNA\n",
    "    ,quarter_DNA\n",
    "\"\"\"\n",
    "\n",
    "global bq_countDNAsPerQuarter\n",
    "bq_countDNAsPerQuarter = pandas.read_gbq(sql_declarations + sql_studyPopulation + sql_CTEs_body + sql_final_select).fillna(0).astype(int)\n",
    "# Double-check that all exclusions have been applied.\n",
    "bq_countDNAsPerQuarter.drop(bq_countDNAsPerQuarter[~bq_countDNAsPerQuarter.person_id.isin(caseness_array.person_id)].index, inplace=True)\n",
    "# Remove rows where a patient does not have any did-not-attends.\n",
    "bq_countDNAsPerQuarter = bq_countDNAsPerQuarter[bq_countDNAsPerQuarter.year_DNA != 0]\n",
    "%store bq_countDNAsPerQuarter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff3046d2-00ad-4e4c-bdd7-2c89e0cad5cb",
   "metadata": {},
   "source": [
    "##### Calculating entropy statistics, using FOR loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd3a57c0-20c5-4b28-8084-39d4835bff22",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pid_processed = []\n",
    "# Set iterator.\n",
    "ls_pids = list(set(numpy.concatenate((bq_countAppointmentsPerQuarter.person_id.unique(), bq_countDNAsPerQuarter.person_id.unique()))))\n",
    "ls_pids.sort()\n",
    "\n",
    "# Set storage.\n",
    "ls_entropyBasedFS = []\n",
    "\n",
    "# Set counter of patients who are capped.\n",
    "cap_counter = 0\n",
    "\n",
    "# Do the work.\n",
    "t1 = time.time()\n",
    "for pid in tqdm.notebook.tqdm_notebook(ls_pids, unit = \" patients\"):\n",
    "    pt_years = \\\n",
    "            bq_countAppointmentsPerQuarter.loc[bq_countAppointmentsPerQuarter.person_id == pid, 'year_appointment'].append(\n",
    "             bq_countDNAsPerQuarter.loc[bq_countDNAsPerQuarter.person_id == pid, 'year_DNA'])\n",
    "\n",
    "    pt_years_lsrange =  pandas.DataFrame(\n",
    "        data = { 'year' : list( range( min(pt_years), max(pt_years) ) ) }\n",
    "        )\n",
    "    # Create a timeline of years and quarters for this particular patient.\n",
    "    pt_quarters = pandas.DataFrame( data = {'qtr': [1,2,3,4]} )\n",
    "    pt_timeline = pt_years_lsrange.merge(pt_quarters, how = 'cross')\n",
    "\n",
    "    # Join the patient's actual count of appointments-per-quarter-per-year to their timeline.\n",
    "    pt_appts = bq_countAppointmentsPerQuarter.loc[bq_countAppointmentsPerQuarter.person_id == pid, :]\n",
    "    pt_timeline_appts = \\\n",
    "        pandas.merge(pt_timeline, pt_appts, how = 'left',\n",
    "                     left_on = ['year', 'qtr'],\n",
    "                     right_on = ['year_appointment',\n",
    "                                 'quarter_appointment']).loc[:,'countAppointmentsPerQuarter'].fillna(0).astype(int)\n",
    "    \n",
    "    # Repeat for did-not-attend events.\n",
    "    pt_DNAs = bq_countDNAsPerQuarter.loc[bq_countDNAsPerQuarter.person_id == pid, :]\n",
    "    pt_timeline_DNAs = \\\n",
    "        pandas.merge(pt_timeline, pt_DNAs, how = 'left',\n",
    "                     left_on = ['year', 'qtr'],\n",
    "                     right_on = ['year_DNA',\n",
    "                                 'quarter_DNA']).loc[:,'countDNAsPerQuarter'].fillna(0).astype(int)\n",
    "    \n",
    "        # Increment cap counter.\n",
    "    if ((pt_timeline_DNAs > 72) + (pt_timeline_appts > 72)).any():\n",
    "        cap_counter += 1\n",
    "    \n",
    "    # Cap the count of events at 72. This fudge exists because the Python kernel in Google Cloud Platform dies trying\n",
    "    # to allocated memory to more than 72 states, as part of the entropy calculations.\n",
    "    pt_timeline_appts = [72 if i >72 else i for i in pt_timeline_appts]\n",
    "    pt_timeline_DNAs = [72 if i >72 else i for i in pt_timeline_DNAs]\n",
    "    \n",
    "    # Create the entropy-based feature sets.\n",
    "    pt_entropyStats_appts = chaoticlifeentropyfs(pt_timeline_appts)\n",
    "    pt_entropyStats_DNAs = chaoticlifeentropyfs(pt_timeline_DNAs)\n",
    "    \n",
    "    # Mark pid as processed, just in case things crash and you need to restart.\n",
    "    pid_processed.append(pid)\n",
    "\n",
    "    # send result using shared queue\n",
    "    ls_entropyBasedFS.append([pid] + pt_entropyStats_appts + pt_entropyStats_DNAs)\n",
    "\n",
    "print(f'It took {time.time() - t1} to process.')\n",
    "ls_entropyBasedFS.sort()\n",
    "\n",
    "# Print feedback.\n",
    "print(f'A total of {cap_counter} patients had more than 72 events in a given quarter, thus requiring capping at 72, for that quarter')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2f84cad-ccd5-4ba2-822e-dce45f12a9da",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'entropyBasedFS' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "entropyBasedFS = \\\n",
    "    pandas.DataFrame(ls_entropyBasedFS,\n",
    "                     columns =\n",
    "                     ['person_id'\n",
    "                     ,'activeInformationAppts'\n",
    "                     ,'entropyRateAppts'\n",
    "                     ,'spectralEntropyAppts'\n",
    "                     ,'sampleEntropyAppts'\n",
    "                     ,'eoeAppts'\n",
    "                     ,'averageEntropyAppts'\n",
    "                     ,'bubbleEntropyAppts'\n",
    "                     ,'activeInformationDNAs'\n",
    "                     ,'entropyRateDNAs'\n",
    "                     ,'spectralEntropyDNAs'\n",
    "                     ,'sampleEntropyDNAs'\n",
    "                     ,'eoeDNAs'\n",
    "                     ,'averageEntropyDNAs'\n",
    "                     ,'bubbleEntropyDNAs'\n",
    "        ])\n",
    "# Double-check that all exclusions have been applied.\n",
    "entropyBasedFS.drop(entropyBasedFS[~entropyBasedFS.person_id.isin(caseness_array.person_id)].index, inplace=True)\n",
    "%store entropyBasedFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8af64259-2363-480a-9a05-8c784179d0c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Join to `feature_set_array`.\n",
    "feature_set_array = feature_set_array.merge(entropyBasedFS, on = 'person_id', how = 'left' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37f49b9c-6c6d-4125-9aa1-89370bd4b209",
   "metadata": {},
   "source": [
    "##### Calculating entropy statistics, using multiprocessing"
   ]
  },
  {
   "cell_type": "raw",
   "id": "369e5ca4-6441-438b-98b3-762c07711291",
   "metadata": {
    "tags": []
   },
   "source": [
    "#######################\n",
    "# The prallel version.#\n",
    "#######################\n",
    "# Set iterator.\n",
    "ls_pids = list(set(numpy.concatenate((bq_countAppointmentsPerQuarter.person_id.unique(), bq_countDNAsPerQuarter.person_id.unique()))))\n",
    "ls_pids.sort()\n",
    "\n",
    "# \n",
    "try:\n",
    "    if len(pid_processed) > 0:\n",
    "        # The following code only runs if `pid_processed` exists and is loaded.\n",
    "        ls_pids = list(set(ls_pids).difference(set(pid_processed)))\n",
    "        print(\"\\nSome person_id values have already been processed. The `ls_pid` iterator will be shortened accordingly.\\n\")\n",
    "    else:\n",
    "        # If `pid_processed` does not exist, then I am starting from scratch\n",
    "        pid_processed = []\n",
    "        print(\"\\nNo person_id values have already been processed. All person_id value will be processed.\\n\")\n",
    "except:\n",
    "    # If `pid_processed` does not exist, then I am starting from scratch\n",
    "    pid_processed = []\n",
    "    print(\"\\nNo person_id values have already been processed. All person_id value will be processed.\\n\")\n",
    "\n",
    "# Set storage.\n",
    "ls_entropyBasedFS = []\n",
    "\n",
    "# Set number of workers.\n",
    "n_workers = 4\n",
    "\n",
    "t1 = time.time()\n",
    "# Do the main job of assessing the feature sets.\n",
    "if __name__ ==  '__main__':\n",
    "    # Create a shared queue\n",
    "    shared_queue = SimpleQueue()\n",
    "     \n",
    "    print('Parallel processing to calculate the entropy-based feature sets has begun...')\n",
    "    # create and configure the process pool\n",
    "    with Pool(processes=n_workers,initializer=init_worker, initargs=(shared_queue,)) as pool:\n",
    "        \n",
    "        # Issue tasks into the process pool\n",
    "        _ = pool.map_async(calculateentropybasedfs, ls_pids)\n",
    "        \n",
    "        # Read results (https://superfastpython.com/multiprocessing-pool-shared-global-variables/).\n",
    "        for i in range(len(ls_pids)):\n",
    "            ls_entropyBasedFS.append(shared_queue.get())\n",
    "    # Close down the pool to release resources. https://superfastpython.com/shutdown-the-multiprocessing-pool-in-python/\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    print('\\t...parallel processing to calculate the entropy-based feature sets has ended.')\n",
    "\n",
    "print(f'It took {time.time() - t1} to process.')\n",
    "ls_entropyBasedFS.sort()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "896e7182-d2c0-4844-a2a7-496349cccb7a",
   "metadata": {
    "tags": []
   },
   "source": [
    "entropyBasedFS = \\\n",
    "    pandas.DataFrame(ls_entropyBasedFS,\n",
    "                     columns =\n",
    "                     ['person_id'\n",
    "                     ,'activeInformationAppts'\n",
    "                     ,'entropyRateAppts'\n",
    "                     ,'spectralEntropyAppts'\n",
    "                     ,'sampleEntroptAppts'\n",
    "                     ,'eoeAppts'\n",
    "                     ,'averageEntropyAppts'\n",
    "                     ,'bubbleEntropyAppts'\n",
    "                     ,'activeInformationDNAs'\n",
    "                     ,'entropyRateDNAs'\n",
    "                     ,'spectralEntropyDNAs'\n",
    "                     ,'sampleEntropyDNAs'\n",
    "                     ,'eoeDNAs'\n",
    "                     ,'averageEntropyDNAs'\n",
    "                     ,'bubbleEntropyDNAs'\n",
    "        ])\n",
    "%store entropyBasedFS"
   ]
  },
  {
   "cell_type": "raw",
   "id": "0573a0b4-cb0f-4ce0-ac7c-2dc448cedd96",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Join to `feature_set_array`.\n",
    "feature_set_array = feature_set_array.merge(entropyBasedFS, on = 'person_id', how = 'left' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47242915-19ad-4a61-905e-d05d9bf94a97",
   "metadata": {},
   "source": [
    "## Store of feature_set_array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2b658d36-2cd0-4e41-ae9f-cda240a61431",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored 'feature_set_array' (DataFrame)\n"
     ]
    }
   ],
   "source": [
    "%store feature_set_array"
   ]
  }
 ],
 "metadata": {
  "environment": {
   "kernel": "python3",
   "name": "r-cpu.m125",
   "type": "gcloud",
   "uri": "us-docker.pkg.dev/deeplearning-platform-release/gcr.io/r-cpu:m125"
  },
  "kernelspec": {
   "display_name": "Python 3 (Local)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
